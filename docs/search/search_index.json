{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started \u00b6 This is an introductory course to Web Scraping using Python. Prerequisites \u00b6 Some exposure to Python and Google Colab (or Jupyter Notebook) is assumed, for example from attending the Data Fluency's Introduction to Python workshop. Setup Requirements \u00b6 Attendees will be required to bring a laptop with Firefox or Google Chrome installed. A Google/Monash account is required (for accessing the Colab environment).","title":"Getting started"},{"location":"#getting-started","text":"This is an introductory course to Web Scraping using Python.","title":"Getting Started"},{"location":"#prerequisites","text":"Some exposure to Python and Google Colab (or Jupyter Notebook) is assumed, for example from attending the Data Fluency's Introduction to Python workshop.","title":"Prerequisites"},{"location":"#setup-requirements","text":"Attendees will be required to bring a laptop with Firefox or Google Chrome installed. A Google/Monash account is required (for accessing the Colab environment).","title":"Setup Requirements"},{"location":"sample/","text":"Specimen \u00b6 Body copy \u00b6 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras arcu libero, mollis sed massa vel, ornare viverra ex . Mauris a ullamcorper lacus. Nullam urna elit, malesuada eget finibus ut, ullamcorper ac tortor. Vestibulum sodales pulvinar nisl, pharetra aliquet est. Quisque volutpat erat ac nisi accumsan tempor. Sed suscipit , orci non pretium pretium, quam mi gravida metus, vel venenatis justo est condimentum diam. Maecenas non ornare justo. Nam a ipsum eros. Nulla aliquam orci sit amet nisl posuere malesuada. Proin aliquet nulla velit, quis ultricies orci feugiat et. Ut tincidunt sollicitudin tincidunt. Aenean ullamcorper sit amet nulla at interdum. Headings \u00b6 The 3rd level \u00b6 The 4th level \u00b6 The 5th level \u00b6 The 6th level \u00b6 Headings with secondary text \u00b6 The 3rd level with secondary text \u00b6 The 4th level with secondary text \u00b6 The 5th level with secondary text \u00b6 The 6th level with secondary text \u00b6 Blockquotes \u00b6 Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a ultricies libero efficitur sed. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Sed molestie imperdiet consectetur. Blockquote nesting \u00b6 Sed aliquet , neque at rutrum mollis, neque nisi tincidunt nibh, vitae faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem libero fermentum urna, ut efficitur elit ligula et nunc. Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla. Ut sit amet placerat ante. Proin sed elementum nulla. Nunc vitae sem odio. Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum. eu odio. Suspendisse rutrum facilisis risus , eu posuere neque commodo a. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo bibendum, sodales mauris ut, tincidunt massa. Other content blocks \u00b6 Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. js hl_lines=\"8\" var _extends = function(target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { target[key] = source[key]; } } return target; }; Praesent at :::js return target , sodales nibh vel, tempor felis. Fusce vel lacinia lacus. Suspendisse rhoncus nunc non nisi iaculis ultrices. Donec consectetur mauris non neque imperdiet, eget volutpat libero. Lists \u00b6 Unordered lists \u00b6 Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam ac, aliquet sed mauris. Nulla et rhoncus turpis. Mauris ultricies elementum leo. Duis efficitur accumsan nibh eu mattis. Vivamus tempus velit eros, porttitor placerat nibh lacinia sed. Aenean in finibus diam. Ordered lists \u00b6 Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla consectetur feugiat sodales. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Aliquam ornare feugiat quam et egestas. Nunc id erat et quam pellentesque lacinia eu vel odio. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a ultricies libero efficitur sed. Mauris dictum mi lacus Ut sit amet placerat ante Suspendisse ac eros arcu Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Sed aliquet, neque at rutrum mollis, neque nisi tincidunt nibh. Pellentesque eget :::js var _extends ornare tellus, ut gravida mi. js hl_lines=\"1\" var _extends = function(target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { target[key] = source[key]; } } return target; }; Vivamus id mi enim. Integer id turpis sapien. Ut condimentum lobortis sagittis. Aliquam purus tellus, faucibus eget urna at, iaculis venenatis nulla. Vivamus a pharetra leo. Definition lists \u00b6 Lorem ipsum dolor sit amet : Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla. 1 2 3 Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Cras arcu libero : Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam ac, aliquet sed mauris. Code blocks \u00b6 Inline \u00b6 Morbi eget dapibus felis . Vivamus venenatis porttitor tortor sit amet rutrum. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Pellentesque aliquet quam enim , eu volutpat urna rutrum a. Nam vehicula nunc :::js return target mauris, a ultricies libero efficitur sed. Sed molestie imperdiet consectetur. Vivamus a pharetra leo. Pellentesque eget ornare tellus, ut gravida mi. Fusce vel lacinia lacus. Listing \u00b6 1 2 3 4 5 6 7 8 9 var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; Horizontal rules \u00b6 Aenean in finibus diam. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla consectetur feugiat sodales. Data tables \u00b6 Sollicitudo / Pellentesi consectetur adipiscing elit arcu sed Vivamus a pharetra yes yes yes yes yes Ornare viverra ex yes yes yes yes yes Mauris a ullamcorper yes yes partial yes yes Nullam urna elit yes yes yes yes yes Malesuada eget finibus yes yes yes yes yes Ullamcorper yes yes yes yes yes Vestibulum sodales yes - yes - yes Pulvinar nisl yes yes yes - - Pharetra aliquet est yes yes yes yes yes Sed suscipit yes yes yes yes yes Orci non pretium yes partial - - - Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero. Left Center Right Lorem dolor amet ipsum sit Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. Table with colgroups (Pandoc) Lorem ipsum dolor sit amet. Sed sagittis eleifend rutrum. Donec vitae suscipit est.","title":"Specimen"},{"location":"sample/#specimen","text":"","title":"Specimen"},{"location":"sample/#body-copy","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras arcu libero, mollis sed massa vel, ornare viverra ex . Mauris a ullamcorper lacus. Nullam urna elit, malesuada eget finibus ut, ullamcorper ac tortor. Vestibulum sodales pulvinar nisl, pharetra aliquet est. Quisque volutpat erat ac nisi accumsan tempor. Sed suscipit , orci non pretium pretium, quam mi gravida metus, vel venenatis justo est condimentum diam. Maecenas non ornare justo. Nam a ipsum eros. Nulla aliquam orci sit amet nisl posuere malesuada. Proin aliquet nulla velit, quis ultricies orci feugiat et. Ut tincidunt sollicitudin tincidunt. Aenean ullamcorper sit amet nulla at interdum.","title":"Body copy"},{"location":"sample/#headings","text":"","title":"Headings"},{"location":"sample/#the-3rd-level","text":"","title":"The 3rd level"},{"location":"sample/#the-4th-level","text":"","title":"The 4th level"},{"location":"sample/#the-5th-level","text":"","title":"The 5th level"},{"location":"sample/#the-6th-level","text":"","title":"The 6th level"},{"location":"sample/#headings-with-secondary-text","text":"","title":"Headings with secondary text"},{"location":"sample/#the-3rd-level-with-secondary-text","text":"","title":"The 3rd level with secondary text"},{"location":"sample/#the-4th-level-with-secondary-text","text":"","title":"The 4th level with secondary text"},{"location":"sample/#the-5th-level-with-secondary-text","text":"","title":"The 5th level with secondary text"},{"location":"sample/#the-6th-level-with-secondary-text","text":"","title":"The 6th level with secondary text"},{"location":"sample/#blockquotes","text":"Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a ultricies libero efficitur sed. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Sed molestie imperdiet consectetur.","title":"Blockquotes"},{"location":"sample/#blockquote-nesting","text":"Sed aliquet , neque at rutrum mollis, neque nisi tincidunt nibh, vitae faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem libero fermentum urna, ut efficitur elit ligula et nunc. Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla. Ut sit amet placerat ante. Proin sed elementum nulla. Nunc vitae sem odio. Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum. eu odio. Suspendisse rutrum facilisis risus , eu posuere neque commodo a. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo bibendum, sodales mauris ut, tincidunt massa.","title":"Blockquote nesting"},{"location":"sample/#other-content-blocks","text":"Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. js hl_lines=\"8\" var _extends = function(target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { target[key] = source[key]; } } return target; }; Praesent at :::js return target , sodales nibh vel, tempor felis. Fusce vel lacinia lacus. Suspendisse rhoncus nunc non nisi iaculis ultrices. Donec consectetur mauris non neque imperdiet, eget volutpat libero.","title":"Other content blocks"},{"location":"sample/#lists","text":"","title":"Lists"},{"location":"sample/#unordered-lists","text":"Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam ac, aliquet sed mauris. Nulla et rhoncus turpis. Mauris ultricies elementum leo. Duis efficitur accumsan nibh eu mattis. Vivamus tempus velit eros, porttitor placerat nibh lacinia sed. Aenean in finibus diam.","title":"Unordered lists"},{"location":"sample/#ordered-lists","text":"Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla consectetur feugiat sodales. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Aliquam ornare feugiat quam et egestas. Nunc id erat et quam pellentesque lacinia eu vel odio. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a ultricies libero efficitur sed. Mauris dictum mi lacus Ut sit amet placerat ante Suspendisse ac eros arcu Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Sed aliquet, neque at rutrum mollis, neque nisi tincidunt nibh. Pellentesque eget :::js var _extends ornare tellus, ut gravida mi. js hl_lines=\"1\" var _extends = function(target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { target[key] = source[key]; } } return target; }; Vivamus id mi enim. Integer id turpis sapien. Ut condimentum lobortis sagittis. Aliquam purus tellus, faucibus eget urna at, iaculis venenatis nulla. Vivamus a pharetra leo.","title":"Ordered lists"},{"location":"sample/#definition-lists","text":"Lorem ipsum dolor sit amet : Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla. 1 2 3 Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Cras arcu libero : Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam ac, aliquet sed mauris.","title":"Definition lists"},{"location":"sample/#code-blocks","text":"","title":"Code blocks"},{"location":"sample/#inline","text":"Morbi eget dapibus felis . Vivamus venenatis porttitor tortor sit amet rutrum. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Pellentesque aliquet quam enim , eu volutpat urna rutrum a. Nam vehicula nunc :::js return target mauris, a ultricies libero efficitur sed. Sed molestie imperdiet consectetur. Vivamus a pharetra leo. Pellentesque eget ornare tellus, ut gravida mi. Fusce vel lacinia lacus.","title":"Inline"},{"location":"sample/#listing","text":"1 2 3 4 5 6 7 8 9 var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; };","title":"Listing"},{"location":"sample/#horizontal-rules","text":"Aenean in finibus diam. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla consectetur feugiat sodales.","title":"Horizontal rules"},{"location":"sample/#data-tables","text":"Sollicitudo / Pellentesi consectetur adipiscing elit arcu sed Vivamus a pharetra yes yes yes yes yes Ornare viverra ex yes yes yes yes yes Mauris a ullamcorper yes yes partial yes yes Nullam urna elit yes yes yes yes yes Malesuada eget finibus yes yes yes yes yes Ullamcorper yes yes yes yes yes Vestibulum sodales yes - yes - yes Pulvinar nisl yes yes yes - - Pharetra aliquet est yes yes yes yes yes Sed suscipit yes yes yes yes yes Orci non pretium yes partial - - - Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero. Left Center Right Lorem dolor amet ipsum sit Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. Table with colgroups (Pandoc) Lorem ipsum dolor sit amet. Sed sagittis eleifend rutrum. Donec vitae suscipit est.","title":"Data tables"},{"location":"section-0-brief-python-refresher/","text":"https://xkcd.com/license.html (CC BY-NC 2.5) Jupyter-style notebooks on Google Colaboratory - A quick tour \u00b6 Go to this link and login with your Google account. Select NEW NOTEBOOK - a new Python3 notebook will be created. Type some Python code in the top cell, eg: 1 print ( \"Hello world!!\" ) 1 Hello world!! Shift-Enter to run the contents of the cell. In this section we will take a quick tour of some of the concepts that will be used for the rest of our web scraping workshop. Dataframes \u00b6 One of the most powerful data structures in Python is the Pandas DataFrame . It allows tabular data, including csv (comma seperated values) and tsv (tab seperated values), to be processed and manipulated. People familiar with Excel will no doubt find it intuitive and easy to grasp. Since most csv (or tsv ) has become the de facto standard for sharing datasets both large and small, Pandas dataframe is the way to go. 1 2 import pandas as pd # importing the package and using `pd` as the alias print ( 'Pandas version : {} ' . format ( pd . __version__ )) 1 Pandas version : 1.0.1 Suppose we wanted to create a dataframe as follows, name title sam physicist rob economist Let's create a dictionary with the headers as keys and their corresponding values as a list as follows, 1 data = { 'name' : [ 'sam' , 'rob' ], 'title' : [ 'physicist' , 'economist' ]} Converting the same to a dataframe, 1 2 3 4 df = pd . DataFrame ( data ) # Note: converting to markdown for ease of display on site # print(df.to_markdown()) df name title 0 sam physicist 1 rob economist Now lets create a bigger dataframe and learn some useful functions that can be performed on them. 1 2 3 4 5 data = { 'Name' : [ 'Sam' , 'Rob' , 'Jack' , 'Jill' , 'Dave' , 'Alex' , 'Steve' ], \\ 'Title' : [ 'Physicist' , 'Economist' , 'Statistician' , 'Data Scientist' , 'Designer' , 'Architect' , 'Doctor' ], \\ 'Age' : [ 59 , 66 , 42 , 28 , 24 , 39 , 52 ], \\ 'City' : [ 'Melbourne' , 'Melbourne' , 'Sydney' , 'Sydney' , 'Melbourne' , 'Perth' , 'Brisbane' ], \\ 'University' : [ 'Monash' , 'Monash' , 'UNSW' , 'UTS' , 'Uni Mel' , 'UWA' , 'UQ' ]} 1 2 df = pd . DataFrame ( data ) df Name Title Age City University 0 Sam Physicist 59 Melbourne Monash 1 Rob Economist 66 Melbourne Monash 2 Jack Statistician 42 Sydney UNSW 3 Jill Data Scientist 28 Sydney UTS 4 Dave Designer 24 Melbourne Uni Mel 5 Alex Architect 39 Perth UWA 6 Steve Doctor 52 Brisbane UQ We can also take a quick glance at its contents by using : df.head() : To display the first 5 rows df.tail() : To display the last 5 rows 1 df . head () Name Title Age City University 0 Sam Physicist 59 Melbourne Monash 1 Rob Economist 66 Melbourne Monash 2 Jack Statistician 42 Sydney UNSW 3 Jill Data Scientist 28 Sydney UTS 4 Dave Designer 24 Melbourne Uni Mel 1 df . tail () Name Title Age City University 2 Jack Statistician 42 Sydney UNSW 3 Jill Data Scientist 28 Sydney UTS 4 Dave Designer 24 Melbourne Uni Mel 5 Alex Architect 39 Perth UWA 6 Steve Doctor 52 Brisbane UQ Lets say we want to fiter out all the people from Sydney . 1 df [ df [ 'City' ] == 'Sydney' ] Name Title Age City University 2 Jack Statistician 42 Sydney UNSW 3 Jill Data Scientist 28 Sydney UTS Now, lets say we want to look at all the people in Melbourne and in Monash . Notice the usage of () and & . 1 df [( df [ 'City' ] == 'Melbourne' ) & ( df [ 'University' ] == 'Monash' )] Name Title Age City University 0 Sam Physicist 59 Melbourne Monash 1 Rob Economist 66 Melbourne Monash Challenge \u00b6 How can we filter people from Melbourne and above the Age of 50 ? We can also fetch specific rows based on their indexes as well. 1 df . iloc [ 1 : 3 ] Name Title Age City University 1 Rob Economist 66 Melbourne Monash 2 Jack Statistician 42 Sydney UNSW Lets try changing Jack's age to 43, because today is his Birthday and he has now turned 43. 1 df . loc [ 2 , 'Age' ] = 43 The above is just one way to do this. Some of the other ways are as follows: df.at[2, 'Age'] = 43 df.loc[df[df['Name'] == 'Jack'].index, 'Age'] = 43 Lets look at the updated data frame. 1 df Name Title Age City University 0 Sam Physicist 59 Melbourne Monash 1 Rob Economist 66 Melbourne Monash 2 Jack Statistician 43 Sydney UNSW 3 Jill Data Scientist 28 Sydney UTS 4 Dave Designer 24 Melbourne Uni Mel 5 Alex Architect 39 Perth UWA 6 Steve Doctor 52 Brisbane UQ For exporting a Pandas dataframe to a csv file, we can use to_csv() as follows 1 df . to_csv ( filename , index = False ) Lets try writing our data frame to a file. 1 df . to_csv ( 'researchers.csv' , index = False ) In order to read external files we use read_csv() function, 1 pd . read_csv ( filename , sep = ',' ) We can read back the file that we just created. 1 2 df_res = pd . read_csv ( 'researchers.csv' , sep = ',' ) df_res Name Title Age City University 0 Sam Physicist 59 Melbourne Monash 1 Rob Economist 66 Melbourne Monash 2 Jack Statistician 43 Sydney UNSW 3 Jill Data Scientist 28 Sydney UTS 4 Dave Designer 24 Melbourne Uni Mel 5 Alex Architect 39 Perth UWA 6 Steve Doctor 52 Brisbane UQ JSON \u00b6 JSON stands for JavaScript Object Notation . When exchanging data between a browser and a server, the data can only be text. Python has a built-in package called json , which can be used to work with JSON data. 1 import json Once we imported the library, now lets look at how we can obtain a JSON object from a string (or more accurately a JSON string). This process is also know as deserialization where we convert a string to an object. JSON is a string that can be turned into ('deserialized') a Python dictionary containing just primitive types (floats, strings, bools, lists, dicts and None). 1 2 3 4 5 6 7 8 9 10 # Convert from JSON to Python: # some JSON: x = '{ \"name\":\"John\", \"age\":30, \"city\":\"New York\"}' # parse x: y = json . loads ( x ) # the result is a Python dictionary: print ( y [ \"age\" ]) 1 30 Lets take a look at y as follows, 1 print ( y ) 1 2 3 4 5 { \"name\": \"John\", \"age\": 30, \"city\": \"New York\" } We can obtain the exact same JSON string we defined earlier from a Python dictionary as follows, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Convert from Python to JSON # a Python object (dict): x = { \"name\" : \"John\" , \"age\" : 30 , \"city\" : \"New York\" } # convert into JSON: y = json . dumps ( x ) # the result is a JSON string: print ( y ) 1 {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"} For better formatting we can indent the same as, 1 2 3 # Indentation y = json . dumps ( x , indent = 4 ) print ( y ) 1 2 3 4 5 { \"name\": \"John\", \"age\": 30, \"city\": \"New York\" } Regex \u00b6 Regular expressions or regex are a powerful tool to extract key pieces of data from raw text. This is a whirlwind tour of regular expressions so you have some familiarity, but becoming proficient will require further study and practise. Regex is deep enough to have a whole workshop of it's own You can try your regex expressions in : pythex for a python oriented regex editor regexr for a more visual explanation behind the expressions (good for getting started) 1 import re # regex package in python is named 're' 1 2 my_str = 'python123good' re . search ( '123' , my_str ) 1 <re.Match object; span=(6, 9), match='123'> 1 2 3 4 if re . search ( '123' , my_str ): print ( \"Found\" ) else : print ( \"Not found\" ) 1 Found We can use [0-9] in the regular expression to identify any one number in the string. 1 2 my_str = 'python123good' re . search ( '[0-9]' , my_str ) 1 <re.Match object; span=(6, 7), match='1'> Notice that it matches the first occurance only. Now, the above regex can be modified to match any three numbers in a string. 1 2 my_str = 'python123good' re . search ( '[0-9][0-9][0-9]' , my_str ) 1 <re.Match object; span=(6, 9), match='123'> 1 2 3 4 5 print ( re . search ( '[0-9][0-9][0-9]' , 'hello123' )) # matches 123 print ( re . search ( '[0-9][0-9][0-9]' , 'great678python' )) # matches 678 print ( re . search ( '[0-9][0-9][0-9]' , '01234webscraing' )) # matches 012 print ( re . search ( '[0-9][0-9][0-9]' , '01web5678scraing' )) # matches 567 print ( re . search ( '[0-9][0-9][0-9]' , 'hello world' )) # matches nothing 1 2 3 4 5 <re.Match object; span=(5, 8), match='123'> <re.Match object; span=(5, 8), match='678'> <re.Match object; span=(0, 3), match='012'> <re.Match object; span=(5, 8), match='567'> None As seen above, it matches the first occurance of three digits occuring together. The above example can be extended to match any number of numbers using the wild character * which matches zero or more repetitions . 1 print ( re . search ( '[a-z]*[0-9]*' , 'hello123@@' )) # matches hello123 1 <re.Match object; span=(0, 8), match='hello123'> What if we just want to capture only the numbers? Capture group is the answer. 1 2 3 num_regex = re . compile ( '[a-z]*([0-9]*)[a-z]*' ) my_str = 'python123good' num_regex . findall ( my_str ) 1 ['123', ''] We see that it matchs an empty string as well because * matches zero or more occurances. To avoid this, we can use + which matches one or more occurances. 1 2 3 num_regex = re . compile ( '[a-z]*([0-9]+)[a-z]*' ) my_str = 'python123good' num_regex . findall ( my_str ) 1 ['123'] We can use ^ and $ to match at the beginning and end of string. As shown in the below 2 examples, we use ^ to get the numbers by which the string is starting. 1 2 3 num_regex = re . compile ( '^([0-9]+)[a-z]*' ) my_str = '123good' num_regex . findall ( my_str ) 1 ['123'] 1 2 my_str = 'hello123good' num_regex . findall ( my_str ) 1 [] Challenge \u00b6 What regular expression can be used to match the numbers only at the end of the string: '[a-z]*([0-9]+)[a-z]+' '[a-z]*([0-9]+)$' '$[a-z]*([0-9]+)' '([0-9]+)' Now, having learnt the regular expressions on basic strings, the same concept can be applied to an html as well as shown below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 html = r ''' <!DOCTYPE html> <html> <head> <title>site</title> </head> <body> <h1>Sam</h1> <h2>Physicist</h2> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod.</p> <h1>Rob</h1> <h2>Economist</h2> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et.</p> </body> </html> ''' print ( html ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 <!DOCTYPE html> <html> <head> <title>site</title> </head> <body> <h1>Sam</h1> <h2>Physicist</h2> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod.</p> <h1>Rob</h1> <h2>Economist</h2> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et.</p> </body> </html> Now, if we are only interested in : - names i.e. the data inside the <h1></h1> tags, and - title i.e. the data inside the <h2></h2> tags we can extract the same using regex. Lets define the expressions (or patterns) to capture all text between the tags as follows : <h1>(.*?)</h1> : capture all text contained within <h1></h1> tags <h2>(.*?)</h2> : capture all text contained within <h2></h2> tags 1 2 regex_h1 = re . compile ( '<h1>(.*?)</h1>' ) regex_h2 = re . compile ( '<h2>(.*?)</h2>' ) and use findall() to return all the instances that match with our pattern, 1 2 3 4 names = regex_h1 . findall ( html ) titles = regex_h2 . findall ( html ) print ( names , titles ) 1 ['Sam', 'Rob'] ['Physicist', 'Economist'] From a web scraping perspective \u00b6 JSON and XML are the most widely used formats to carry data all over the internet. To work with CSV s (or TSV s), Pandas DataFrames are the de facto standard. Regexes help us extract key pieces of information (sub-strings) from raw, messy and unstructured text (strings). References \u00b6 https://xkcd.com/353/ https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html https://realpython.com/regex-python/","title":"A (brief) Python refresher"},{"location":"section-0-brief-python-refresher/#jupyter-style-notebooks-on-google-colaboratory-a-quick-tour","text":"Go to this link and login with your Google account. Select NEW NOTEBOOK - a new Python3 notebook will be created. Type some Python code in the top cell, eg: 1 print ( \"Hello world!!\" ) 1 Hello world!! Shift-Enter to run the contents of the cell. In this section we will take a quick tour of some of the concepts that will be used for the rest of our web scraping workshop.","title":"Jupyter-style notebooks on Google Colaboratory - A quick tour"},{"location":"section-0-brief-python-refresher/#dataframes","text":"One of the most powerful data structures in Python is the Pandas DataFrame . It allows tabular data, including csv (comma seperated values) and tsv (tab seperated values), to be processed and manipulated. People familiar with Excel will no doubt find it intuitive and easy to grasp. Since most csv (or tsv ) has become the de facto standard for sharing datasets both large and small, Pandas dataframe is the way to go. 1 2 import pandas as pd # importing the package and using `pd` as the alias print ( 'Pandas version : {} ' . format ( pd . __version__ )) 1 Pandas version : 1.0.1 Suppose we wanted to create a dataframe as follows, name title sam physicist rob economist Let's create a dictionary with the headers as keys and their corresponding values as a list as follows, 1 data = { 'name' : [ 'sam' , 'rob' ], 'title' : [ 'physicist' , 'economist' ]} Converting the same to a dataframe, 1 2 3 4 df = pd . DataFrame ( data ) # Note: converting to markdown for ease of display on site # print(df.to_markdown()) df name title 0 sam physicist 1 rob economist Now lets create a bigger dataframe and learn some useful functions that can be performed on them. 1 2 3 4 5 data = { 'Name' : [ 'Sam' , 'Rob' , 'Jack' , 'Jill' , 'Dave' , 'Alex' , 'Steve' ], \\ 'Title' : [ 'Physicist' , 'Economist' , 'Statistician' , 'Data Scientist' , 'Designer' , 'Architect' , 'Doctor' ], \\ 'Age' : [ 59 , 66 , 42 , 28 , 24 , 39 , 52 ], \\ 'City' : [ 'Melbourne' , 'Melbourne' , 'Sydney' , 'Sydney' , 'Melbourne' , 'Perth' , 'Brisbane' ], \\ 'University' : [ 'Monash' , 'Monash' , 'UNSW' , 'UTS' , 'Uni Mel' , 'UWA' , 'UQ' ]} 1 2 df = pd . DataFrame ( data ) df Name Title Age City University 0 Sam Physicist 59 Melbourne Monash 1 Rob Economist 66 Melbourne Monash 2 Jack Statistician 42 Sydney UNSW 3 Jill Data Scientist 28 Sydney UTS 4 Dave Designer 24 Melbourne Uni Mel 5 Alex Architect 39 Perth UWA 6 Steve Doctor 52 Brisbane UQ We can also take a quick glance at its contents by using : df.head() : To display the first 5 rows df.tail() : To display the last 5 rows 1 df . head () Name Title Age City University 0 Sam Physicist 59 Melbourne Monash 1 Rob Economist 66 Melbourne Monash 2 Jack Statistician 42 Sydney UNSW 3 Jill Data Scientist 28 Sydney UTS 4 Dave Designer 24 Melbourne Uni Mel 1 df . tail () Name Title Age City University 2 Jack Statistician 42 Sydney UNSW 3 Jill Data Scientist 28 Sydney UTS 4 Dave Designer 24 Melbourne Uni Mel 5 Alex Architect 39 Perth UWA 6 Steve Doctor 52 Brisbane UQ Lets say we want to fiter out all the people from Sydney . 1 df [ df [ 'City' ] == 'Sydney' ] Name Title Age City University 2 Jack Statistician 42 Sydney UNSW 3 Jill Data Scientist 28 Sydney UTS Now, lets say we want to look at all the people in Melbourne and in Monash . Notice the usage of () and & . 1 df [( df [ 'City' ] == 'Melbourne' ) & ( df [ 'University' ] == 'Monash' )] Name Title Age City University 0 Sam Physicist 59 Melbourne Monash 1 Rob Economist 66 Melbourne Monash","title":"Dataframes"},{"location":"section-0-brief-python-refresher/#challenge","text":"How can we filter people from Melbourne and above the Age of 50 ? We can also fetch specific rows based on their indexes as well. 1 df . iloc [ 1 : 3 ] Name Title Age City University 1 Rob Economist 66 Melbourne Monash 2 Jack Statistician 42 Sydney UNSW Lets try changing Jack's age to 43, because today is his Birthday and he has now turned 43. 1 df . loc [ 2 , 'Age' ] = 43 The above is just one way to do this. Some of the other ways are as follows: df.at[2, 'Age'] = 43 df.loc[df[df['Name'] == 'Jack'].index, 'Age'] = 43 Lets look at the updated data frame. 1 df Name Title Age City University 0 Sam Physicist 59 Melbourne Monash 1 Rob Economist 66 Melbourne Monash 2 Jack Statistician 43 Sydney UNSW 3 Jill Data Scientist 28 Sydney UTS 4 Dave Designer 24 Melbourne Uni Mel 5 Alex Architect 39 Perth UWA 6 Steve Doctor 52 Brisbane UQ For exporting a Pandas dataframe to a csv file, we can use to_csv() as follows 1 df . to_csv ( filename , index = False ) Lets try writing our data frame to a file. 1 df . to_csv ( 'researchers.csv' , index = False ) In order to read external files we use read_csv() function, 1 pd . read_csv ( filename , sep = ',' ) We can read back the file that we just created. 1 2 df_res = pd . read_csv ( 'researchers.csv' , sep = ',' ) df_res Name Title Age City University 0 Sam Physicist 59 Melbourne Monash 1 Rob Economist 66 Melbourne Monash 2 Jack Statistician 43 Sydney UNSW 3 Jill Data Scientist 28 Sydney UTS 4 Dave Designer 24 Melbourne Uni Mel 5 Alex Architect 39 Perth UWA 6 Steve Doctor 52 Brisbane UQ","title":"Challenge"},{"location":"section-0-brief-python-refresher/#json","text":"JSON stands for JavaScript Object Notation . When exchanging data between a browser and a server, the data can only be text. Python has a built-in package called json , which can be used to work with JSON data. 1 import json Once we imported the library, now lets look at how we can obtain a JSON object from a string (or more accurately a JSON string). This process is also know as deserialization where we convert a string to an object. JSON is a string that can be turned into ('deserialized') a Python dictionary containing just primitive types (floats, strings, bools, lists, dicts and None). 1 2 3 4 5 6 7 8 9 10 # Convert from JSON to Python: # some JSON: x = '{ \"name\":\"John\", \"age\":30, \"city\":\"New York\"}' # parse x: y = json . loads ( x ) # the result is a Python dictionary: print ( y [ \"age\" ]) 1 30 Lets take a look at y as follows, 1 print ( y ) 1 2 3 4 5 { \"name\": \"John\", \"age\": 30, \"city\": \"New York\" } We can obtain the exact same JSON string we defined earlier from a Python dictionary as follows, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Convert from Python to JSON # a Python object (dict): x = { \"name\" : \"John\" , \"age\" : 30 , \"city\" : \"New York\" } # convert into JSON: y = json . dumps ( x ) # the result is a JSON string: print ( y ) 1 {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"} For better formatting we can indent the same as, 1 2 3 # Indentation y = json . dumps ( x , indent = 4 ) print ( y ) 1 2 3 4 5 { \"name\": \"John\", \"age\": 30, \"city\": \"New York\" }","title":"JSON"},{"location":"section-0-brief-python-refresher/#regex","text":"Regular expressions or regex are a powerful tool to extract key pieces of data from raw text. This is a whirlwind tour of regular expressions so you have some familiarity, but becoming proficient will require further study and practise. Regex is deep enough to have a whole workshop of it's own You can try your regex expressions in : pythex for a python oriented regex editor regexr for a more visual explanation behind the expressions (good for getting started) 1 import re # regex package in python is named 're' 1 2 my_str = 'python123good' re . search ( '123' , my_str ) 1 <re.Match object; span=(6, 9), match='123'> 1 2 3 4 if re . search ( '123' , my_str ): print ( \"Found\" ) else : print ( \"Not found\" ) 1 Found We can use [0-9] in the regular expression to identify any one number in the string. 1 2 my_str = 'python123good' re . search ( '[0-9]' , my_str ) 1 <re.Match object; span=(6, 7), match='1'> Notice that it matches the first occurance only. Now, the above regex can be modified to match any three numbers in a string. 1 2 my_str = 'python123good' re . search ( '[0-9][0-9][0-9]' , my_str ) 1 <re.Match object; span=(6, 9), match='123'> 1 2 3 4 5 print ( re . search ( '[0-9][0-9][0-9]' , 'hello123' )) # matches 123 print ( re . search ( '[0-9][0-9][0-9]' , 'great678python' )) # matches 678 print ( re . search ( '[0-9][0-9][0-9]' , '01234webscraing' )) # matches 012 print ( re . search ( '[0-9][0-9][0-9]' , '01web5678scraing' )) # matches 567 print ( re . search ( '[0-9][0-9][0-9]' , 'hello world' )) # matches nothing 1 2 3 4 5 <re.Match object; span=(5, 8), match='123'> <re.Match object; span=(5, 8), match='678'> <re.Match object; span=(0, 3), match='012'> <re.Match object; span=(5, 8), match='567'> None As seen above, it matches the first occurance of three digits occuring together. The above example can be extended to match any number of numbers using the wild character * which matches zero or more repetitions . 1 print ( re . search ( '[a-z]*[0-9]*' , 'hello123@@' )) # matches hello123 1 <re.Match object; span=(0, 8), match='hello123'> What if we just want to capture only the numbers? Capture group is the answer. 1 2 3 num_regex = re . compile ( '[a-z]*([0-9]*)[a-z]*' ) my_str = 'python123good' num_regex . findall ( my_str ) 1 ['123', ''] We see that it matchs an empty string as well because * matches zero or more occurances. To avoid this, we can use + which matches one or more occurances. 1 2 3 num_regex = re . compile ( '[a-z]*([0-9]+)[a-z]*' ) my_str = 'python123good' num_regex . findall ( my_str ) 1 ['123'] We can use ^ and $ to match at the beginning and end of string. As shown in the below 2 examples, we use ^ to get the numbers by which the string is starting. 1 2 3 num_regex = re . compile ( '^([0-9]+)[a-z]*' ) my_str = '123good' num_regex . findall ( my_str ) 1 ['123'] 1 2 my_str = 'hello123good' num_regex . findall ( my_str ) 1 []","title":"Regex"},{"location":"section-0-brief-python-refresher/#challenge_1","text":"What regular expression can be used to match the numbers only at the end of the string: '[a-z]*([0-9]+)[a-z]+' '[a-z]*([0-9]+)$' '$[a-z]*([0-9]+)' '([0-9]+)' Now, having learnt the regular expressions on basic strings, the same concept can be applied to an html as well as shown below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 html = r ''' <!DOCTYPE html> <html> <head> <title>site</title> </head> <body> <h1>Sam</h1> <h2>Physicist</h2> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod.</p> <h1>Rob</h1> <h2>Economist</h2> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et.</p> </body> </html> ''' print ( html ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 <!DOCTYPE html> <html> <head> <title>site</title> </head> <body> <h1>Sam</h1> <h2>Physicist</h2> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod.</p> <h1>Rob</h1> <h2>Economist</h2> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et.</p> </body> </html> Now, if we are only interested in : - names i.e. the data inside the <h1></h1> tags, and - title i.e. the data inside the <h2></h2> tags we can extract the same using regex. Lets define the expressions (or patterns) to capture all text between the tags as follows : <h1>(.*?)</h1> : capture all text contained within <h1></h1> tags <h2>(.*?)</h2> : capture all text contained within <h2></h2> tags 1 2 regex_h1 = re . compile ( '<h1>(.*?)</h1>' ) regex_h2 = re . compile ( '<h2>(.*?)</h2>' ) and use findall() to return all the instances that match with our pattern, 1 2 3 4 names = regex_h1 . findall ( html ) titles = regex_h2 . findall ( html ) print ( names , titles ) 1 ['Sam', 'Rob'] ['Physicist', 'Economist']","title":"Challenge"},{"location":"section-0-brief-python-refresher/#from-a-web-scraping-perspective","text":"JSON and XML are the most widely used formats to carry data all over the internet. To work with CSV s (or TSV s), Pandas DataFrames are the de facto standard. Regexes help us extract key pieces of information (sub-strings) from raw, messy and unstructured text (strings).","title":"From a web scraping perspective"},{"location":"section-0-brief-python-refresher/#references","text":"https://xkcd.com/353/ https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html https://realpython.com/regex-python/","title":"References"},{"location":"section-1-intro-to-web-scraping/","text":"https://xkcd.com/license.html (CC BY-NC 2.5) What is web scraping? \u00b6 Web scraping is a technique for extracting information from websites. This can be done manually but it is usually faster, more efficient and less error-prone to automate the task. Web scraping allows you to acquire non-tabular or poorly structured data from websites and convert it into a usable, structured format, such as a .csv file or spreadsheet. Scraping is about more than just acquiring data: it can also help you archive data and track changes to data online. It is closely related to the practice of web indexing, which is what search engines like Google do when mass-analysing the Web to build their indices. But contrary to web indexing, which typically parses the entire content of a web page to make it searchable, web scraping targets specific information on the pages visited. For example, online stores will often scour the publicly available pages of their competitors, scrape item prices, and then use this information to adjust their own prices. Another common practice is \u201ccontact scraping\u201d in which personal information like email addresses or phone numbers is collected for marketing purposes. Why do we need it as a skill? \u00b6 Web scraping is increasingly being used by academics and researchers to create data sets for text mining projects; these might be collections of journal articles or digitised texts. The practice of data journalism, in particular, relies on the ability of investigative journalists to harvest data that is not always presented or published in a form that allows analysis. When do we need scraping? \u00b6 As useful as scraping is, there might be better options for the task. Choose the right (i.e. the easiest) tool for the job. Check whether or not you can easily copy and paste data from a site into Excel or Google Sheets. This might be quicker than scraping. Check if the site or service already provides an API to extract structured data. If it does, that will be a much more efficient and effective pathway. Good examples are the Facebook API, the Twitter APIs or the YouTube comments API. For much larger needs, Freedom of information requests can be useful. Be specific about the formats required for the data you want. Challenge \u00b6 If you had to gather data from a website which provides updated figures every 4 hours on an ongoing pandemic, would you : Check their terms of service Scrape the site directly Ask for permission and then scrape the site Use an official API (if it exists) that might have limitations Structured vs unstructured data \u00b6 When presented with information, human beings are good at quickly categorizing it and extracting the data that they are interested in. For example, when we look at a magazine rack, provided the titles are written in a script that we are able to read, we can rapidly figure out the titles of the magazines, the stories they contain, the language they are written in, etc. and we can probably also easily organize them by topic, recognize those that are aimed at children, or even whether they lean toward a particular end of the political spectrum. Computers have a much harder time making sense of such unstructured data unless we specifically tell them what elements data is made of, for example by adding labels such as this is the title of this magazine or this is a magazine about food. Data in which individual elements are separated and labelled is said to be structured. We see that this data has been structured for displaying purposes (it is arranged in rows inside a table) but the different elements of information are not clearly labelled. What if we wanted to download this dataset and, for example, compare the revenues of these companies against each other or the industry that they work in? We could try copy-pasting the entire table into a spreadsheet or even manually copy-pasting the names and websites in another document, but this can quickly become impractical when faced with a large set of data. What if we wanted to collect this information for all the companies that are there? Fortunately, there are tools to automate at least part of the process. This technique is called web scraping. From Wikipedia, \" Web scraping (web harvesting or web data extraction) is a computer software technique of extracting information from websites. \" Web scraping typically targets one web site at a time to extract unstructured information and put it in a structured form for reuse. In this lesson, we will continue exploring the examples above and try different techniques to extract the information they contain. But before we launch into web scraping proper, we need to look a bit closer at how information is organized within an HTML document and how to build queries to access a specific subset of that information. Challenge \u00b6 Which of the following would you consider to be structure and unstructured data? A. 1 \"The latest figures showed that webscraper INC saw a 120 % i ncrease in their revenue bringing their market cap to 2 Billion Dollars. This could be attributed to their new policies.\" B. 1 2 3 4 5 < company > < name > webscraper INC </ name > < revenue > 120% </ revenue > < marketcap > 2 billion </ marketcap > </ company > C. 1 2 3 4 5 { 'company_name' : 'webscraper INC' , 'revenue_in_%)' : 120 , 'market_cap' : '2 billion USD' } What is HTML? \u00b6 HTML stands for HyperText Markup Language It is the standard markup language for the webpages which make up the internet. HTML contains a series of elements which make up a webpage which can connect with other webpages altogether forming a website. The HTML elements are represented in tags which tell the web browser how to display the web content. A sample raw HTML file below : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 <!DOCTYPE html> < html > < head > < title > My Title </ title > </ head > < body > < h1 > A Heading </ h1 > < a href = \"#\" > Link text </ a > </ body > </ html > A webpage is simply a document. Every HTML element within this document corresponds to display specific content on the web browser. The following image shows the HTML code and the webpage generated (please refer to intro_html_example.html ). What is XML? \u00b6 XML stands for eXtensible Markup Language XML is a markup language much like HTML XML was designed to store and transport data XML was designed to be self-descriptive 1 2 3 4 5 6 7 <note> <date> 2015-09-01 </date> <hour> 08:30 </hour> <to> Tove </to> <from> Jani </from> <body> Don't forget me this weekend! </body> </note> HTML DOM (or Document Object Model) \u00b6 From the World Wide Web Consortium (W3C), \" The W3C Document Object Model (DOM) is a platform and language-neutral interface that allows programs and scripts to dynamically access and update the content, structure, and style of a document. \" Everytime a web page is loaded in the browser, it creates a D ocument O bject M odel of the page. It essentially treats the HTML (or XML) document as a tree structure and the different HTML elements are represented as nodes and objects. More broadly, it is a programming interface for HTML and XML documents and can be considered as the object-oriented representation of a web page which can be modified with a scripting language like JavaScript. It also provides us with a rich visual representation of how the different elements interact and inform us about their relative position within the tree. This helps us find and target crucial tags , id or classes within the document and extract the same. To sumarize, DOM is a standard which allows us to : get change add , or delete HTML elements. Here we will be primarily interested in accessing and getting the data as opposed to manipulation of the document itself. Let's look at the DOM for the HTML from our previous example below The next question then is : How do we access the source code or DOM of any web page on the internet? DOM inspector and F12 to the rescue! \u00b6 To inspect individual elements within a web page, we can simply use the DOM inspector (or its variants) that comes with every browser. Easiest way to access the source code of any web page is through the console by clicking F12 Alternatively, we can right-click on a specific element in the webpage and select inspect or inspect element from the dropdown. This is especially useful in cases where we want to target a specific piece of data present within some HTML element. It helps highlight different attributes, properties and styles within the HTML It is known as DOM inspector and Developers Tools in Firefox and Chrome respectively. Note : Some webpages prohibit right-click and in those cases we might have to resort to inspecting the source code via F12. A Google Chrome window along with the developer console accessed though F12 (found under Developers Tool ) below References \u00b6 https://xkcd.com/2054/ https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Introduction https://en.wikipedia.org/wiki/Document_Object_Model https://www.w3schools.com/html/ https://www.w3schools.com/js/js_htmldom.asp","title":"Introduction to Web scraping"},{"location":"section-1-intro-to-web-scraping/#what-is-web-scraping","text":"Web scraping is a technique for extracting information from websites. This can be done manually but it is usually faster, more efficient and less error-prone to automate the task. Web scraping allows you to acquire non-tabular or poorly structured data from websites and convert it into a usable, structured format, such as a .csv file or spreadsheet. Scraping is about more than just acquiring data: it can also help you archive data and track changes to data online. It is closely related to the practice of web indexing, which is what search engines like Google do when mass-analysing the Web to build their indices. But contrary to web indexing, which typically parses the entire content of a web page to make it searchable, web scraping targets specific information on the pages visited. For example, online stores will often scour the publicly available pages of their competitors, scrape item prices, and then use this information to adjust their own prices. Another common practice is \u201ccontact scraping\u201d in which personal information like email addresses or phone numbers is collected for marketing purposes.","title":"What is web scraping?"},{"location":"section-1-intro-to-web-scraping/#why-do-we-need-it-as-a-skill","text":"Web scraping is increasingly being used by academics and researchers to create data sets for text mining projects; these might be collections of journal articles or digitised texts. The practice of data journalism, in particular, relies on the ability of investigative journalists to harvest data that is not always presented or published in a form that allows analysis.","title":"Why do we need it as a skill?"},{"location":"section-1-intro-to-web-scraping/#when-do-we-need-scraping","text":"As useful as scraping is, there might be better options for the task. Choose the right (i.e. the easiest) tool for the job. Check whether or not you can easily copy and paste data from a site into Excel or Google Sheets. This might be quicker than scraping. Check if the site or service already provides an API to extract structured data. If it does, that will be a much more efficient and effective pathway. Good examples are the Facebook API, the Twitter APIs or the YouTube comments API. For much larger needs, Freedom of information requests can be useful. Be specific about the formats required for the data you want.","title":"When do we need scraping?"},{"location":"section-1-intro-to-web-scraping/#challenge","text":"If you had to gather data from a website which provides updated figures every 4 hours on an ongoing pandemic, would you : Check their terms of service Scrape the site directly Ask for permission and then scrape the site Use an official API (if it exists) that might have limitations","title":"Challenge"},{"location":"section-1-intro-to-web-scraping/#structured-vs-unstructured-data","text":"When presented with information, human beings are good at quickly categorizing it and extracting the data that they are interested in. For example, when we look at a magazine rack, provided the titles are written in a script that we are able to read, we can rapidly figure out the titles of the magazines, the stories they contain, the language they are written in, etc. and we can probably also easily organize them by topic, recognize those that are aimed at children, or even whether they lean toward a particular end of the political spectrum. Computers have a much harder time making sense of such unstructured data unless we specifically tell them what elements data is made of, for example by adding labels such as this is the title of this magazine or this is a magazine about food. Data in which individual elements are separated and labelled is said to be structured. We see that this data has been structured for displaying purposes (it is arranged in rows inside a table) but the different elements of information are not clearly labelled. What if we wanted to download this dataset and, for example, compare the revenues of these companies against each other or the industry that they work in? We could try copy-pasting the entire table into a spreadsheet or even manually copy-pasting the names and websites in another document, but this can quickly become impractical when faced with a large set of data. What if we wanted to collect this information for all the companies that are there? Fortunately, there are tools to automate at least part of the process. This technique is called web scraping. From Wikipedia, \" Web scraping (web harvesting or web data extraction) is a computer software technique of extracting information from websites. \" Web scraping typically targets one web site at a time to extract unstructured information and put it in a structured form for reuse. In this lesson, we will continue exploring the examples above and try different techniques to extract the information they contain. But before we launch into web scraping proper, we need to look a bit closer at how information is organized within an HTML document and how to build queries to access a specific subset of that information.","title":"Structured vs unstructured data"},{"location":"section-1-intro-to-web-scraping/#challenge_1","text":"Which of the following would you consider to be structure and unstructured data? A. 1 \"The latest figures showed that webscraper INC saw a 120 % i ncrease in their revenue bringing their market cap to 2 Billion Dollars. This could be attributed to their new policies.\" B. 1 2 3 4 5 < company > < name > webscraper INC </ name > < revenue > 120% </ revenue > < marketcap > 2 billion </ marketcap > </ company > C. 1 2 3 4 5 { 'company_name' : 'webscraper INC' , 'revenue_in_%)' : 120 , 'market_cap' : '2 billion USD' }","title":"Challenge"},{"location":"section-1-intro-to-web-scraping/#what-is-html","text":"HTML stands for HyperText Markup Language It is the standard markup language for the webpages which make up the internet. HTML contains a series of elements which make up a webpage which can connect with other webpages altogether forming a website. The HTML elements are represented in tags which tell the web browser how to display the web content. A sample raw HTML file below : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 <!DOCTYPE html> < html > < head > < title > My Title </ title > </ head > < body > < h1 > A Heading </ h1 > < a href = \"#\" > Link text </ a > </ body > </ html > A webpage is simply a document. Every HTML element within this document corresponds to display specific content on the web browser. The following image shows the HTML code and the webpage generated (please refer to intro_html_example.html ).","title":"What is HTML?"},{"location":"section-1-intro-to-web-scraping/#what-is-xml","text":"XML stands for eXtensible Markup Language XML is a markup language much like HTML XML was designed to store and transport data XML was designed to be self-descriptive 1 2 3 4 5 6 7 <note> <date> 2015-09-01 </date> <hour> 08:30 </hour> <to> Tove </to> <from> Jani </from> <body> Don't forget me this weekend! </body> </note>","title":"What is XML?"},{"location":"section-1-intro-to-web-scraping/#html-dom-or-document-object-model","text":"From the World Wide Web Consortium (W3C), \" The W3C Document Object Model (DOM) is a platform and language-neutral interface that allows programs and scripts to dynamically access and update the content, structure, and style of a document. \" Everytime a web page is loaded in the browser, it creates a D ocument O bject M odel of the page. It essentially treats the HTML (or XML) document as a tree structure and the different HTML elements are represented as nodes and objects. More broadly, it is a programming interface for HTML and XML documents and can be considered as the object-oriented representation of a web page which can be modified with a scripting language like JavaScript. It also provides us with a rich visual representation of how the different elements interact and inform us about their relative position within the tree. This helps us find and target crucial tags , id or classes within the document and extract the same. To sumarize, DOM is a standard which allows us to : get change add , or delete HTML elements. Here we will be primarily interested in accessing and getting the data as opposed to manipulation of the document itself. Let's look at the DOM for the HTML from our previous example below The next question then is : How do we access the source code or DOM of any web page on the internet?","title":"HTML DOM (or Document Object Model)"},{"location":"section-1-intro-to-web-scraping/#dom-inspector-and-f12-to-the-rescue","text":"To inspect individual elements within a web page, we can simply use the DOM inspector (or its variants) that comes with every browser. Easiest way to access the source code of any web page is through the console by clicking F12 Alternatively, we can right-click on a specific element in the webpage and select inspect or inspect element from the dropdown. This is especially useful in cases where we want to target a specific piece of data present within some HTML element. It helps highlight different attributes, properties and styles within the HTML It is known as DOM inspector and Developers Tools in Firefox and Chrome respectively. Note : Some webpages prohibit right-click and in those cases we might have to resort to inspecting the source code via F12. A Google Chrome window along with the developer console accessed though F12 (found under Developers Tool ) below","title":"DOM inspector and F12 to the rescue!"},{"location":"section-1-intro-to-web-scraping/#references","text":"https://xkcd.com/2054/ https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Introduction https://en.wikipedia.org/wiki/Document_Object_Model https://www.w3schools.com/html/ https://www.w3schools.com/js/js_htmldom.asp","title":"References"},{"location":"section-2-HTML-based-scraping/","text":"In this section we will look at a few HTML fundamentals to understand how web scraping works and scrape a website for structured data URL request and response \u00b6 A URL is Uniform Resource Locator. It acts as a web address to different webpages. Every URL on the internet work on a request-response basis. The browser requests the server for a webpage and the response by the server would be the content of the webpage. This web content is then displayed on the browser. URL Request - Requesting a web server for content to be viewed by the user. This request is triggered whenever you click on a link or open a webpage. URL Response - A response for the request irrespective of success or failure. For every request to the web server, a mandatory response is provided by the web server and most of the times this would be the respective content requested by the URL Request. HTTP request, response, headers and status codes \u00b6 Instead of the browser requesting for the content of the webpage, Python can be used for the same. A HTTP request to the web server can be sent with the requests library and the response can be examined. Typically every request receives a response with response headers and status code details. Let us request for the web content for the Monash University front webpage with the URL - https://www.monash.edu/ . The requests library can be used to work with webpages and web content. A request is made to get the content of the webpage with the get() method. 1 2 3 4 import requests monash_web_url = \"https://www.monash.edu/\" response = requests . get ( monash_web_url ) A response is received from the web server. This response will have response headers and status codes associated to that particular request. Response headers give the detailed information about the request made to the web server. 1 response . headers 1 {'Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Vary': 'Accept-Encoding, Accept-Encoding', 'Cache-Control': 'max-age=0, private', 'X-Content-Type-Options': 'nosniff', 'X-Cache': 'HIT from squizedge.net', 'Pragma': 'cache', 'Age': '89', 'Server': 'openresty', 'Via': '1.1 squizedge.net', 'Date': 'Thu, 16 Jul 2020 10:09:08 GMT', 'X-upgrade-enabled': 'off', 'X-Frame-Options': 'SAMEORIGIN', 'Expires': 'Thu, 16 Jul 2020 10:39:08 GMT', 'X-Request-ID': '417b2838-f05d-49fe-912f-e2ce77294f3d', 'Content-Encoding': 'gzip'} Every response will have a status code. The status codes indicate whether a specific HTTP request has been successfully completed. Responses are grouped in five classes: Informational responses (100\u2013199) Successful responses (200\u2013299) Redirects (300\u2013399) Client errors (400\u2013499) Server errors (500\u2013599) Let us check the response status code for the HTTP request we placed 1 response . status_code 1 200 The response has a status code of 200 . This is a successful response and hence there should be relevant content of the webpage in the obtained response. This can be checked by printing the content. This content received is the HTML source code of the webpage. 1 response . content [: 2000 ] # check the end index 1 b'<!DOCTYPE html>\\n<html lang=\"en\">\\n <head>\\n <meta charset=\"utf-8\" />\\n <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n <title>Monash University - one of the top universities in Australia</title>\\n\\n <link rel=\"canonical\" href=\"https://www.monash.edu\" />\\n \\n \\n \\r\\n<!-- For iPad with high-resolution Retina display running iOS \\xe2\\x89\\xa5 7: -->\\r\\n<link rel=\"apple-touch-icon-precomposed\" sizes=\"152x152\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-152.png\">\\r\\n\\r\\n<!-- For iPad with high-resolution Retina display running iOS \\xe2\\x89\\xa4 6: -->\\r\\n<link rel=\"apple-touch-icon-precomposed\" sizes=\"144x144\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-144.png\">\\r\\n\\r\\n<!-- For iPhone with high-resolution Retina display running iOS \\xe2\\x89\\xa5 7: -->\\r\\n<link rel=\"apple-touch-icon-precomposed\" sizes=\"120x120\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-120.png\">\\r\\n\\r\\n<!-- For iPhone with high-resolution Retina display running iOS \\xe2\\x89\\xa4 6: -->\\r\\n<link rel=\"apple-touch-icon-precomposed\" sizes=\"114x114\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-114.png\">\\r\\n\\r\\n<!-- For first- and second-generation iPad: -->\\r\\n<link rel=\"apple-touch-icon-precomposed\" sizes=\"72x72\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-72.png\">\\r\\n\\r\\n<!-- For non-Retina iPhone, iPod Touch, and Android 2.1+ devices: -->\\r\\n<link rel=\"apple-touch-icon-precomposed\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-57.png\">\\r\\n\\r\\n<link rel=\"icon\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon.ico\">\\r\\n<!--[if IE]><link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favico' This text which looks very gibberish actually has a structure underneath. This can be observed if printed on a HTML editor. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 <!DOCTYPE html> \\n < html lang = \"en\" > \\n < head > \\n < meta charset = \"utf-8\" /> \\n < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" > \\n < title > Monash University - one of the top universities in Australia </ title > \\n\\n < link rel = \"canonical\" href = \"https://www.monash.edu\" /> \\n \\n \\n \\r\\n <!-- For iPad with high-resolution Retina display running iOS \\xe2\\x89\\xa5 7: --> \\r\\n < link rel = \"apple-touch-icon-precomposed\" sizes = \"152x152\" href = \"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-152.png\" > \\r\\n\\r\\n <!-- For iPad with high-resolution Retina display running iOS \\xe2\\x89\\xa4 6: --> \\r\\n < link rel = \"apple-touch-icon-precomposed\" sizes = \"144x144\" href = \"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-144.png\" > \\r\\n\\r\\n <!-- For iPhone with high-resolution Retina display running iOS \\xe2\\x89\\xa5 7: --> \\r\\n < link rel = \"apple-touch-icon-precomposed\" sizes = \"120x120\" href = \"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-120.png\" > \\r\\n\\r\\n <!-- For iPhone with high-resolution Retina display running iOS \\xe2\\x89\\xa4 6: --> \\r\\n < link rel = \"apple-touch-icon-precomposed\" sizes = \"114x114\" href = \"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-114.png\" > \\r\\n\\r\\n <!-- For first- and second-generation iPad: --> \\r\\n < link rel = \"apple-touch-icon-precomposed\" sizes = \"72x72\" href = \"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-72.png\" > \\r\\n\\r\\n <!-- For non-Retina iPhone, iPod Touch, and Android 2.1+ devices: --> \\r\\n < link rel = \"apple-touch-icon-precomposed\" href = \"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-57.png\" > \\r\\n\\r\\n < link rel = \"icon\" href = \"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon.ico\" > \\r\\n <!--[if IE]><link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favico GET and POST calls to retrieve response \u00b6 There are mainly two types of requests which can be made to the web server. A GET request/call or a POST request/call. GET call - GET is used to request data from a specified source. They are one of the most common HTTP requests. They are usually used to only receive content from the web server. An example would be to receive the content of the complete webpage. POST call - POST is used to send data in the URL request to either update details or request specific content from the web server. In a POST call, data is sent and then a response can be expected from the web server. An example would be to request content from a web server based on a particular selection from a drop-down menu. The selection option is upadted while also respective content is sent back. Scraping a webpage \u00b6 Let us now scrape a list of the fotune 500 companies for the year 2018 . The website from which the data is to be scraped is this . It can be seen on this website that the list contains the rank, company name and the website of the company. The whole content of this website can be received as a response when requested with the request library in Python 1 2 3 4 5 6 7 8 9 10 import requests import pandas as pd from bs4 import BeautifulSoup web_url = 'https://www.zyxware.com/articles/5914/list-of-fortune-500-companies-and-their-websites-2018' response = requests . get ( web_url ) print ( 'Status code \\n ' , response . status_code ) print ( ' \\n -- \\n ' ) print ( 'Content of the website \\n ' , response . content [: 2000 ]) 1 2 3 4 5 6 7 Status code 200 -- Content of the website b'<!DOCTYPE html>\\n<html lang=\"en\" dir=\"ltr\" prefix=\"content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ og: http://ogp.me/ns# rdfs: http://www.w3.org/2000/01/rdf-schema# schema: http://schema.org/ sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema# \">\\n <head>\\n <meta charset=\"utf-8\" />\\n<script>dataLayer = [];dataLayer.push({\"tag\": \"5914\"});</script>\\n<script>window.dataLayer = window.dataLayer || []; window.dataLayer.push({\"drupalLanguage\":\"en\",\"drupalCountry\":\"IN\",\"siteName\":\"Zyxware Technologies\",\"entityCreated\":\"1562300185\",\"entityLangcode\":\"en\",\"entityStatus\":\"1\",\"entityUid\":\"1\",\"entityUuid\":\"6fdfb477-ce5d-4081-9010-3afd9260cdf7\",\"entityVid\":\"15541\",\"entityName\":\"webmaster\",\"entityType\":\"node\",\"entityBundle\":\"story\",\"entityId\":\"5914\",\"entityTitle\":\"List of Fortune 500 companies and their websites (2018)\",\"entityTaxonomy\":{\"vocabulary_2\":\"Business Insight, Fortune 500, Drupal Insight, Marketing Resources\"},\"userUid\":0});</script>\\n<script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-1488254-2\"></script>\\n<script>window.google_analytics_uacct = \"UA-1488254-2\";window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments)};gtag(\"js\", new Date());window[\\'GoogleAnalyticsObject\\'] = \\'ga\\';\\r\\n window[\\'ga\\'] = window[\\'ga\\'] || function() {\\r\\n (window[\\'ga\\'].q = window[\\'ga\\'].q || []).push(arguments)\\r\\n };\\r\\nga(\"set\", \"dimension2\", window.analytics_manager_node_age);\\r\\nga(\"set\", \"dimension3\", window.analytics_manager_node_author);gtag(\"config\", \"UA-1488254-2\", {\"groups\":\"default\",\"anonymize_ip\":true,\"page_path\":location.pathname + location.search + location.hash,\"link_attribution\":true,\"allow_ad_personalization_signals\":false});</script>\\n<script>(function(w,d,t,u,n,a,m){w[\\'MauticTrackingObject\\']=n;w[n]=w[n]||function(){(w[n].q=w[n].q||[]).push(arguments)},a=d.createElement(t),m=d.ge' This text when formatted looks like this, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 <!DOCTYPE html> \\n < html lang = \"en\" dir = \"ltr\" prefix = \"content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ og: http://ogp.me/ns# rdfs: http://www.w3.org/2000/01/rdf-schema# schema: http://schema.org/ sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema# \" > \\n < head > \\n < meta charset = \"utf-8\" /> \\n < script > dataLayer = []; dataLayer . push ({ \"tag\" : \"5914\" }); </ script > \\n < script > window . dataLayer = window . dataLayer || []; window . dataLayer . push ({ \"drupalLanguage\" : \"en\" , \"drupalCountry\" : \"IN\" , \"siteName\" : \"Zyxware Technologies\" , \"entityCreated\" : \"1562300185\" , \"entityLangcode\" : \"en\" , \"entityStatus\" : \"1\" , \"entityUid\" : \"1\" , \"entityUuid\" : \"6fdfb477-ce5d-4081-9010-3afd9260cdf7\" , \"entityVid\" : \"15541\" , \"entityName\" : \"webmaster\" , \"entityType\" : \"node\" , \"entityBundle\" : \"story\" , \"entityId\" : \"5914\" , \"entityTitle\" : \"List of Fortune 500 companies and their websites (2018)\" , \"entityTaxonomy\" : { \"vocabulary_2\" : \"Business Insight, Fortune 500, Drupal Insight, Marketing Resources\" }, \"userUid\" : 0 }); </ script > \\n < script async src = \"https://www.googletagmanager.com/gtag/js?id=UA-1488254-2\" ></ script > \\n < script > window . google_analytics_uacct = \"UA-1488254-2\" ; window . dataLayer = window . dataLayer || []; function gtag () { dataLayer . push ( arguments ) }; gtag ( \"js\" , new Date ()); window [ \\ 'GoogleAnalyticsObject\\'] = \\'ga\\';\\r\\n window[\\'ga\\'] = window[\\'ga\\'] || function() {\\r\\n (window[\\'ga\\'].q = window[\\'ga\\' ]. q || []). push ( arguments ) \\ r \\ n }; \\ r \\ nga ( \"set\" , \"dimension2\" , window . analytics_manager_node_age ); \\ r \\ nga ( \"set\" , \"dimension3\" , window . analytics_manager_node_author ); gtag ( \"config\" , \"UA-1488254-2\" , { \"groups\" : \"default\" , \"anonymize_ip\" : true , \"page_path\" : location . pathname + location . search + location . hash , \"link_attribution\" : true , \"allow_ad_personalization_signals\" : false }); </ script > \\n < meta name = \"title\" content = \"List of Fortune 500 companies and their websites (2018) | Zyxware Technologies\" /> .. and so on Note : It is always a good idea to \"prettify\" HTML, XML or JSON strings for visual clarity. Parsing HTML and accessing different elements \u00b6 bs4 is a Python library which parses through HTML content and understands the complete structure of the content. The response content can be passed to a BeautifulSoup() method to obtain a soup object which looks very structured. 1 2 3 4 soup_object = BeautifulSoup ( response . content ) # Uncomment the below line and look into the contents of soup_object # soup_object Explore the schema: Manipulating it into a tabular structure \u00b6 To be able to accurately extract relevant data from the webpage, it is important to explore the schema and understand the structure of the webpage. A good way to do this is to inspect the webpage directly on a web browser. To do this, - Open the webpage on a browser - Right click on the data content to be extracted - Click on Inspect or Inspect element option This will open a console window which shows the real time HTML code corresponding to the web content. Now identify the type of HTML tag which contains all the data along with any id names or class names associated to that HTML tag. In our case, the data is enclosed in the <table> HTML tag with the class name 'data-table' . This information can be used to search for the web content directly in our soup object with the find_all() method. This will return a soup object. 1 2 3 4 data_table = soup_object . find_all ( 'table' , 'data-table' )[ 0 ] # Uncomment the below line and look into the contents of data_table # data_table It can be seen that relevant block of data has been extracted but further extracted needs to be done to individually extract the rank, company name and the company website data. On further analysis, it can be seen that every row of data is enclosed under a <tr> HTML tag which means table row . All these row values can be extracted into a list of values by finding the <tr> values from our newly created soup object data_table . 1 2 all_values = data_table . find_all ( 'tr' ) all_values [: 10 ] # Prints the first 10 captured tag elements 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 [<tr><th>Rank</th> <th>Company</th> <th>Website</th> </tr>, <tr><td>1</td> <td>Walmart</td> <td><a href=\"http://www.stock.walmart.com\">http://www.stock.walmart.com</a></td> </tr>, <tr><td>2</td> <td>Exxon Mobil</td> <td><a href=\"http://www.exxonmobil.com\">http://www.exxonmobil.com</a></td> </tr>, <tr><td>3</td> <td>Berkshire Hathaway</td> <td><a href=\"http://www.berkshirehathaway.com\">http://www.berkshirehathaway.com</a></td> </tr>, <tr><td>4</td> <td>Apple</td> <td><a href=\"http://www.apple.com\">http://www.apple.com</a></td> </tr>, <tr><td>5</td> <td>UnitedHealth Group</td> <td><a href=\"http://www.unitedhealthgroup.com\">http://www.unitedhealthgroup.com</a></td> </tr>, <tr><td>6</td> <td>McKesson</td> <td><a href=\"http://www.mckesson.com\">http://www.mckesson.com</a></td> </tr>, <tr><td>7</td> <td>CVS Health</td> <td><a href=\"http://www.cvshealth.com\">http://www.cvshealth.com</a></td> </tr>, <tr><td>8</td> <td>Amazon.com</td> <td><a href=\"http://www.amazon.com\">http://www.amazon.com</a></td> </tr>, <tr><td>9</td> <td>AT&amp;T</td> <td><a href=\"http://www.att.com\">http://www.att.com</a></td> </tr>] 1 2 3 4 5 print ( all_values [ 0 ]) print ( '--' ) print ( all_values [ 1 ]) print ( '--' ) print ( all_values [ 2 ]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 <tr><th>Rank</th> <th>Company</th> <th>Website</th> </tr> -- <tr><td>1</td> <td>Walmart</td> <td><a href=\"http://www.stock.walmart.com\">http://www.stock.walmart.com</a></td> </tr> -- <tr><td>2</td> <td>Exxon Mobil</td> <td><a href=\"http://www.exxonmobil.com\">http://www.exxonmobil.com</a></td> </tr> Challenge \u00b6 Explore the schema further, extract the column names which are located in the first row and print them. The first element of the list contains the column names 'Rank, Company and Website'. The next elements of the list contain soup objects which contain the company data including the rank. This data can be extracted in a loop since the structure for all the list elements is the same. An empty dataframe fortune_500_df is created with the column names rank , company_name and company_website The index is initiated to zero A for loop is designed to go through all the elements of the list in order and extract the rank, company name and company website from the list element which are enclosed in the <td> HTML tag. A find_all() will return a list of td tags. The .text attribute can be used to just pick the text part from the tag. In our case this is the rank, company name and the company website These values are then put into the dataframe and the index value is incremented 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 fortune_500_df = pd . DataFrame ( columns = [ 'rank' , 'company_name' , 'company_website' ]) # Create an empty dataframe ix = 0 # Initialise index to zero for row in all_values [ 1 :]: values = row . find_all ( 'td' ) # Extract all elements with tag <td> # Pick only the text part from the <td> tag rank = values [ 0 ] . text company = values [ 1 ] . text website = values [ 2 ] . text fortune_500_df . loc [ ix ] = [ rank , company , website ] # Store it in the dataframe as a row ix += 1 # Print the first 5 rows of the dataframe fortune_500_df . head () rank company_name company_website 0 1 Walmart http://www.stock.walmart.com 1 2 Exxon Mobil http://www.exxonmobil.com 2 3 Berkshire Hathaway http://www.berkshirehathaway.com 3 4 Apple http://www.apple.com 4 5 UnitedHealth Group http://www.unitedhealthgroup.com Store it in the appropriate format \u00b6 The dataframe can now be stored as a csv file. Pandas has a to_csv method which can be used to save the data into the file. 1 fortune_500_df . to_csv ( 'fortune_500_companies.csv' , index = False )","title":"HTML based scraping"},{"location":"section-2-HTML-based-scraping/#url-request-and-response","text":"A URL is Uniform Resource Locator. It acts as a web address to different webpages. Every URL on the internet work on a request-response basis. The browser requests the server for a webpage and the response by the server would be the content of the webpage. This web content is then displayed on the browser. URL Request - Requesting a web server for content to be viewed by the user. This request is triggered whenever you click on a link or open a webpage. URL Response - A response for the request irrespective of success or failure. For every request to the web server, a mandatory response is provided by the web server and most of the times this would be the respective content requested by the URL Request.","title":"URL request and response"},{"location":"section-2-HTML-based-scraping/#http-request-response-headers-and-status-codes","text":"Instead of the browser requesting for the content of the webpage, Python can be used for the same. A HTTP request to the web server can be sent with the requests library and the response can be examined. Typically every request receives a response with response headers and status code details. Let us request for the web content for the Monash University front webpage with the URL - https://www.monash.edu/ . The requests library can be used to work with webpages and web content. A request is made to get the content of the webpage with the get() method. 1 2 3 4 import requests monash_web_url = \"https://www.monash.edu/\" response = requests . get ( monash_web_url ) A response is received from the web server. This response will have response headers and status codes associated to that particular request. Response headers give the detailed information about the request made to the web server. 1 response . headers 1 {'Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Vary': 'Accept-Encoding, Accept-Encoding', 'Cache-Control': 'max-age=0, private', 'X-Content-Type-Options': 'nosniff', 'X-Cache': 'HIT from squizedge.net', 'Pragma': 'cache', 'Age': '89', 'Server': 'openresty', 'Via': '1.1 squizedge.net', 'Date': 'Thu, 16 Jul 2020 10:09:08 GMT', 'X-upgrade-enabled': 'off', 'X-Frame-Options': 'SAMEORIGIN', 'Expires': 'Thu, 16 Jul 2020 10:39:08 GMT', 'X-Request-ID': '417b2838-f05d-49fe-912f-e2ce77294f3d', 'Content-Encoding': 'gzip'} Every response will have a status code. The status codes indicate whether a specific HTTP request has been successfully completed. Responses are grouped in five classes: Informational responses (100\u2013199) Successful responses (200\u2013299) Redirects (300\u2013399) Client errors (400\u2013499) Server errors (500\u2013599) Let us check the response status code for the HTTP request we placed 1 response . status_code 1 200 The response has a status code of 200 . This is a successful response and hence there should be relevant content of the webpage in the obtained response. This can be checked by printing the content. This content received is the HTML source code of the webpage. 1 response . content [: 2000 ] # check the end index 1 b'<!DOCTYPE html>\\n<html lang=\"en\">\\n <head>\\n <meta charset=\"utf-8\" />\\n <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n <title>Monash University - one of the top universities in Australia</title>\\n\\n <link rel=\"canonical\" href=\"https://www.monash.edu\" />\\n \\n \\n \\r\\n<!-- For iPad with high-resolution Retina display running iOS \\xe2\\x89\\xa5 7: -->\\r\\n<link rel=\"apple-touch-icon-precomposed\" sizes=\"152x152\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-152.png\">\\r\\n\\r\\n<!-- For iPad with high-resolution Retina display running iOS \\xe2\\x89\\xa4 6: -->\\r\\n<link rel=\"apple-touch-icon-precomposed\" sizes=\"144x144\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-144.png\">\\r\\n\\r\\n<!-- For iPhone with high-resolution Retina display running iOS \\xe2\\x89\\xa5 7: -->\\r\\n<link rel=\"apple-touch-icon-precomposed\" sizes=\"120x120\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-120.png\">\\r\\n\\r\\n<!-- For iPhone with high-resolution Retina display running iOS \\xe2\\x89\\xa4 6: -->\\r\\n<link rel=\"apple-touch-icon-precomposed\" sizes=\"114x114\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-114.png\">\\r\\n\\r\\n<!-- For first- and second-generation iPad: -->\\r\\n<link rel=\"apple-touch-icon-precomposed\" sizes=\"72x72\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-72.png\">\\r\\n\\r\\n<!-- For non-Retina iPhone, iPod Touch, and Android 2.1+ devices: -->\\r\\n<link rel=\"apple-touch-icon-precomposed\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-57.png\">\\r\\n\\r\\n<link rel=\"icon\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon.ico\">\\r\\n<!--[if IE]><link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favico' This text which looks very gibberish actually has a structure underneath. This can be observed if printed on a HTML editor. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 <!DOCTYPE html> \\n < html lang = \"en\" > \\n < head > \\n < meta charset = \"utf-8\" /> \\n < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" > \\n < title > Monash University - one of the top universities in Australia </ title > \\n\\n < link rel = \"canonical\" href = \"https://www.monash.edu\" /> \\n \\n \\n \\r\\n <!-- For iPad with high-resolution Retina display running iOS \\xe2\\x89\\xa5 7: --> \\r\\n < link rel = \"apple-touch-icon-precomposed\" sizes = \"152x152\" href = \"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-152.png\" > \\r\\n\\r\\n <!-- For iPad with high-resolution Retina display running iOS \\xe2\\x89\\xa4 6: --> \\r\\n < link rel = \"apple-touch-icon-precomposed\" sizes = \"144x144\" href = \"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-144.png\" > \\r\\n\\r\\n <!-- For iPhone with high-resolution Retina display running iOS \\xe2\\x89\\xa5 7: --> \\r\\n < link rel = \"apple-touch-icon-precomposed\" sizes = \"120x120\" href = \"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-120.png\" > \\r\\n\\r\\n <!-- For iPhone with high-resolution Retina display running iOS \\xe2\\x89\\xa4 6: --> \\r\\n < link rel = \"apple-touch-icon-precomposed\" sizes = \"114x114\" href = \"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-114.png\" > \\r\\n\\r\\n <!-- For first- and second-generation iPad: --> \\r\\n < link rel = \"apple-touch-icon-precomposed\" sizes = \"72x72\" href = \"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-72.png\" > \\r\\n\\r\\n <!-- For non-Retina iPhone, iPod Touch, and Android 2.1+ devices: --> \\r\\n < link rel = \"apple-touch-icon-precomposed\" href = \"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon-57.png\" > \\r\\n\\r\\n < link rel = \"icon\" href = \"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favicon.ico\" > \\r\\n <!--[if IE]><link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"https://www.monash.edu/__data/assets/git_bridge/0006/509343/deploy/mysource_files/favico","title":"HTTP request, response, headers and status codes"},{"location":"section-2-HTML-based-scraping/#get-and-post-calls-to-retrieve-response","text":"There are mainly two types of requests which can be made to the web server. A GET request/call or a POST request/call. GET call - GET is used to request data from a specified source. They are one of the most common HTTP requests. They are usually used to only receive content from the web server. An example would be to receive the content of the complete webpage. POST call - POST is used to send data in the URL request to either update details or request specific content from the web server. In a POST call, data is sent and then a response can be expected from the web server. An example would be to request content from a web server based on a particular selection from a drop-down menu. The selection option is upadted while also respective content is sent back.","title":"GET and POST calls to retrieve response"},{"location":"section-2-HTML-based-scraping/#scraping-a-webpage","text":"Let us now scrape a list of the fotune 500 companies for the year 2018 . The website from which the data is to be scraped is this . It can be seen on this website that the list contains the rank, company name and the website of the company. The whole content of this website can be received as a response when requested with the request library in Python 1 2 3 4 5 6 7 8 9 10 import requests import pandas as pd from bs4 import BeautifulSoup web_url = 'https://www.zyxware.com/articles/5914/list-of-fortune-500-companies-and-their-websites-2018' response = requests . get ( web_url ) print ( 'Status code \\n ' , response . status_code ) print ( ' \\n -- \\n ' ) print ( 'Content of the website \\n ' , response . content [: 2000 ]) 1 2 3 4 5 6 7 Status code 200 -- Content of the website b'<!DOCTYPE html>\\n<html lang=\"en\" dir=\"ltr\" prefix=\"content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ og: http://ogp.me/ns# rdfs: http://www.w3.org/2000/01/rdf-schema# schema: http://schema.org/ sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema# \">\\n <head>\\n <meta charset=\"utf-8\" />\\n<script>dataLayer = [];dataLayer.push({\"tag\": \"5914\"});</script>\\n<script>window.dataLayer = window.dataLayer || []; window.dataLayer.push({\"drupalLanguage\":\"en\",\"drupalCountry\":\"IN\",\"siteName\":\"Zyxware Technologies\",\"entityCreated\":\"1562300185\",\"entityLangcode\":\"en\",\"entityStatus\":\"1\",\"entityUid\":\"1\",\"entityUuid\":\"6fdfb477-ce5d-4081-9010-3afd9260cdf7\",\"entityVid\":\"15541\",\"entityName\":\"webmaster\",\"entityType\":\"node\",\"entityBundle\":\"story\",\"entityId\":\"5914\",\"entityTitle\":\"List of Fortune 500 companies and their websites (2018)\",\"entityTaxonomy\":{\"vocabulary_2\":\"Business Insight, Fortune 500, Drupal Insight, Marketing Resources\"},\"userUid\":0});</script>\\n<script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-1488254-2\"></script>\\n<script>window.google_analytics_uacct = \"UA-1488254-2\";window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments)};gtag(\"js\", new Date());window[\\'GoogleAnalyticsObject\\'] = \\'ga\\';\\r\\n window[\\'ga\\'] = window[\\'ga\\'] || function() {\\r\\n (window[\\'ga\\'].q = window[\\'ga\\'].q || []).push(arguments)\\r\\n };\\r\\nga(\"set\", \"dimension2\", window.analytics_manager_node_age);\\r\\nga(\"set\", \"dimension3\", window.analytics_manager_node_author);gtag(\"config\", \"UA-1488254-2\", {\"groups\":\"default\",\"anonymize_ip\":true,\"page_path\":location.pathname + location.search + location.hash,\"link_attribution\":true,\"allow_ad_personalization_signals\":false});</script>\\n<script>(function(w,d,t,u,n,a,m){w[\\'MauticTrackingObject\\']=n;w[n]=w[n]||function(){(w[n].q=w[n].q||[]).push(arguments)},a=d.createElement(t),m=d.ge' This text when formatted looks like this, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 <!DOCTYPE html> \\n < html lang = \"en\" dir = \"ltr\" prefix = \"content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ og: http://ogp.me/ns# rdfs: http://www.w3.org/2000/01/rdf-schema# schema: http://schema.org/ sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema# \" > \\n < head > \\n < meta charset = \"utf-8\" /> \\n < script > dataLayer = []; dataLayer . push ({ \"tag\" : \"5914\" }); </ script > \\n < script > window . dataLayer = window . dataLayer || []; window . dataLayer . push ({ \"drupalLanguage\" : \"en\" , \"drupalCountry\" : \"IN\" , \"siteName\" : \"Zyxware Technologies\" , \"entityCreated\" : \"1562300185\" , \"entityLangcode\" : \"en\" , \"entityStatus\" : \"1\" , \"entityUid\" : \"1\" , \"entityUuid\" : \"6fdfb477-ce5d-4081-9010-3afd9260cdf7\" , \"entityVid\" : \"15541\" , \"entityName\" : \"webmaster\" , \"entityType\" : \"node\" , \"entityBundle\" : \"story\" , \"entityId\" : \"5914\" , \"entityTitle\" : \"List of Fortune 500 companies and their websites (2018)\" , \"entityTaxonomy\" : { \"vocabulary_2\" : \"Business Insight, Fortune 500, Drupal Insight, Marketing Resources\" }, \"userUid\" : 0 }); </ script > \\n < script async src = \"https://www.googletagmanager.com/gtag/js?id=UA-1488254-2\" ></ script > \\n < script > window . google_analytics_uacct = \"UA-1488254-2\" ; window . dataLayer = window . dataLayer || []; function gtag () { dataLayer . push ( arguments ) }; gtag ( \"js\" , new Date ()); window [ \\ 'GoogleAnalyticsObject\\'] = \\'ga\\';\\r\\n window[\\'ga\\'] = window[\\'ga\\'] || function() {\\r\\n (window[\\'ga\\'].q = window[\\'ga\\' ]. q || []). push ( arguments ) \\ r \\ n }; \\ r \\ nga ( \"set\" , \"dimension2\" , window . analytics_manager_node_age ); \\ r \\ nga ( \"set\" , \"dimension3\" , window . analytics_manager_node_author ); gtag ( \"config\" , \"UA-1488254-2\" , { \"groups\" : \"default\" , \"anonymize_ip\" : true , \"page_path\" : location . pathname + location . search + location . hash , \"link_attribution\" : true , \"allow_ad_personalization_signals\" : false }); </ script > \\n < meta name = \"title\" content = \"List of Fortune 500 companies and their websites (2018) | Zyxware Technologies\" /> .. and so on Note : It is always a good idea to \"prettify\" HTML, XML or JSON strings for visual clarity.","title":"Scraping a webpage"},{"location":"section-2-HTML-based-scraping/#parsing-html-and-accessing-different-elements","text":"bs4 is a Python library which parses through HTML content and understands the complete structure of the content. The response content can be passed to a BeautifulSoup() method to obtain a soup object which looks very structured. 1 2 3 4 soup_object = BeautifulSoup ( response . content ) # Uncomment the below line and look into the contents of soup_object # soup_object","title":"Parsing HTML and accessing different elements"},{"location":"section-2-HTML-based-scraping/#explore-the-schema-manipulating-it-into-a-tabular-structure","text":"To be able to accurately extract relevant data from the webpage, it is important to explore the schema and understand the structure of the webpage. A good way to do this is to inspect the webpage directly on a web browser. To do this, - Open the webpage on a browser - Right click on the data content to be extracted - Click on Inspect or Inspect element option This will open a console window which shows the real time HTML code corresponding to the web content. Now identify the type of HTML tag which contains all the data along with any id names or class names associated to that HTML tag. In our case, the data is enclosed in the <table> HTML tag with the class name 'data-table' . This information can be used to search for the web content directly in our soup object with the find_all() method. This will return a soup object. 1 2 3 4 data_table = soup_object . find_all ( 'table' , 'data-table' )[ 0 ] # Uncomment the below line and look into the contents of data_table # data_table It can be seen that relevant block of data has been extracted but further extracted needs to be done to individually extract the rank, company name and the company website data. On further analysis, it can be seen that every row of data is enclosed under a <tr> HTML tag which means table row . All these row values can be extracted into a list of values by finding the <tr> values from our newly created soup object data_table . 1 2 all_values = data_table . find_all ( 'tr' ) all_values [: 10 ] # Prints the first 10 captured tag elements 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 [<tr><th>Rank</th> <th>Company</th> <th>Website</th> </tr>, <tr><td>1</td> <td>Walmart</td> <td><a href=\"http://www.stock.walmart.com\">http://www.stock.walmart.com</a></td> </tr>, <tr><td>2</td> <td>Exxon Mobil</td> <td><a href=\"http://www.exxonmobil.com\">http://www.exxonmobil.com</a></td> </tr>, <tr><td>3</td> <td>Berkshire Hathaway</td> <td><a href=\"http://www.berkshirehathaway.com\">http://www.berkshirehathaway.com</a></td> </tr>, <tr><td>4</td> <td>Apple</td> <td><a href=\"http://www.apple.com\">http://www.apple.com</a></td> </tr>, <tr><td>5</td> <td>UnitedHealth Group</td> <td><a href=\"http://www.unitedhealthgroup.com\">http://www.unitedhealthgroup.com</a></td> </tr>, <tr><td>6</td> <td>McKesson</td> <td><a href=\"http://www.mckesson.com\">http://www.mckesson.com</a></td> </tr>, <tr><td>7</td> <td>CVS Health</td> <td><a href=\"http://www.cvshealth.com\">http://www.cvshealth.com</a></td> </tr>, <tr><td>8</td> <td>Amazon.com</td> <td><a href=\"http://www.amazon.com\">http://www.amazon.com</a></td> </tr>, <tr><td>9</td> <td>AT&amp;T</td> <td><a href=\"http://www.att.com\">http://www.att.com</a></td> </tr>] 1 2 3 4 5 print ( all_values [ 0 ]) print ( '--' ) print ( all_values [ 1 ]) print ( '--' ) print ( all_values [ 2 ]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 <tr><th>Rank</th> <th>Company</th> <th>Website</th> </tr> -- <tr><td>1</td> <td>Walmart</td> <td><a href=\"http://www.stock.walmart.com\">http://www.stock.walmart.com</a></td> </tr> -- <tr><td>2</td> <td>Exxon Mobil</td> <td><a href=\"http://www.exxonmobil.com\">http://www.exxonmobil.com</a></td> </tr>","title":"Explore the schema: Manipulating it into a tabular structure"},{"location":"section-2-HTML-based-scraping/#challenge","text":"Explore the schema further, extract the column names which are located in the first row and print them. The first element of the list contains the column names 'Rank, Company and Website'. The next elements of the list contain soup objects which contain the company data including the rank. This data can be extracted in a loop since the structure for all the list elements is the same. An empty dataframe fortune_500_df is created with the column names rank , company_name and company_website The index is initiated to zero A for loop is designed to go through all the elements of the list in order and extract the rank, company name and company website from the list element which are enclosed in the <td> HTML tag. A find_all() will return a list of td tags. The .text attribute can be used to just pick the text part from the tag. In our case this is the rank, company name and the company website These values are then put into the dataframe and the index value is incremented 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 fortune_500_df = pd . DataFrame ( columns = [ 'rank' , 'company_name' , 'company_website' ]) # Create an empty dataframe ix = 0 # Initialise index to zero for row in all_values [ 1 :]: values = row . find_all ( 'td' ) # Extract all elements with tag <td> # Pick only the text part from the <td> tag rank = values [ 0 ] . text company = values [ 1 ] . text website = values [ 2 ] . text fortune_500_df . loc [ ix ] = [ rank , company , website ] # Store it in the dataframe as a row ix += 1 # Print the first 5 rows of the dataframe fortune_500_df . head () rank company_name company_website 0 1 Walmart http://www.stock.walmart.com 1 2 Exxon Mobil http://www.exxonmobil.com 2 3 Berkshire Hathaway http://www.berkshirehathaway.com 3 4 Apple http://www.apple.com 4 5 UnitedHealth Group http://www.unitedhealthgroup.com","title":"Challenge"},{"location":"section-2-HTML-based-scraping/#store-it-in-the-appropriate-format","text":"The dataframe can now be stored as a csv file. Pandas has a to_csv method which can be used to save the data into the file. 1 fortune_500_df . to_csv ( 'fortune_500_companies.csv' , index = False )","title":"Store it in the appropriate format"},{"location":"section-3-API-based-scraping/","text":"A brief introduction to APIs \u00b6 In this section, we will take a look at an alternative way to gather data than the previous pattern based HTML scraping. Sometimes websites offer an API (or Application Programming Interface) as a service which provides a high level interface to directly retrieve data from their repositories or databases at the backend. From Wikipedia, \" An API is typically defined as a set of specifications, such as Hypertext Transfer Protocol (HTTP) request messages, along with a definition of the structure of response messages, usually in an Extensible Markup Language (XML) or JavaScript Object Notation (JSON) format. \" They typically tend to be URL endpoints (to be fired as requests) that need to be modified based on our requirements (what we desire in the response body) which then returns some a payload (data) within the response, formatted as either JSON, XML or HTML. A popular web architecture style called REST (or representational state transfer) allows users to interact with web services via GET and POST calls (two most commonly used) which we briefly saw in the previous section. For example, Twitter's REST API allows developers to access core Twitter data and the Search API provides methods for developers to interact with Twitter Search and trends data. There are primarily two ways to use APIs : Through the command terminal using URL endpoints, or Through programming language specific wrappers For example, Tweepy is a famous python wrapper for Twitter API whereas twurl is a command line interface (CLI) tool but both can achieve the same outcomes. Here we focus on the latter approach and will use a Python library (a wrapper) called wptools based around the original MediaWiki API. One advantage of using official APIs is that they are usually compliant of the terms of service (ToS) of a particular service that researchers are looking to gather data from. However, third-party libraries or packages which claim to provide more throughput than the official APIs (rate limits, number of requests/sec) generally operate in a gray area as they tend to violate ToS. Always be sure to read their documentation throughly. Wikipedia API \u00b6 Let's say we want to gather some additional data about the Fortune 500 companies and since wikipedia is a rich source for data we decide to use the MediaWiki API to scrape this data. One very good place to start would be to look at the infoboxes (as wikipedia defines them) of articles corresponsing to each company on the list. They essentially contain a wealth of metadata about a particular entity the article belongs to which in our case is a company. For e.g. consider the wikipedia article for Walmart (https://en.wikipedia.org/wiki/Walmart) which includes the following infobox : As we can see from above, the infoboxes could provide us with a lot of valuable information such as : Year of founding Industry Founder(s) Products Services Operating income Net income Total assets Total equity Number of employees etc Although we expect this data to be fairly organized, it would require some post-processing which we will tackle in our next section. We pick a subset of our data and focus only on the top 20 of the Fortune 500 from the full list. Let's begin by installing some of libraries we will use for this excercise as follows, 1 2 3 4 # sudo apt install libcurl4-openssl-dev libssl-dev ! pip install wptools ! pip install wikipedia ! pip install wordcloud Importing the same, 1 2 3 4 5 6 import json import wptools import wikipedia import pandas as pd print ( 'wptools version : {} ' . format ( wptools . __version__ )) # checking the installed version 1 wptools version : 0.4.17 Now let's load the data which we scrapped in the previous section as follows, 1 2 3 4 # If you dont have the file, you can use the below code to fetch it: import urllib.request url = 'https://raw.githubusercontent.com/MonashDataFluency/python-web-scraping/master/data/fortune_500_companies.csv' urllib . request . urlretrieve ( url , 'fortune_500_companies.csv' ) 1 ('fortune_500_companies.csv', <http.client.HTTPMessage at 0x25ede05b320>) 1 2 3 fname = 'fortune_500_companies.csv' # scrapped data from previous section df = pd . read_csv ( fname ) # reading the csv file as a pandas df df . head () # displaying the first 5 rows rank company_name company_website 0 1 Walmart http://www.stock.walmart.com 1 2 Exxon Mobil http://www.exxonmobil.com 2 3 Berkshire Hathaway http://www.berkshirehathaway.com 3 4 Apple http://www.apple.com 4 5 UnitedHealth Group http://www.unitedhealthgroup.com Let's focus and select only the top 20 companies from the list as follows, 1 2 3 no_of_companies = 20 # no of companies we are interested df_sub = df . iloc [: no_of_companies , :] . copy () # only selecting the top 20 companies companies = df_sub [ 'company_name' ] . tolist () # converting the column to a list Taking a brief look at the same, 1 2 for i , j in enumerate ( companies ): # looping through the list of 20 company print ( ' {} . {} ' . format ( i + 1 , j )) # printing out the same 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 1. Walmart 2. Exxon Mobil 3. Berkshire Hathaway 4. Apple 5. UnitedHealth Group 6. McKesson 7. CVS Health 8. Amazon.com 9. AT&T 10. General Motors 11. Ford Motor 12. AmerisourceBergen 13. Chevron 14. Cardinal Health 15. Costco 16. Verizon 17. Kroger 18. General Electric 19. Walgreens Boots Alliance 20. JPMorgan Chase Getting article names from wiki \u00b6 Right off the bat, as you might have guessed, one issue with matching the top 20 Fortune 500 companies to their wikipedia article names is that both of them would not be exactly the same i.e. they match character for character. There will be slight variation in their names. To overcome this problem and ensure that we have all the company names and its corresponding wikipedia article, we will use the wikipedia package to get suggestions for the company names and their equivalent in wikipedia. 1 wiki_search = [{ company : wikipedia . search ( company )} for company in companies ] Inspecting the same, 1 2 3 4 for idx , company in enumerate ( wiki_search ): for i , j in company . items (): print ( ' {} . {} : \\n {} ' . format ( idx + 1 , i , ', ' . join ( j ))) print ( ' \\n ' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 1. Walmart : Walmart, History of Walmart, Criticism of Walmart, Walmarting, People of Walmart, Walmart (disambiguation), Walmart Canada, List of Walmart brands, Walmart Watch, 2019 El Paso shooting 2. Exxon Mobil : ExxonMobil, Exxon, Mobil, Esso, ExxonMobil climate change controversy, Exxon Valdez oil spill, ExxonMobil Building, ExxonMobil Electrofrac, List of public corporations by market capitalization, Exxon Valdez 3. Berkshire Hathaway : Berkshire Hathaway, Berkshire Hathaway Energy, List of assets owned by Berkshire Hathaway, Berkshire Hathaway Assurance, Berkshire Hathaway GUARD Insurance Companies, Warren Buffett, List of Berkshire Hathaway publications, The World's Billionaires, List of public corporations by market capitalization, David L. Sokol 4. Apple : Apple, Apple Inc., IPhone, Apple (disambiguation), IPad, Apple Silicon, IOS, MacOS, Macintosh, Fiona Apple 5. UnitedHealth Group : UnitedHealth Group, Optum, Pharmacy benefit management, William W. McGuire, Stephen J. Hemsley, Golden Rule Insurance Company, Catamaran Corporation, PacifiCare Health Systems, Gail Koziara Boudreaux, Amelia Warren Tyagi 6. McKesson : McKesson Corporation, DeRay Mckesson, McKesson Europe, Malcolm McKesson, Rexall (Canada), McKesson Plaza, McKesson (disambiguation), Johnetta Elzie, McKesson & Robbins scandal (1938), John Hammergren 7. CVS Health : CVS Health, CVS Pharmacy, CVS Health Charity Classic, CVS Caremark, Pharmacy benefit management, Larry Merlo, CVS, Encompass Health, Longs Drugs, MinuteClinic 8. Amazon.com : Amazon (company), History of Amazon, List of Amazon products and services, Prime Video, List of Amazon original programming, Amazon Web Services, Dot-com bubble, List of mergers and acquisitions by Amazon, Amazon S3, .amazon 9. AT&T : AT&T, AT&T Mobility, AT&T Corporation, AT&T TV, AT&T Stadium, T & T Supermarket, T, AT&T Communications, AT&T U-verse, AT&T SportsNet 10. General Motors : General Motors, History of General Motors, General Motors EV1, General Motors Vortec engine, Vauxhall Motors, GMC (automobile), General Motors 122 engine, General Motors 60\u00b0 V6 engine, General Motors Chapter 11 reorganization, List of General Motors factories 11. Ford Motor : Ford Motor Company, History of Ford Motor Company, Lincoln Motor Company, Ford Trimotor, Henry Ford, Henry Ford II, Ford Foundation, Ford F-Series, Edsel Ford, Ford Germany 12. AmerisourceBergen : AmerisourceBergen, List of largest companies by revenue, Cardinal Health, Steven H. Collis, Ornella Barra, Good Neighbor Pharmacy, Family Pharmacy, PharMerica, Remdesivir, Michael DiCandilo 13. Chevron : Chevron Corporation, Chevron, Chevron (insignia), Philip Chevron, Chevron Cars Ltd, Chevron Cars, Chevron bead, Wound Chevron, Chevron (anatomy), Chevron Phillips Chemical 14. Cardinal Health : Cardinal Health, Cardinal, Catalent, Cardinal (TV series), Robert D. Walter, Dublin, Ohio, Northern cardinal, List of largest companies by revenue, Cordis (medical), George S. Barrett 15. Costco : Costco, W. Craig Jelinek, American Express, Price Club, James Sinegal, Rotisserie chicken, Jeffrey Brotman, Warehouse club, Richard Chang (Costco), Costco bear 16. Verizon : Verizon Communications, Verizon Wireless, Verizon Media, Verizon Fios, Verizon Building, Verizon Delaware, Verizon Business, 4G, Verizon Hub, Verizon Hum 17. Kroger : Kroger, Murder Kroger, Kroger (disambiguation), Chad Kroeger, Bernard Kroger, Michael Kroger, Stanley Kamel, Tonio Kr\u00f6ger, Rodney McMullen, List of Monk characters 18. General Electric : General Electric, General Electric GEnx, General Electric CF6, General Electric F110, General Electric F404, General Electric GE9X, General Electric GE90, General Electric J85, General Electric F414, General Electric Company 19. Walgreens Boots Alliance : Walgreens Boots Alliance, Alliance Boots, Walgreens, Boots (company), Alliance Healthcare, Stefano Pessina, Boots Opticians, Rite Aid, Ken Murphy (businessman), Gregory Wasson 20. JPMorgan Chase : JPMorgan Chase, Chase Bank, 2012 JPMorgan Chase trading loss, JPMorgan Chase Tower (Houston), 270 Park Avenue, Chase Paymentech, 2014 JPMorgan Chase data breach, Bear Stearns, Jamie Dimon, JPMorgan Chase Building (Houston) Now let's get the most probable ones (the first suggestion) for each of the first 20 companies on the Fortune 500 list, 1 2 3 4 most_probable = [( company , wiki_search [ i ][ company ][ 0 ]) for i , company in enumerate ( companies )] companies = [ x [ 1 ] for x in most_probable ] print ( most_probable ) 1 [('Walmart', 'Walmart'), ('Exxon Mobil', 'ExxonMobil'), ('Berkshire Hathaway', 'Berkshire Hathaway'), ('Apple', 'Apple'), ('UnitedHealth Group', 'UnitedHealth Group'), ('McKesson', 'McKesson Corporation'), ('CVS Health', 'CVS Health'), ('Amazon.com', 'Amazon (company)'), ('AT&T', 'AT&T'), ('General Motors', 'General Motors'), ('Ford Motor', 'Ford Motor Company'), ('AmerisourceBergen', 'AmerisourceBergen'), ('Chevron', 'Chevron Corporation'), ('Cardinal Health', 'Cardinal Health'), ('Costco', 'Costco'), ('Verizon', 'Verizon Communications'), ('Kroger', 'Kroger'), ('General Electric', 'General Electric'), ('Walgreens Boots Alliance', 'Walgreens Boots Alliance'), ('JPMorgan Chase', 'JPMorgan Chase')] We can notice that most of the wiki article titles make sense. However, Apple is quite ambiguous in this regard as it can indicate the fruit as well as the company. However we can see that the second suggestion returned by was Apple Inc. . Hence, we can manually replace it with Apple Inc. as follows, 1 2 companies [ companies . index ( 'Apple' )] = 'Apple Inc.' # replacing \"Apple\" print ( companies ) # final list of wikipedia article titles 1 ['Walmart', 'ExxonMobil', 'Berkshire Hathaway', 'Apple Inc.', 'UnitedHealth Group', 'McKesson Corporation', 'CVS Health', 'Amazon (company)', 'AT&T', 'General Motors', 'Ford Motor Company', 'AmerisourceBergen', 'Chevron Corporation', 'Cardinal Health', 'Costco', 'Verizon Communications', 'Kroger', 'General Electric', 'Walgreens Boots Alliance', 'JPMorgan Chase'] Retrieving the infoboxes \u00b6 Now that we have mapped the names of the companies to their corresponding wikipedia article let's retrieve the infobox data from those pages. wptools provides easy to use methods to directly call the MediaWiki API on our behalf and get us all the wikipedia data. Let's try retrieving data for Walmart as follows, 1 2 page = wptools . page ( 'Walmart' ) page . get_parse () # parses the wikipedia article 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 en.wikipedia.org (parse) Walmart en.wikipedia.org (imageinfo) File:Walmart store exterior 5266815680.jpg Walmart (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Walmart s... infobox: <dict(30)> name, logo, logo_caption, image, image_size,... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:W... pageid: 33589 parsetree: <str(347698)> <root><template><title>about</title><pa... requests: <list(2)> parse, imageinfo title: Walmart wikibase: Q483551 wikidata_url: https://www.wikidata.org/wiki/Q483551 wikitext: <str(277438)> {{about|the retail chain|other uses}}{{p... } <wptools.page.WPToolsPage at 0x25ede0ed588> As we can see from the output above, wptools successfully retrieved the wikipedia and wikidata corresponding to the query Walmart . Now inspecting the fetched attributes, 1 page . data . keys () 1 dict_keys(['requests', 'iwlinks', 'pageid', 'wikitext', 'parsetree', 'infobox', 'title', 'wikibase', 'wikidata_url', 'image']) The attribute infobox contains the data we require, 1 page . data [ 'infobox' ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 {'name': 'Walmart Inc.', 'logo': 'Walmart logo.svg', 'logo_caption': \"Walmart's current logo since 2008\", 'image': 'Walmart store exterior 5266815680.jpg', 'image_size': '270px', 'image_caption': 'Exterior of a Walmart store', 'former_name': \"{{Unbulleted list|Walton's (1950\u20131969)|Wal-Mart, Inc. (1969\u20131970)|Wal-Mart Stores, Inc. (1970\u20132018)}}\", 'type': '[[Public company|Public]]', 'ISIN': 'US9311421039', 'industry': '[[Retail]]', 'traded_as': '{{Unbulleted list|NYSE|WMT|[[DJIA]] component|[[S&P 100]] component|[[S&P 500]] component}} {{NYSE|WMT}}', 'foundation': '{{Start date and age|1962|7|2}} (in [[Rogers, Arkansas]])', 'founder': '[[Sam Walton]]', 'location_city': '[[Bentonville, Arkansas]]', 'location_country': 'U.S.', 'locations': '{{decrease}} 11,484 stores worldwide (April 30, 2020)', 'area_served': 'Worldwide', 'key_people': '{{plainlist|\\n* [[Greg Penner]] ([[Chairman]])\\n* [[Doug McMillon]] ([[President (corporate title)|President]], [[CEO]])}}', 'products': '{{hlist|Electronics|Movies and music|Home and furniture|Home improvement|Clothing|Footwear|Jewelry|Toys|Health and beauty|Pet supplies|Sporting goods and fitness|Auto|Photo finishing|Craft supplies|Party supplies|Grocery}}', 'services': '{{hlist|[[Ria Money Transfer|Walmart-2-Walmart]]|Walmart MoneyCard|Pickup Today|Walmart.com|Financial Services| Walmart Pay}}', 'revenue': '{{increase}} {{US$|523.964 billion|link|=|yes}} {{small|([[Fiscal Year|FY]] 2020)}}', 'operating_income': '{{decrease}} {{US$|20.568 billion}} {{small|(FY 2020)}}', 'net_income': '{{increase}} {{US$|14.881 billion}} {{small|(FY 2020)}}', 'assets': '{{increase}} {{US$|236.495 billion}} {{small|(FY 2020)}}', 'equity': '{{increase}} {{US$|74.669 billion}} {{small|(FY 2020)}}', 'owner': '[[Walton family]] (50.85%)', 'num_employees': '{{plainlist|\\n* 2.2|nbsp|million, Worldwide (2018)|ref| name=\"xbrlus_1\" |\\n* 1.5|nbsp|million, U.S. (2017)|ref| name=\"Walmart\"|{{cite web |url = http://corporate.walmart.com/our-story/locations/united-states |title = Walmart Locations Around the World \u2013 United States |publisher = |url-status=live |archiveurl = https://web.archive.org/web/20150926012456/http://corporate.walmart.com/our-story/locations/united-states |archivedate = September 26, 2015 |df = mdy-all }}|</ref>|\\n* 700,000, International}} {{nbsp}} million, Worldwide (2018) * 1.5 {{nbsp}} million, U.S. (2017) * 700,000, International', 'divisions': \"{{Unbulleted list|Walmart U.S.|Walmart International|[[Sam's Club]]|Global eCommerce}}\", 'subsid': '[[List of assets owned by Walmart|List of subsidiaries]]', 'homepage': '{{URL|walmart.com}}'} Let's define a list of features that we want from the infoboxes as follows, 1 2 3 4 wiki_data = [] # attributes of interest contained within the wiki infoboxes features = [ 'founder' , 'location_country' , 'revenue' , 'operating_income' , 'net_income' , 'assets' , 'equity' , 'type' , 'industry' , 'products' , 'num_employees' ] Now fetching the data for all the companies (this may take a while), 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 for company in companies : page = wptools . page ( company ) # create a page object try : page . get_parse () # call the API and parse the data if page . data [ 'infobox' ] != None : # if infobox is present infobox = page . data [ 'infobox' ] # get data for the interested features/attributes data = { feature : infobox [ feature ] if feature in infobox else '' for feature in features } else : data = { feature : '' for feature in features } data [ 'company_name' ] = company wiki_data . append ( data ) except KeyError : pass 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 en.wikipedia.org (parse) Walmart en.wikipedia.org (imageinfo) File:Walmart store exterior 5266815680.jpg Walmart (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Walmart s... infobox: <dict(30)> name, logo, logo_caption, image, image_size,... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:W... pageid: 33589 parsetree: <str(347698)> <root><template><title>about</title><pa... requests: <list(2)> parse, imageinfo title: Walmart wikibase: Q483551 wikidata_url: https://www.wikidata.org/wiki/Q483551 wikitext: <str(277438)> {{about|the retail chain|other uses}}{{p... } en.wikipedia.org (parse) ExxonMobil en.wikipedia.org (imageinfo) File:Exxonmobil-headquarters-1.jpg ExxonMobil (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Exxonmobi... infobox: <dict(30)> name, logo, image, image_caption, type, trad... iwlinks: <list(4)> https://commons.wikimedia.org/wiki/Category:E... pageid: 18848197 parsetree: <str(192545)> <root><template><title>short descriptio... requests: <list(2)> parse, imageinfo title: ExxonMobil wikibase: Q156238 wikidata_url: https://www.wikidata.org/wiki/Q156238 wikitext: <str(157036)> {{short description|American multination... } en.wikipedia.org (parse) Berkshire Hathaway Berkshire Hathaway (en) data { image: <list(0)> infobox: <dict(24)> name, former_name, logo, image, image_captio... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:B... pageid: 314333 parsetree: <str(105467)> <root><template><title>short descriptio... requests: <list(1)> parse title: Berkshire Hathaway wikibase: Q217583 wikidata_url: https://www.wikidata.org/wiki/Q217583 wikitext: <str(89908)> {{short description|American multinationa... } en.wikipedia.org (parse) Apple Inc. en.wikipedia.org (imageinfo) File:Apple park cupertino 2019.jpg Apple Inc. (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Apple par... infobox: <dict(35)> name, logo, logo_size, image, image_size, im... iwlinks: <list(8)> https://commons.wikimedia.org/wiki/Special:Se... pageid: 856 parsetree: <str(419620)> <root><template><title>Redirect</title>... requests: <list(2)> parse, imageinfo title: Apple Inc. wikibase: Q312 wikidata_url: https://www.wikidata.org/wiki/Q312 wikitext: <str(335917)> {{Redirect|Apple (company)|other compani... } en.wikipedia.org (parse) UnitedHealth Group UnitedHealth Group (en) data { infobox: <dict(17)> name, logo, type, traded_as, founder, key_pe... pageid: 1845551 parsetree: <str(87066)> <root><template><title>Redirect</title><... requests: <list(1)> parse title: UnitedHealth Group wikibase: Q2103926 wikidata_url: https://www.wikidata.org/wiki/Q2103926 wikitext: <str(74588)> {{Redirect|UnitedHealthcare|the cycling t... } en.wikipedia.org (parse) McKesson Corporation McKesson Corporation (en) data { infobox: <dict(19)> name, logo, type, traded_as, founder, locati... pageid: 1041603 parsetree: <str(40259)> <root><template><title>Redirect</title><... requests: <list(1)> parse title: McKesson Corporation wikibase: Q570473 wikidata_url: https://www.wikidata.org/wiki/Q570473 wikitext: <str(32180)> {{Redirect|McKesson}}{{short description|... } en.wikipedia.org (parse) CVS Health CVS Health (en) data { infobox: <dict(28)> name, logo, logo_size, former_name, type, tr... pageid: 10377597 parsetree: <str(70825)> <root><template><title>short description... requests: <list(1)> parse title: CVS Health wikibase: Q624375 wikidata_url: https://www.wikidata.org/wiki/Q624375 wikitext: <str(54943)> {{short description|American healthcare c... } en.wikipedia.org (parse) Amazon (company) en.wikipedia.org (imageinfo) File:Amazon Spheres 05.jpg Amazon (company) (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Amazon Sp... infobox: <dict(32)> name, logo, logo_size, image, image_caption,... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:A... pageid: 90451 parsetree: <str(183373)> <root><template><title>short descriptio... requests: <list(2)> parse, imageinfo title: Amazon (company) wikibase: Q3884 wikidata_url: https://www.wikidata.org/wiki/Q3884 wikitext: <str(142559)> {{short description|American technology ... } en.wikipedia.org (parse) AT&T en.wikipedia.org (imageinfo) File:AT&THQDallas.jpg AT&T (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:AT&THQDal... infobox: <dict(28)> name, logo, logo_size, image, image_size, im... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:AT%26T pageid: 17555269 parsetree: <str(136294)> <root><template><title>about</title><pa... requests: <list(2)> parse, imageinfo title: AT&T wikibase: Q35476 wikidata_url: https://www.wikidata.org/wiki/Q35476 wikitext: <str(109258)> {{about|the company known as AT&T since ... } en.wikipedia.org (parse) General Motors en.wikipedia.org (imageinfo) File:RenCen.JPG General Motors (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:RenCen.JP... infobox: <dict(30)> name, former_name, logo, logo_size, image, i... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:G... pageid: 12102 parsetree: <str(187427)> <root><template><title>short descriptio... requests: <list(2)> parse, imageinfo title: General Motors wikibase: Q81965 wikidata_url: https://www.wikidata.org/wiki/Q81965 wikitext: <str(146919)> {{short description|American automotive ... } en.wikipedia.org (parse) Ford Motor Company en.wikipedia.org (imageinfo) File:FordGlassHouse.jpg Ford Motor Company (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:FordGlass... infobox: <dict(27)> name, logo, image, image_size, image_caption... iwlinks: <list(8)> https://commons.wikimedia.org/wiki/Category:F... pageid: 30433662 parsetree: <str(197053)> <root><template><title>Redirect</title>... requests: <list(2)> parse, imageinfo title: Ford Motor Company wikibase: Q44294 wikidata_url: https://www.wikidata.org/wiki/Q44294 wikitext: <str(160388)> {{Redirect|Ford}}{{pp-semi-indef}}{{pp-m... } en.wikipedia.org (parse) AmerisourceBergen AmerisourceBergen (en) data { infobox: <dict(17)> name, logo, type, traded_as, foundation, loc... pageid: 1445945 parsetree: <str(21535)> <root><template><title>short description... requests: <list(1)> parse title: AmerisourceBergen wikibase: Q470156 wikidata_url: https://www.wikidata.org/wiki/Q470156 wikitext: <str(16172)> {{short description|American healthcare c... } en.wikipedia.org (parse) Chevron Corporation Chevron Corporation (en) data { image: <list(0)> infobox: <dict(24)> name, logo, logo_size, logo_caption, image, ... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:C... pageid: 284749 parsetree: <str(125957)> <root><template><title>short descriptio... requests: <list(1)> parse title: Chevron Corporation wikibase: Q319642 wikidata_url: https://www.wikidata.org/wiki/Q319642 wikitext: <str(102739)> {{short description|American multination... } en.wikipedia.org (parse) Cardinal Health Cardinal Health (en) data { infobox: <dict(17)> name, logo, type, traded_as, industry, found... pageid: 1041632 parsetree: <str(33864)> <root><template><title>Infobox company</... requests: <list(1)> parse title: Cardinal Health wikibase: Q902397 wikidata_url: https://www.wikidata.org/wiki/Q902397 wikitext: <str(26601)> {{Infobox company| name = Cardinal Health... } en.wikipedia.org (parse) Costco en.wikipedia.org (imageinfo) File:Costcoheadquarters.jpg Costco (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Costcohea... infobox: <dict(35)> name, logo, logo_caption, image, image_size,... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:Costco pageid: 446056 parsetree: <str(101971)> <root><template><title>Distinguish</tit... requests: <list(2)> parse, imageinfo title: Costco wikibase: Q715583 wikidata_url: https://www.wikidata.org/wiki/Q715583 wikitext: <str(76641)> {{Distinguish|COSCO|Cosco (India) Limited... } en.wikipedia.org (parse) Verizon Communications en.wikipedia.org (imageinfo) File:Verizon Building (8156005279).jpg Verizon Communications (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Verizon B... infobox: <dict(32)> name, logo, image, image_size, image_caption... iwlinks: <list(3)> https://commons.wikimedia.org/wiki/Category:T... pageid: 18619278 parsetree: <str(154091)> <root><template><title>redirect</title>... requests: <list(2)> parse, imageinfo title: Verizon Communications wikibase: Q467752 wikidata_url: https://www.wikidata.org/wiki/Q467752 wikitext: <str(130536)> {{redirect|Verizon|its mobile network su... } en.wikipedia.org (parse) Kroger en.wikipedia.org (imageinfo) File:Cincinnati-kroger-building.jpg Kroger (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Cincinnat... infobox: <dict(24)> name, logo, image, image_caption, type, trad... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:Kroger pageid: 367762 parsetree: <str(121075)> <root><template><title>short descriptio... requests: <list(2)> parse, imageinfo title: Kroger wikibase: Q153417 wikidata_url: https://www.wikidata.org/wiki/Q153417 wikitext: <str(101498)> {{short description|American multination... } en.wikipedia.org (parse) General Electric General Electric (en) data { infobox: <dict(20)> name, logo, type, traded_as, ISIN, industry,... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:G... pageid: 12730 parsetree: <str(165322)> <root><template><title>redirect</title>... requests: <list(1)> parse title: General Electric wikibase: Q54173 wikidata_url: https://www.wikidata.org/wiki/Q54173 wikitext: <str(140011)> {{redirect|GE}}{{distinguish|text=the fo... } en.wikipedia.org (parse) Walgreens Boots Alliance Walgreens Boots Alliance (en) data { infobox: <dict(29)> name, logo, logo_size, type, traded_as, pred... pageid: 44732533 parsetree: <str(32556)> <root><template><title>Use mdy dates</ti... requests: <list(1)> parse title: Walgreens Boots Alliance wikibase: Q18712620 wikidata_url: https://www.wikidata.org/wiki/Q18712620 wikitext: <str(25068)> {{Use mdy dates|date=October 2019}}{{shor... } en.wikipedia.org (parse) JPMorgan Chase en.wikipedia.org (imageinfo) File:383 Madison Ave Bear Stearns C ... JPMorgan Chase (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:383 Madis... infobox: <dict(31)> name, logo, image, image_caption, type, trad... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:J... pageid: 231001 parsetree: <str(144960)> <root><template><title>About</title><pa... requests: <list(2)> parse, imageinfo title: JPMorgan Chase wikibase: Q192314 wikidata_url: https://www.wikidata.org/wiki/Q192314 wikitext: <str(117507)> {{About|JPMorgan Chase & Co|its main sub... } Let's take a look at the first instance in wiki_data i.e. Walmart , 1 wiki_data [ 0 ] 1 2 3 4 5 6 7 8 9 10 11 12 {'founder': '[[Sam Walton]]', 'location_country': 'U.S.', 'revenue': '{{increase}} {{US$|523.964 billion|link|=|yes}} {{small|([[Fiscal Year|FY]] 2020)}}', 'operating_income': '{{decrease}} {{US$|20.568 billion}} {{small|(FY 2020)}}', 'net_income': '{{increase}} {{US$|14.881 billion}} {{small|(FY 2020)}}', 'assets': '{{increase}} {{US$|236.495 billion}} {{small|(FY 2020)}}', 'equity': '{{increase}} {{US$|74.669 billion}} {{small|(FY 2020)}}', 'type': '[[Public company|Public]]', 'industry': '[[Retail]]', 'products': '{{hlist|Electronics|Movies and music|Home and furniture|Home improvement|Clothing|Footwear|Jewelry|Toys|Health and beauty|Pet supplies|Sporting goods and fitness|Auto|Photo finishing|Craft supplies|Party supplies|Grocery}}', 'num_employees': '{{plainlist|\\n* 2.2|nbsp|million, Worldwide (2018)|ref| name=\"xbrlus_1\" |\\n* 1.5|nbsp|million, U.S. (2017)|ref| name=\"Walmart\"|{{cite web |url = http://corporate.walmart.com/our-story/locations/united-states |title = Walmart Locations Around the World \u2013 United States |publisher = |url-status=live |archiveurl = https://web.archive.org/web/20150926012456/http://corporate.walmart.com/our-story/locations/united-states |archivedate = September 26, 2015 |df = mdy-all }}|</ref>|\\n* 700,000, International}} {{nbsp}} million, Worldwide (2018) * 1.5 {{nbsp}} million, U.S. (2017) * 700,000, International', 'company_name': 'Walmart'} So, we have successfully retrieved all the infobox data for the companies. Also we can notice that some additional wrangling and cleaning is required which we will perform in the next section. Finally, let's export the scraped infoboxes as a single JSON file to a convenient location as follows, 1 2 with open ( 'infoboxes.json' , 'w' ) as file : json . dump ( wiki_data , file ) References \u00b6 https://phpenthusiast.com/blog/what-is-rest-api https://github.com/siznax/wptools/wiki/Data-captured https://en.wikipedia.org/w/api.php https://wikipedia.readthedocs.io/en/latest/code.html","title":"API based scraping"},{"location":"section-3-API-based-scraping/#a-brief-introduction-to-apis","text":"In this section, we will take a look at an alternative way to gather data than the previous pattern based HTML scraping. Sometimes websites offer an API (or Application Programming Interface) as a service which provides a high level interface to directly retrieve data from their repositories or databases at the backend. From Wikipedia, \" An API is typically defined as a set of specifications, such as Hypertext Transfer Protocol (HTTP) request messages, along with a definition of the structure of response messages, usually in an Extensible Markup Language (XML) or JavaScript Object Notation (JSON) format. \" They typically tend to be URL endpoints (to be fired as requests) that need to be modified based on our requirements (what we desire in the response body) which then returns some a payload (data) within the response, formatted as either JSON, XML or HTML. A popular web architecture style called REST (or representational state transfer) allows users to interact with web services via GET and POST calls (two most commonly used) which we briefly saw in the previous section. For example, Twitter's REST API allows developers to access core Twitter data and the Search API provides methods for developers to interact with Twitter Search and trends data. There are primarily two ways to use APIs : Through the command terminal using URL endpoints, or Through programming language specific wrappers For example, Tweepy is a famous python wrapper for Twitter API whereas twurl is a command line interface (CLI) tool but both can achieve the same outcomes. Here we focus on the latter approach and will use a Python library (a wrapper) called wptools based around the original MediaWiki API. One advantage of using official APIs is that they are usually compliant of the terms of service (ToS) of a particular service that researchers are looking to gather data from. However, third-party libraries or packages which claim to provide more throughput than the official APIs (rate limits, number of requests/sec) generally operate in a gray area as they tend to violate ToS. Always be sure to read their documentation throughly.","title":"A brief introduction to APIs"},{"location":"section-3-API-based-scraping/#wikipedia-api","text":"Let's say we want to gather some additional data about the Fortune 500 companies and since wikipedia is a rich source for data we decide to use the MediaWiki API to scrape this data. One very good place to start would be to look at the infoboxes (as wikipedia defines them) of articles corresponsing to each company on the list. They essentially contain a wealth of metadata about a particular entity the article belongs to which in our case is a company. For e.g. consider the wikipedia article for Walmart (https://en.wikipedia.org/wiki/Walmart) which includes the following infobox : As we can see from above, the infoboxes could provide us with a lot of valuable information such as : Year of founding Industry Founder(s) Products Services Operating income Net income Total assets Total equity Number of employees etc Although we expect this data to be fairly organized, it would require some post-processing which we will tackle in our next section. We pick a subset of our data and focus only on the top 20 of the Fortune 500 from the full list. Let's begin by installing some of libraries we will use for this excercise as follows, 1 2 3 4 # sudo apt install libcurl4-openssl-dev libssl-dev ! pip install wptools ! pip install wikipedia ! pip install wordcloud Importing the same, 1 2 3 4 5 6 import json import wptools import wikipedia import pandas as pd print ( 'wptools version : {} ' . format ( wptools . __version__ )) # checking the installed version 1 wptools version : 0.4.17 Now let's load the data which we scrapped in the previous section as follows, 1 2 3 4 # If you dont have the file, you can use the below code to fetch it: import urllib.request url = 'https://raw.githubusercontent.com/MonashDataFluency/python-web-scraping/master/data/fortune_500_companies.csv' urllib . request . urlretrieve ( url , 'fortune_500_companies.csv' ) 1 ('fortune_500_companies.csv', <http.client.HTTPMessage at 0x25ede05b320>) 1 2 3 fname = 'fortune_500_companies.csv' # scrapped data from previous section df = pd . read_csv ( fname ) # reading the csv file as a pandas df df . head () # displaying the first 5 rows rank company_name company_website 0 1 Walmart http://www.stock.walmart.com 1 2 Exxon Mobil http://www.exxonmobil.com 2 3 Berkshire Hathaway http://www.berkshirehathaway.com 3 4 Apple http://www.apple.com 4 5 UnitedHealth Group http://www.unitedhealthgroup.com Let's focus and select only the top 20 companies from the list as follows, 1 2 3 no_of_companies = 20 # no of companies we are interested df_sub = df . iloc [: no_of_companies , :] . copy () # only selecting the top 20 companies companies = df_sub [ 'company_name' ] . tolist () # converting the column to a list Taking a brief look at the same, 1 2 for i , j in enumerate ( companies ): # looping through the list of 20 company print ( ' {} . {} ' . format ( i + 1 , j )) # printing out the same 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 1. Walmart 2. Exxon Mobil 3. Berkshire Hathaway 4. Apple 5. UnitedHealth Group 6. McKesson 7. CVS Health 8. Amazon.com 9. AT&T 10. General Motors 11. Ford Motor 12. AmerisourceBergen 13. Chevron 14. Cardinal Health 15. Costco 16. Verizon 17. Kroger 18. General Electric 19. Walgreens Boots Alliance 20. JPMorgan Chase","title":"Wikipedia API"},{"location":"section-3-API-based-scraping/#getting-article-names-from-wiki","text":"Right off the bat, as you might have guessed, one issue with matching the top 20 Fortune 500 companies to their wikipedia article names is that both of them would not be exactly the same i.e. they match character for character. There will be slight variation in their names. To overcome this problem and ensure that we have all the company names and its corresponding wikipedia article, we will use the wikipedia package to get suggestions for the company names and their equivalent in wikipedia. 1 wiki_search = [{ company : wikipedia . search ( company )} for company in companies ] Inspecting the same, 1 2 3 4 for idx , company in enumerate ( wiki_search ): for i , j in company . items (): print ( ' {} . {} : \\n {} ' . format ( idx + 1 , i , ', ' . join ( j ))) print ( ' \\n ' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 1. Walmart : Walmart, History of Walmart, Criticism of Walmart, Walmarting, People of Walmart, Walmart (disambiguation), Walmart Canada, List of Walmart brands, Walmart Watch, 2019 El Paso shooting 2. Exxon Mobil : ExxonMobil, Exxon, Mobil, Esso, ExxonMobil climate change controversy, Exxon Valdez oil spill, ExxonMobil Building, ExxonMobil Electrofrac, List of public corporations by market capitalization, Exxon Valdez 3. Berkshire Hathaway : Berkshire Hathaway, Berkshire Hathaway Energy, List of assets owned by Berkshire Hathaway, Berkshire Hathaway Assurance, Berkshire Hathaway GUARD Insurance Companies, Warren Buffett, List of Berkshire Hathaway publications, The World's Billionaires, List of public corporations by market capitalization, David L. Sokol 4. Apple : Apple, Apple Inc., IPhone, Apple (disambiguation), IPad, Apple Silicon, IOS, MacOS, Macintosh, Fiona Apple 5. UnitedHealth Group : UnitedHealth Group, Optum, Pharmacy benefit management, William W. McGuire, Stephen J. Hemsley, Golden Rule Insurance Company, Catamaran Corporation, PacifiCare Health Systems, Gail Koziara Boudreaux, Amelia Warren Tyagi 6. McKesson : McKesson Corporation, DeRay Mckesson, McKesson Europe, Malcolm McKesson, Rexall (Canada), McKesson Plaza, McKesson (disambiguation), Johnetta Elzie, McKesson & Robbins scandal (1938), John Hammergren 7. CVS Health : CVS Health, CVS Pharmacy, CVS Health Charity Classic, CVS Caremark, Pharmacy benefit management, Larry Merlo, CVS, Encompass Health, Longs Drugs, MinuteClinic 8. Amazon.com : Amazon (company), History of Amazon, List of Amazon products and services, Prime Video, List of Amazon original programming, Amazon Web Services, Dot-com bubble, List of mergers and acquisitions by Amazon, Amazon S3, .amazon 9. AT&T : AT&T, AT&T Mobility, AT&T Corporation, AT&T TV, AT&T Stadium, T & T Supermarket, T, AT&T Communications, AT&T U-verse, AT&T SportsNet 10. General Motors : General Motors, History of General Motors, General Motors EV1, General Motors Vortec engine, Vauxhall Motors, GMC (automobile), General Motors 122 engine, General Motors 60\u00b0 V6 engine, General Motors Chapter 11 reorganization, List of General Motors factories 11. Ford Motor : Ford Motor Company, History of Ford Motor Company, Lincoln Motor Company, Ford Trimotor, Henry Ford, Henry Ford II, Ford Foundation, Ford F-Series, Edsel Ford, Ford Germany 12. AmerisourceBergen : AmerisourceBergen, List of largest companies by revenue, Cardinal Health, Steven H. Collis, Ornella Barra, Good Neighbor Pharmacy, Family Pharmacy, PharMerica, Remdesivir, Michael DiCandilo 13. Chevron : Chevron Corporation, Chevron, Chevron (insignia), Philip Chevron, Chevron Cars Ltd, Chevron Cars, Chevron bead, Wound Chevron, Chevron (anatomy), Chevron Phillips Chemical 14. Cardinal Health : Cardinal Health, Cardinal, Catalent, Cardinal (TV series), Robert D. Walter, Dublin, Ohio, Northern cardinal, List of largest companies by revenue, Cordis (medical), George S. Barrett 15. Costco : Costco, W. Craig Jelinek, American Express, Price Club, James Sinegal, Rotisserie chicken, Jeffrey Brotman, Warehouse club, Richard Chang (Costco), Costco bear 16. Verizon : Verizon Communications, Verizon Wireless, Verizon Media, Verizon Fios, Verizon Building, Verizon Delaware, Verizon Business, 4G, Verizon Hub, Verizon Hum 17. Kroger : Kroger, Murder Kroger, Kroger (disambiguation), Chad Kroeger, Bernard Kroger, Michael Kroger, Stanley Kamel, Tonio Kr\u00f6ger, Rodney McMullen, List of Monk characters 18. General Electric : General Electric, General Electric GEnx, General Electric CF6, General Electric F110, General Electric F404, General Electric GE9X, General Electric GE90, General Electric J85, General Electric F414, General Electric Company 19. Walgreens Boots Alliance : Walgreens Boots Alliance, Alliance Boots, Walgreens, Boots (company), Alliance Healthcare, Stefano Pessina, Boots Opticians, Rite Aid, Ken Murphy (businessman), Gregory Wasson 20. JPMorgan Chase : JPMorgan Chase, Chase Bank, 2012 JPMorgan Chase trading loss, JPMorgan Chase Tower (Houston), 270 Park Avenue, Chase Paymentech, 2014 JPMorgan Chase data breach, Bear Stearns, Jamie Dimon, JPMorgan Chase Building (Houston) Now let's get the most probable ones (the first suggestion) for each of the first 20 companies on the Fortune 500 list, 1 2 3 4 most_probable = [( company , wiki_search [ i ][ company ][ 0 ]) for i , company in enumerate ( companies )] companies = [ x [ 1 ] for x in most_probable ] print ( most_probable ) 1 [('Walmart', 'Walmart'), ('Exxon Mobil', 'ExxonMobil'), ('Berkshire Hathaway', 'Berkshire Hathaway'), ('Apple', 'Apple'), ('UnitedHealth Group', 'UnitedHealth Group'), ('McKesson', 'McKesson Corporation'), ('CVS Health', 'CVS Health'), ('Amazon.com', 'Amazon (company)'), ('AT&T', 'AT&T'), ('General Motors', 'General Motors'), ('Ford Motor', 'Ford Motor Company'), ('AmerisourceBergen', 'AmerisourceBergen'), ('Chevron', 'Chevron Corporation'), ('Cardinal Health', 'Cardinal Health'), ('Costco', 'Costco'), ('Verizon', 'Verizon Communications'), ('Kroger', 'Kroger'), ('General Electric', 'General Electric'), ('Walgreens Boots Alliance', 'Walgreens Boots Alliance'), ('JPMorgan Chase', 'JPMorgan Chase')] We can notice that most of the wiki article titles make sense. However, Apple is quite ambiguous in this regard as it can indicate the fruit as well as the company. However we can see that the second suggestion returned by was Apple Inc. . Hence, we can manually replace it with Apple Inc. as follows, 1 2 companies [ companies . index ( 'Apple' )] = 'Apple Inc.' # replacing \"Apple\" print ( companies ) # final list of wikipedia article titles 1 ['Walmart', 'ExxonMobil', 'Berkshire Hathaway', 'Apple Inc.', 'UnitedHealth Group', 'McKesson Corporation', 'CVS Health', 'Amazon (company)', 'AT&T', 'General Motors', 'Ford Motor Company', 'AmerisourceBergen', 'Chevron Corporation', 'Cardinal Health', 'Costco', 'Verizon Communications', 'Kroger', 'General Electric', 'Walgreens Boots Alliance', 'JPMorgan Chase']","title":"Getting article names from wiki"},{"location":"section-3-API-based-scraping/#retrieving-the-infoboxes","text":"Now that we have mapped the names of the companies to their corresponding wikipedia article let's retrieve the infobox data from those pages. wptools provides easy to use methods to directly call the MediaWiki API on our behalf and get us all the wikipedia data. Let's try retrieving data for Walmart as follows, 1 2 page = wptools . page ( 'Walmart' ) page . get_parse () # parses the wikipedia article 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 en.wikipedia.org (parse) Walmart en.wikipedia.org (imageinfo) File:Walmart store exterior 5266815680.jpg Walmart (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Walmart s... infobox: <dict(30)> name, logo, logo_caption, image, image_size,... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:W... pageid: 33589 parsetree: <str(347698)> <root><template><title>about</title><pa... requests: <list(2)> parse, imageinfo title: Walmart wikibase: Q483551 wikidata_url: https://www.wikidata.org/wiki/Q483551 wikitext: <str(277438)> {{about|the retail chain|other uses}}{{p... } <wptools.page.WPToolsPage at 0x25ede0ed588> As we can see from the output above, wptools successfully retrieved the wikipedia and wikidata corresponding to the query Walmart . Now inspecting the fetched attributes, 1 page . data . keys () 1 dict_keys(['requests', 'iwlinks', 'pageid', 'wikitext', 'parsetree', 'infobox', 'title', 'wikibase', 'wikidata_url', 'image']) The attribute infobox contains the data we require, 1 page . data [ 'infobox' ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 {'name': 'Walmart Inc.', 'logo': 'Walmart logo.svg', 'logo_caption': \"Walmart's current logo since 2008\", 'image': 'Walmart store exterior 5266815680.jpg', 'image_size': '270px', 'image_caption': 'Exterior of a Walmart store', 'former_name': \"{{Unbulleted list|Walton's (1950\u20131969)|Wal-Mart, Inc. (1969\u20131970)|Wal-Mart Stores, Inc. (1970\u20132018)}}\", 'type': '[[Public company|Public]]', 'ISIN': 'US9311421039', 'industry': '[[Retail]]', 'traded_as': '{{Unbulleted list|NYSE|WMT|[[DJIA]] component|[[S&P 100]] component|[[S&P 500]] component}} {{NYSE|WMT}}', 'foundation': '{{Start date and age|1962|7|2}} (in [[Rogers, Arkansas]])', 'founder': '[[Sam Walton]]', 'location_city': '[[Bentonville, Arkansas]]', 'location_country': 'U.S.', 'locations': '{{decrease}} 11,484 stores worldwide (April 30, 2020)', 'area_served': 'Worldwide', 'key_people': '{{plainlist|\\n* [[Greg Penner]] ([[Chairman]])\\n* [[Doug McMillon]] ([[President (corporate title)|President]], [[CEO]])}}', 'products': '{{hlist|Electronics|Movies and music|Home and furniture|Home improvement|Clothing|Footwear|Jewelry|Toys|Health and beauty|Pet supplies|Sporting goods and fitness|Auto|Photo finishing|Craft supplies|Party supplies|Grocery}}', 'services': '{{hlist|[[Ria Money Transfer|Walmart-2-Walmart]]|Walmart MoneyCard|Pickup Today|Walmart.com|Financial Services| Walmart Pay}}', 'revenue': '{{increase}} {{US$|523.964 billion|link|=|yes}} {{small|([[Fiscal Year|FY]] 2020)}}', 'operating_income': '{{decrease}} {{US$|20.568 billion}} {{small|(FY 2020)}}', 'net_income': '{{increase}} {{US$|14.881 billion}} {{small|(FY 2020)}}', 'assets': '{{increase}} {{US$|236.495 billion}} {{small|(FY 2020)}}', 'equity': '{{increase}} {{US$|74.669 billion}} {{small|(FY 2020)}}', 'owner': '[[Walton family]] (50.85%)', 'num_employees': '{{plainlist|\\n* 2.2|nbsp|million, Worldwide (2018)|ref| name=\"xbrlus_1\" |\\n* 1.5|nbsp|million, U.S. (2017)|ref| name=\"Walmart\"|{{cite web |url = http://corporate.walmart.com/our-story/locations/united-states |title = Walmart Locations Around the World \u2013 United States |publisher = |url-status=live |archiveurl = https://web.archive.org/web/20150926012456/http://corporate.walmart.com/our-story/locations/united-states |archivedate = September 26, 2015 |df = mdy-all }}|</ref>|\\n* 700,000, International}} {{nbsp}} million, Worldwide (2018) * 1.5 {{nbsp}} million, U.S. (2017) * 700,000, International', 'divisions': \"{{Unbulleted list|Walmart U.S.|Walmart International|[[Sam's Club]]|Global eCommerce}}\", 'subsid': '[[List of assets owned by Walmart|List of subsidiaries]]', 'homepage': '{{URL|walmart.com}}'} Let's define a list of features that we want from the infoboxes as follows, 1 2 3 4 wiki_data = [] # attributes of interest contained within the wiki infoboxes features = [ 'founder' , 'location_country' , 'revenue' , 'operating_income' , 'net_income' , 'assets' , 'equity' , 'type' , 'industry' , 'products' , 'num_employees' ] Now fetching the data for all the companies (this may take a while), 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 for company in companies : page = wptools . page ( company ) # create a page object try : page . get_parse () # call the API and parse the data if page . data [ 'infobox' ] != None : # if infobox is present infobox = page . data [ 'infobox' ] # get data for the interested features/attributes data = { feature : infobox [ feature ] if feature in infobox else '' for feature in features } else : data = { feature : '' for feature in features } data [ 'company_name' ] = company wiki_data . append ( data ) except KeyError : pass 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 en.wikipedia.org (parse) Walmart en.wikipedia.org (imageinfo) File:Walmart store exterior 5266815680.jpg Walmart (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Walmart s... infobox: <dict(30)> name, logo, logo_caption, image, image_size,... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:W... pageid: 33589 parsetree: <str(347698)> <root><template><title>about</title><pa... requests: <list(2)> parse, imageinfo title: Walmart wikibase: Q483551 wikidata_url: https://www.wikidata.org/wiki/Q483551 wikitext: <str(277438)> {{about|the retail chain|other uses}}{{p... } en.wikipedia.org (parse) ExxonMobil en.wikipedia.org (imageinfo) File:Exxonmobil-headquarters-1.jpg ExxonMobil (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Exxonmobi... infobox: <dict(30)> name, logo, image, image_caption, type, trad... iwlinks: <list(4)> https://commons.wikimedia.org/wiki/Category:E... pageid: 18848197 parsetree: <str(192545)> <root><template><title>short descriptio... requests: <list(2)> parse, imageinfo title: ExxonMobil wikibase: Q156238 wikidata_url: https://www.wikidata.org/wiki/Q156238 wikitext: <str(157036)> {{short description|American multination... } en.wikipedia.org (parse) Berkshire Hathaway Berkshire Hathaway (en) data { image: <list(0)> infobox: <dict(24)> name, former_name, logo, image, image_captio... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:B... pageid: 314333 parsetree: <str(105467)> <root><template><title>short descriptio... requests: <list(1)> parse title: Berkshire Hathaway wikibase: Q217583 wikidata_url: https://www.wikidata.org/wiki/Q217583 wikitext: <str(89908)> {{short description|American multinationa... } en.wikipedia.org (parse) Apple Inc. en.wikipedia.org (imageinfo) File:Apple park cupertino 2019.jpg Apple Inc. (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Apple par... infobox: <dict(35)> name, logo, logo_size, image, image_size, im... iwlinks: <list(8)> https://commons.wikimedia.org/wiki/Special:Se... pageid: 856 parsetree: <str(419620)> <root><template><title>Redirect</title>... requests: <list(2)> parse, imageinfo title: Apple Inc. wikibase: Q312 wikidata_url: https://www.wikidata.org/wiki/Q312 wikitext: <str(335917)> {{Redirect|Apple (company)|other compani... } en.wikipedia.org (parse) UnitedHealth Group UnitedHealth Group (en) data { infobox: <dict(17)> name, logo, type, traded_as, founder, key_pe... pageid: 1845551 parsetree: <str(87066)> <root><template><title>Redirect</title><... requests: <list(1)> parse title: UnitedHealth Group wikibase: Q2103926 wikidata_url: https://www.wikidata.org/wiki/Q2103926 wikitext: <str(74588)> {{Redirect|UnitedHealthcare|the cycling t... } en.wikipedia.org (parse) McKesson Corporation McKesson Corporation (en) data { infobox: <dict(19)> name, logo, type, traded_as, founder, locati... pageid: 1041603 parsetree: <str(40259)> <root><template><title>Redirect</title><... requests: <list(1)> parse title: McKesson Corporation wikibase: Q570473 wikidata_url: https://www.wikidata.org/wiki/Q570473 wikitext: <str(32180)> {{Redirect|McKesson}}{{short description|... } en.wikipedia.org (parse) CVS Health CVS Health (en) data { infobox: <dict(28)> name, logo, logo_size, former_name, type, tr... pageid: 10377597 parsetree: <str(70825)> <root><template><title>short description... requests: <list(1)> parse title: CVS Health wikibase: Q624375 wikidata_url: https://www.wikidata.org/wiki/Q624375 wikitext: <str(54943)> {{short description|American healthcare c... } en.wikipedia.org (parse) Amazon (company) en.wikipedia.org (imageinfo) File:Amazon Spheres 05.jpg Amazon (company) (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Amazon Sp... infobox: <dict(32)> name, logo, logo_size, image, image_caption,... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:A... pageid: 90451 parsetree: <str(183373)> <root><template><title>short descriptio... requests: <list(2)> parse, imageinfo title: Amazon (company) wikibase: Q3884 wikidata_url: https://www.wikidata.org/wiki/Q3884 wikitext: <str(142559)> {{short description|American technology ... } en.wikipedia.org (parse) AT&T en.wikipedia.org (imageinfo) File:AT&THQDallas.jpg AT&T (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:AT&THQDal... infobox: <dict(28)> name, logo, logo_size, image, image_size, im... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:AT%26T pageid: 17555269 parsetree: <str(136294)> <root><template><title>about</title><pa... requests: <list(2)> parse, imageinfo title: AT&T wikibase: Q35476 wikidata_url: https://www.wikidata.org/wiki/Q35476 wikitext: <str(109258)> {{about|the company known as AT&T since ... } en.wikipedia.org (parse) General Motors en.wikipedia.org (imageinfo) File:RenCen.JPG General Motors (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:RenCen.JP... infobox: <dict(30)> name, former_name, logo, logo_size, image, i... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:G... pageid: 12102 parsetree: <str(187427)> <root><template><title>short descriptio... requests: <list(2)> parse, imageinfo title: General Motors wikibase: Q81965 wikidata_url: https://www.wikidata.org/wiki/Q81965 wikitext: <str(146919)> {{short description|American automotive ... } en.wikipedia.org (parse) Ford Motor Company en.wikipedia.org (imageinfo) File:FordGlassHouse.jpg Ford Motor Company (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:FordGlass... infobox: <dict(27)> name, logo, image, image_size, image_caption... iwlinks: <list(8)> https://commons.wikimedia.org/wiki/Category:F... pageid: 30433662 parsetree: <str(197053)> <root><template><title>Redirect</title>... requests: <list(2)> parse, imageinfo title: Ford Motor Company wikibase: Q44294 wikidata_url: https://www.wikidata.org/wiki/Q44294 wikitext: <str(160388)> {{Redirect|Ford}}{{pp-semi-indef}}{{pp-m... } en.wikipedia.org (parse) AmerisourceBergen AmerisourceBergen (en) data { infobox: <dict(17)> name, logo, type, traded_as, foundation, loc... pageid: 1445945 parsetree: <str(21535)> <root><template><title>short description... requests: <list(1)> parse title: AmerisourceBergen wikibase: Q470156 wikidata_url: https://www.wikidata.org/wiki/Q470156 wikitext: <str(16172)> {{short description|American healthcare c... } en.wikipedia.org (parse) Chevron Corporation Chevron Corporation (en) data { image: <list(0)> infobox: <dict(24)> name, logo, logo_size, logo_caption, image, ... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:C... pageid: 284749 parsetree: <str(125957)> <root><template><title>short descriptio... requests: <list(1)> parse title: Chevron Corporation wikibase: Q319642 wikidata_url: https://www.wikidata.org/wiki/Q319642 wikitext: <str(102739)> {{short description|American multination... } en.wikipedia.org (parse) Cardinal Health Cardinal Health (en) data { infobox: <dict(17)> name, logo, type, traded_as, industry, found... pageid: 1041632 parsetree: <str(33864)> <root><template><title>Infobox company</... requests: <list(1)> parse title: Cardinal Health wikibase: Q902397 wikidata_url: https://www.wikidata.org/wiki/Q902397 wikitext: <str(26601)> {{Infobox company| name = Cardinal Health... } en.wikipedia.org (parse) Costco en.wikipedia.org (imageinfo) File:Costcoheadquarters.jpg Costco (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Costcohea... infobox: <dict(35)> name, logo, logo_caption, image, image_size,... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:Costco pageid: 446056 parsetree: <str(101971)> <root><template><title>Distinguish</tit... requests: <list(2)> parse, imageinfo title: Costco wikibase: Q715583 wikidata_url: https://www.wikidata.org/wiki/Q715583 wikitext: <str(76641)> {{Distinguish|COSCO|Cosco (India) Limited... } en.wikipedia.org (parse) Verizon Communications en.wikipedia.org (imageinfo) File:Verizon Building (8156005279).jpg Verizon Communications (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Verizon B... infobox: <dict(32)> name, logo, image, image_size, image_caption... iwlinks: <list(3)> https://commons.wikimedia.org/wiki/Category:T... pageid: 18619278 parsetree: <str(154091)> <root><template><title>redirect</title>... requests: <list(2)> parse, imageinfo title: Verizon Communications wikibase: Q467752 wikidata_url: https://www.wikidata.org/wiki/Q467752 wikitext: <str(130536)> {{redirect|Verizon|its mobile network su... } en.wikipedia.org (parse) Kroger en.wikipedia.org (imageinfo) File:Cincinnati-kroger-building.jpg Kroger (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Cincinnat... infobox: <dict(24)> name, logo, image, image_caption, type, trad... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:Kroger pageid: 367762 parsetree: <str(121075)> <root><template><title>short descriptio... requests: <list(2)> parse, imageinfo title: Kroger wikibase: Q153417 wikidata_url: https://www.wikidata.org/wiki/Q153417 wikitext: <str(101498)> {{short description|American multination... } en.wikipedia.org (parse) General Electric General Electric (en) data { infobox: <dict(20)> name, logo, type, traded_as, ISIN, industry,... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:G... pageid: 12730 parsetree: <str(165322)> <root><template><title>redirect</title>... requests: <list(1)> parse title: General Electric wikibase: Q54173 wikidata_url: https://www.wikidata.org/wiki/Q54173 wikitext: <str(140011)> {{redirect|GE}}{{distinguish|text=the fo... } en.wikipedia.org (parse) Walgreens Boots Alliance Walgreens Boots Alliance (en) data { infobox: <dict(29)> name, logo, logo_size, type, traded_as, pred... pageid: 44732533 parsetree: <str(32556)> <root><template><title>Use mdy dates</ti... requests: <list(1)> parse title: Walgreens Boots Alliance wikibase: Q18712620 wikidata_url: https://www.wikidata.org/wiki/Q18712620 wikitext: <str(25068)> {{Use mdy dates|date=October 2019}}{{shor... } en.wikipedia.org (parse) JPMorgan Chase en.wikipedia.org (imageinfo) File:383 Madison Ave Bear Stearns C ... JPMorgan Chase (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:383 Madis... infobox: <dict(31)> name, logo, image, image_caption, type, trad... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:J... pageid: 231001 parsetree: <str(144960)> <root><template><title>About</title><pa... requests: <list(2)> parse, imageinfo title: JPMorgan Chase wikibase: Q192314 wikidata_url: https://www.wikidata.org/wiki/Q192314 wikitext: <str(117507)> {{About|JPMorgan Chase & Co|its main sub... } Let's take a look at the first instance in wiki_data i.e. Walmart , 1 wiki_data [ 0 ] 1 2 3 4 5 6 7 8 9 10 11 12 {'founder': '[[Sam Walton]]', 'location_country': 'U.S.', 'revenue': '{{increase}} {{US$|523.964 billion|link|=|yes}} {{small|([[Fiscal Year|FY]] 2020)}}', 'operating_income': '{{decrease}} {{US$|20.568 billion}} {{small|(FY 2020)}}', 'net_income': '{{increase}} {{US$|14.881 billion}} {{small|(FY 2020)}}', 'assets': '{{increase}} {{US$|236.495 billion}} {{small|(FY 2020)}}', 'equity': '{{increase}} {{US$|74.669 billion}} {{small|(FY 2020)}}', 'type': '[[Public company|Public]]', 'industry': '[[Retail]]', 'products': '{{hlist|Electronics|Movies and music|Home and furniture|Home improvement|Clothing|Footwear|Jewelry|Toys|Health and beauty|Pet supplies|Sporting goods and fitness|Auto|Photo finishing|Craft supplies|Party supplies|Grocery}}', 'num_employees': '{{plainlist|\\n* 2.2|nbsp|million, Worldwide (2018)|ref| name=\"xbrlus_1\" |\\n* 1.5|nbsp|million, U.S. (2017)|ref| name=\"Walmart\"|{{cite web |url = http://corporate.walmart.com/our-story/locations/united-states |title = Walmart Locations Around the World \u2013 United States |publisher = |url-status=live |archiveurl = https://web.archive.org/web/20150926012456/http://corporate.walmart.com/our-story/locations/united-states |archivedate = September 26, 2015 |df = mdy-all }}|</ref>|\\n* 700,000, International}} {{nbsp}} million, Worldwide (2018) * 1.5 {{nbsp}} million, U.S. (2017) * 700,000, International', 'company_name': 'Walmart'} So, we have successfully retrieved all the infobox data for the companies. Also we can notice that some additional wrangling and cleaning is required which we will perform in the next section. Finally, let's export the scraped infoboxes as a single JSON file to a convenient location as follows, 1 2 with open ( 'infoboxes.json' , 'w' ) as file : json . dump ( wiki_data , file )","title":"Retrieving the infoboxes"},{"location":"section-3-API-based-scraping/#references","text":"https://phpenthusiast.com/blog/what-is-rest-api https://github.com/siznax/wptools/wiki/Data-captured https://en.wikipedia.org/w/api.php https://wikipedia.readthedocs.io/en/latest/code.html","title":"References"},{"location":"section-4-wrangling-and-analysis/","text":"In this section, we will clean, join perform some basic analysis on the data to answer a few questions. Let's import a few libraries we will require. 1 2 3 4 5 6 7 8 import re import json import pandas as pd from wordcloud import WordCloud import matplotlib.pyplot as plt % matplotlib inline plt . style . use ( 'ggplot' ) # setting the style to ggplot Now let's read the infoboxes.json file we exported from Section 3 to a dictionary as follows, 1 2 3 4 # If you dont have the file, you can use the below code to fetch it: import urllib.request url = 'https://raw.githubusercontent.com/MonashDataFluency/python-web-scraping/master/data/infoboxes.json' urllib . request . urlretrieve ( url , 'infoboxes.json' ) 1 ('infoboxes.json', <http.client.HTTPMessage at 0x7fa6328de910>) 1 2 with open ( 'infoboxes.json' , 'r' ) as file : wiki_data = json . load ( file ) and have a quick glance at the first element of the same, 1 wiki_data [ 0 ] 1 2 3 4 5 6 7 8 9 10 11 12 {'founder': '[[Sam Walton]]', 'location_country': 'U.S.', 'revenue': '{{increase}} {{US$|514.405 billion|link|=|yes}} (2019)', 'operating_income': '{{increase}} {{US$|21.957 billion}} (2019)', 'net_income': '{{decrease}} {{US$|6.67 billion}} (2019)', 'assets': '{{increase}} {{US$|219.295 billion}} (2019)', 'equity': '{{decrease}} {{US$|79.634 billion}} (2019)', 'type': '[[Public company|Public]]', 'industry': '[[Retail]]', 'products': '{{hlist|Electronics|Movies and music|Home and furniture|Home improvement|Clothing|Footwear|Jewelry|Toys|Health and beauty|Pet supplies|Sporting goods and fitness|Auto|Photo finishing|Craft supplies|Party supplies|Grocery}}', 'num_employees': '{{plainlist|\\n* 2.2|nbsp|million, Worldwide (2018)|ref| name=\"xbrlus_1\" |\\n* 1.5|nbsp|million, U.S. (2017)|ref| name=\"Walmart\"|{{cite web |url = http://corporate.walmart.com/our-story/locations/united-states |title = Walmart Locations Around the World \u2013 United States |publisher = |url-status=live |archiveurl = https://web.archive.org/web/20150926012456/http://corporate.walmart.com/our-story/locations/united-states |archivedate = September 26, 2015 |df = mdy-all }}|</ref>|\\n* 700,000, International}} {{nbsp}} million, Worldwide (2018) * 1.5 {{nbsp}} million, U.S. (2017) * 700,000, International', 'company_name': 'Walmart'} As evident from above, we can see that the data is in quite a messy format. For this excercise, we will primarily focus on the following attributes : products industries and assets and try to answer a few questions using the data. What type of products are sold by the top 20 companies? \u00b6 Looking at a sample for products , 1 2 3 for i in range ( 0 , 3 ): print ( wiki_data [ i ][ 'products' ]) print ( ' \\n ' ) 1 2 3 4 5 6 7 8 9 10 11 12 {{hlist|Electronics|Movies and music|Home and furniture|Home improvement|Clothing|Footwear|Jewelry|Toys|Health and beauty|Pet supplies|Sporting goods and fitness|Auto|Photo finishing|Craft supplies|Party supplies|Grocery}} {{Unbulleted list | [[Crude oil]] | [[Oil products]] | [[Natural gas]] | [[Petrochemical]]s | [[Power generation]]}} [[Investment|Diversified investments]], [[Insurance#Types|Property & casualty insurance]], [[Public utility|Utilities]], [[Restaurants]], [[Food processing]], [[Aerospace]], [[Toys]], [[Mass media|Media]], [[Automotive industry|Automotive]], [[Sports equipment|Sporting goods]], [[Final good|Consumer products]], [[Internet]], [[Real estate]] We can observe that we need to : Extract only the products from between {{ and }} or [[ and ]] , and Only keep alphanumeric characters, - and preserve the spaces between Let's define a regular expressions to clean and extract the products from the dictionary. 1 regex = re . compile ( r '[\\w\\s-]+' ) Regex breakdown : \\w : Indicates alphanumeric characters i.e. a-z , A-z and 0-9 \\s : Indicates a space [..] : Captures a single instance of the above (a single letter or number) + : Captures one or more of the above Note: \\ is used for escaping and to avoid the regex from interpreting \\w and \\s as alphabets: w and s . Also, we notice several words are part of HTML tags and not relevant to the data we require. Lets define a list called rogue_words to handle the same, 1 2 # list of words we want filtered out rogue_words = [ 'unbulleted list' , 'hlist' , 'flat list' , 'flatlist' , 'plainlist' , 's' , 'br' , '' ] Now we can extract the products as follows, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 products = [] data = [] for x in wiki_data : product = x [ 'products' ] . lower () # get products and lowercase them # replace 'and' with '|' and ']]' with empty string product = product . replace ( ' and ' , '|' ) . replace ( ']]' , '' ) product = regex . findall ( product ) # extract all alphanumeric occurences product = [ p . strip () for p in product ] # get rid of leading or trailing whitespace # filter rogue words and de-duplicate product = list ( set ([ p for p in product if p not in rogue_words ])) # add to data data . append ({ 'wiki_title' : x [ 'company_name' ], 'product' : ', ' . join ( product ) }) # add to list of products products . extend ( product ) print ( products ) 1 ['beauty', 'footwear', 'furniture', 'party supplies', 'auto', 'fitness', 'pet supplies', 'movies', 'electronics', 'jewelry', 'craft supplies', 'clothing', 'grocery', 'music', 'photo finishing', 'home improvement', 'home', 'toys', 'sporting goods', 'health', 'oil products', 'petrochemicals', 'crude oil', 'power generation', 'natural gas', 'diversified investments', 'mass media', 'automotive industry', 'media', 'final good', 'internet', 'food processing', 'public utility', 'sports equipment', 'insurance', 'investment', 'restaurants', 'casualty insurance', 'real estate', 'types', 'aerospace', 'automotive', 'consumer products', 'property', 'utilities', 'toys', 'sporting goods', 'ipad', 'apple tv', 'ilife', 'application', 'ipados', 'siri', 'ios', 'shazam', 'iwork', 'ipod', 'homepod', 'macintosh', 'tvos', 'logic pro', 'garageband', 'watchos', 'apple watch', 'macos', 'iphone', 'final cut pro', 'ingenix', 'service', 'specialized care services', 'uniprise', 'health care', 'economics', 'services', 'pharmaceuticals', 'medical technology', 'health care services', 'amazon fire os', 'fire os', 'amazon fire tablet', 'amazon kindle', 'amazon fire tv', 'amazon fire', 'amazon echo', 'film production', 'sports management', 'video games', 'landline', 'podcasts', 'publishing', 'pay television', 'satellite television', 'television production', 'internet service provider', 'network security', 'iptv', 'cable television', 'fixed-line telephones', 'ott services', 'mobile telephones', 'internet services', 'digital television', 'mobile phone', 'news agency', 'home security', 'over-the-top media services', 'broadband', 'filmmaking', 'automobiles', 'car', 'automobile parts', 'commercial vehicles', 'automobiles', 'automotive parts', 'pickup trucks', 'car', 'luxury car', 'commercial vehicles', 'list of auto parts', 'suvs', 'luxury vehicles', 'commercial vehicle', 'pharmacy services', 'pharmaceuticals', 'other', 'see chevron products', 'petrochemicals', 'marketing brands', 'petroleum', 'natural gas', 'pharmaceutical products', 'medical', 'services', 'mobile phone', 'iptv', 'telematics', 'internet', 'cable television', 'broadband', 'internet of things', 'landline', 'digital media', 'digital television', 'superstore', 'other specialty', 'supermarket', 'supercenter', 'lighting', 'finance', 'wind turbines', 'electrical distribution', 'software', 'aircraft engines', 'energy', 'health care', 'electric power distribution', 'electric motors', 'drug store', 'pharmacy', 'broker services', 'finance', 'institutional investing', 'trustee services', 'currency exchange', 'debt settlement', 'financial markets', 'venture capital', 'foreign exchange market', 'loan servicing', 'mortgage brokers', 'index funds', 'investment banking', 'hedge funds', 'exchange-traded funds', 'mortgage-backed security', 'mortgage', 'mutual funds', 'underwriting', 'stock trading', 'security', 'investment management', 'prime brokerage', 'mortgage brokering', 'retail banking', 'backed securities', 'mortgage loans', 'bond', 'digital banking', 'merchant services', 'money market trading', 'american depositary receipts', 'financial analysis', 'credit cards', 'institutional investor', 'private equity', 'commodity market', 'insurance', 'capital market services', 'investment', 'retail', 'mobile banking', 'treasury services', 'alternative financial services', 'futures exchange', 'private banking', 'portfolios', 'subprime lending', 'bond trading', 'financial capital', 'custodian banking', 'commodities trading', 'risk management', 'pension funds', 'estate planning', 'brokerage', 'collateralized debt obligations', 'portfolio', 'asset management', 'securities lending', 'wholesale mortgage lenders', 'credit default swap', 'wealth management', 'remittance', 'wholesale mortgage lending', 'wire transfers', 'investment capital', 'credit derivative trading', 'information processing', 'security services', 'stock trader', 'asset allocation', 'wholesale funding', 'commercial banking'] Now let's create a wordcloud function which will visually inform us about which products are more prominent than the others. The size of the word would indicate its frequency. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def create_wordcloud ( items , stopwords = []): '''create the wordcloud object args items : the items we need to display stopwords : a list of stopwords to filter out tokens ''' text = ' ' . join ( items ) wordcloud = WordCloud ( width = 1600 , height = 800 , margin = 0 , stopwords = stopwords ) . generate ( text ) # optionally we can set max_words=20 plt . figure ( figsize = ( 20 , 10 )) # set the final figure size plt . imshow ( wordcloud , interpolation = \"bilinear\" ) # pass the image and set interpolation type plt . axis ( \"off\" ) # turn all axes off plt . tight_layout ( pad = 0 ) # no padding plt . show () # display plot Creating wordclouds for products , 1 create_wordcloud ( products ) What type of industries do the top 20 company belong from? \u00b6 Similarly, looking at a sample for industry , 1 2 for i in range ( 0 , 5 ): print ( wiki_data [ i ][ 'industry' ]) 1 2 3 4 5 [[Retail]] [[Energy industry|Energy]]: [[Oil and gas industry|Oil and gas]] [[Conglomerate (company)|Conglomerate]] {{Unbulleted list | [[Computer hardware]] | [[Computer software]] | [[Consumer electronics]] | [[Cloud computing]] | [[Digital distribution]] | [[Fabless manufacturing|Fabless silicon design]] | [[Semiconductors]] | [[Financial technology]] | [[Artificial intelligence]]}} [[Managed health care]] We can observe that we need to : - Extract products from between [[ and ]] - Split and seperate by the delimiter | - Only keep alphanumeric characters, - and preserve spaces Using the same regex and rogue_words to clean and extract the industries, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 industries = [] for i , x in enumerate ( wiki_data ): industry = x [ 'industry' ] . lower () # get industries and lowercase # replace 'and' with '|' and ']]' with empty space industry = industry . replace ( ' and ' , '|' ) . replace ( ']]' , '' ) # extract all industries industry = regex . findall ( industry ) # strip trailing and leading spaces industry = [ i . strip () for i in industry ] # filter rogue words and de-duplicate industry = list ( set ([ i for i in industry if i not in rogue_words ])) # add to data data [ i ][ 'industry' ] = ', ' . join ( industry ) # add to list of industries industries . extend ( industry ) print ( industries ) 1 ['retail', 'gas industry', 'gas', 'energy', 'energy industry', 'oil', 'conglomerate', 'company', 'artificial intelligence', 'cloud computing', 'digital distribution', 'computer software', 'financial technology', 'computer hardware', 'semiconductors', 'consumer electronics', 'fabless manufacturing', 'fabless silicon design', 'managed health care', 'healthcare', 'health care', 'retail', 'artificial intelligence', 'e-commerce', 'cloud computing', 'digital distribution', 'consumer electronics', 'grocery stores', 'technology', 'entertainment', 'mass media', 'technology company', 'telecommunications industry', 'telecommunications', 'automotive', 'automotive industry', 'automotive', 'automotive industry', 'pharmaceutical', 'gas', 'gas industry', 'oil', 'pharmaceuticals', 'retail', 'telecommunications industry', 'telecommunications', 'mass media', 'retail', 'conglomerate', 'company', 'pharmaceutical', 'retail', 'financial services', 'banking'] Just as before let's create a wordcloud for industry , 1 create_wordcloud ( industries , [ 'industry' ]) # adding \"industry\" to the stopword list What the assets of the top 20 companies look like? \u00b6 Taking a look at a sample of assets below, 1 2 for i in range ( 0 , len ( wiki_data )): print ( wiki_data [ i ][ 'assets' ]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 {{increase}} {{US$|219.295 billion}} (2019) {{Nowrap|Decrease| |US$|346.2 billion|ref| name=\"201310K\"}} {{Decrease}} {{US$|346.2 billion}} {{increase}} US$707.8 billion (2018) {{Decrease}} {{US$|338.516&nbsp;billion}} {{increase}} $173.889 billion (2019) {{nowrap|increase| |US$|60.381 billion| |small|(2018)|ref| name=FY}} {{increase}} {{US$|60.381 billion}} {{small|(2018)}} {{increase}} {{US$|196.456 billion}} {{decrease}} {{US$|162.648 billion}} {{increase}} {{US$|link|=|yes}} 531 billion (2018) {{increase}} US$227.339 billion {{small|(2018)}} {{decrease}} {{US$|256.54 billion}} {{small|(2018)}} {{increase}} {{US$|37.66 billion}} (2018) {{decrease}} {{US$|253.9 billion}} {{small|(2018)}} {{increase}} US$39.95 billion {{small|(2018)}} {{increase}} US$45.40 billion {{increase}} {{US$|264.82 billion}} {{increase}} {{US$|38.11 billion}} (2019) {{nowrap|Decrease| US$ 309.129 billion |small|(2018)}} {{Decrease}} US$ 309.129 billion {{small|(2018)}} {{decrease}} {{US$|67.59 billion}} {{increase}} [[United States dollar|US$]]2.687 [[trillion]] We would need to : Extract both numbers and the unit i.e. billion or trillion Keep only the monetary values (discard year) We can also observe that the asset value always appears first and then the year follows. Defining a simple regular expression for the same, 1 regex1 = re . compile ( '([\\d\\.]+)' ) Regex breakdown : ([\\d\\.]+) : matches and captures one or more (+) numbers (0-9) with decimal (.) . 1 regex2 = re . compile ( '(billion|trillion)' ) Regex breakdown : (billion|trillion) : matches and captures either billion or trillion 1 2 3 4 5 6 7 8 9 10 11 assets = [] for i , x in enumerate ( wiki_data ): y = x [ 'assets' ] # get assets z = regex1 . findall ( y )[ 0 ] # extract assets unit = regex2 . findall ( y )[ 0 ] # extract the unit asset = float ( z ) # convert to the numeric data [ i ][ 'assets' ] = str ( asset ) + ' ' + unit # add to data # add to list of assets assets . append ({ 'company' : x [ 'company_name' ], 'value' : asset , 'unit' : unit }) assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [{'company': 'Walmart', 'value': 219.295, 'unit': 'billion'}, {'company': 'ExxonMobil', 'value': 346.2, 'unit': 'billion'}, {'company': 'Berkshire Hathaway', 'value': 707.8, 'unit': 'billion'}, {'company': 'Apple Inc.', 'value': 338.516, 'unit': 'billion'}, {'company': 'UnitedHealth Group', 'value': 173.889, 'unit': 'billion'}, {'company': 'McKesson Corporation', 'value': 60.381, 'unit': 'billion'}, {'company': 'CVS Health', 'value': 196.456, 'unit': 'billion'}, {'company': 'Amazon (company)', 'value': 162.648, 'unit': 'billion'}, {'company': 'AT&T', 'value': 531.0, 'unit': 'billion'}, {'company': 'General Motors', 'value': 227.339, 'unit': 'billion'}, {'company': 'Ford Motor Company', 'value': 256.54, 'unit': 'billion'}, {'company': 'AmerisourceBergen', 'value': 37.66, 'unit': 'billion'}, {'company': 'Chevron Corporation', 'value': 253.9, 'unit': 'billion'}, {'company': 'Cardinal Health', 'value': 39.95, 'unit': 'billion'}, {'company': 'Costco', 'value': 45.4, 'unit': 'billion'}, {'company': 'Verizon Communications', 'value': 264.82, 'unit': 'billion'}, {'company': 'Kroger', 'value': 38.11, 'unit': 'billion'}, {'company': 'General Electric', 'value': 309.129, 'unit': 'billion'}, {'company': 'Walgreens Boots Alliance', 'value': 67.59, 'unit': 'billion'}, {'company': 'JPMorgan Chase', 'value': 2.687, 'unit': 'trillion'}] Since we have both billion as well as trillion , let's normalize all the values, 1 2 3 4 for i , asset in enumerate ( assets ): if asset [ 'unit' ] == 'trillion' : # if unit is in trillion assets [ i ][ 'value' ] = asset [ 'value' ] * 1000 # convert trillion to billion assets [ i ][ 'unit' ] = 'billion' And create a new dataframe from the same, 1 2 df_assets = pd . DataFrame ( assets ) df_assets company value unit 0 Walmart 219.295 billion 1 ExxonMobil 346.2 billion 2 Berkshire Hathaway 707.8 billion 3 Apple Inc. 338.516 billion 4 UnitedHealth Group 173.889 billion 5 McKesson Corporation 60.381 billion 6 CVS Health 196.456 billion 7 Amazon (company) 162.648 billion 8 AT&T 531 billion 9 General Motors 227.339 billion 10 Ford Motor Company 256.54 billion 11 AmerisourceBergen 37.66 billion 12 Chevron Corporation 253.9 billion 13 Cardinal Health 39.95 billion 14 Costco 45.4 billion 15 Verizon Communications 264.82 billion 16 Kroger 38.11 billion 17 General Electric 309.129 billion 18 Walgreens Boots Alliance 67.59 billion 19 JPMorgan Chase 2687 billion Now finally let's create a bar plot showcasing the assets from all the companies, 1 2 3 4 5 6 7 8 9 10 11 12 ax = df_assets . plot ( kind = 'bar' , title = \"Assets from the Top 20 Companies on Fortune 500\" , color = 'skyblue' , figsize = ( 15 , 10 ), legend = True , fontsize = 12 ) ax . set_xticklabels ( df_assets . company ) ax . set_xlabel ( \"Company Name\" , fontsize = 12 ) ax . set_ylabel ( \"Assets (in Billions)\" , fontsize = 12 ) plt . show () Now let's create a new dataframe containing data related to products , industry and assets as follows, 1 2 df_wiki = pd . DataFrame ( data ) df_wiki . head () wiki_title product industry assets 0 Walmart pet supplies, party supplies, clothing, footwear, photo finishing, fitness, auto, grocery, electronics, home, movies, sporting goods, toys, beauty, jewelry, craft supplies, home improvement, music, furniture, health retail 219.295 billion 1 ExxonMobil petrochemicals, crude oil, oil products, natural gas, power generation gas, energy, oil, energy industry, gas industry 346.2 billion 2 Berkshire Hathaway food processing, casualty insurance, property, mass media, media, aerospace, insurance, final good, utilities, real estate, automotive industry, consumer products, internet, public utility, restaurants, sports equipment, sporting goods, toys, types, diversified investments, investment, automotive company, conglomerate 707.8 billion 3 Apple Inc. siri, iphone, watchos, tvos, shazam, ilife, iwork, final cut pro, ipod, application, apple watch, ipad, logic pro, macintosh, homepod, apple tv, macos, garageband, ios, ipados fabless manufacturing, cloud computing, semiconductors, artificial intelligence, fabless silicon design, consumer electronics, computer software, financial technology, digital distribution, computer hardware 338.516 billion 4 UnitedHealth Group health care, uniprise, service, specialized care services, ingenix, economics, services managed health care 173.889 billion And finally let's combine the datasets from Section 2 and 3 as follows, 1 2 3 4 # If you dont have the file, you can use the below code to fetch it: import urllib.request url = 'https://raw.githubusercontent.com/MonashDataFluency/python-web-scraping/master/data/fortune_500_companies.csv' urllib . request . urlretrieve ( url , 'fortune_500_companies.csv' ) 1 ('fortune_500_companies.csv', <http.client.HTTPMessage at 0x7fa6743ebc40>) 1 2 3 df = pd . read_csv ( 'fortune_500_companies.csv' , nrows = 20 ) # reading only the first 20 rows df = pd . concat ([ df , df_wiki ], axis = 1 ) # concatenating both the datasets df rank company_name company_website wiki_title product industry assets 0 1 Walmart http://www.stock.walmart.com Walmart beauty, footwear, furniture, party supplies, auto, fitness, pet supplies, movies, electronics, jewelry, craft supplies, clothing, grocery, music, photo finishing, home improvement, home, toys, sporting goods, health retail 219.295 billion 1 2 Exxon Mobil http://www.exxonmobil.com ExxonMobil oil products, petrochemicals, crude oil, power generation, natural gas gas industry, gas, energy, energy industry, oil 346.2 billion 2 3 Berkshire Hathaway http://www.berkshirehathaway.com Berkshire Hathaway diversified investments, mass media, automotive industry, media, final good, internet, food processing, public utility, sports equipment, insurance, investment, restaurants, casualty insurance, real estate, types, aerospace, automotive, consumer products, property, utilities, toys, sporting goods conglomerate, company 707.8 billion 3 4 Apple http://www.apple.com Apple Inc. ipad, apple tv, ilife, application, ipados, siri, ios, shazam, iwork, ipod, homepod, macintosh, tvos, logic pro, garageband, watchos, apple watch, macos, iphone, final cut pro artificial intelligence, cloud computing, digital distribution, computer software, financial technology, computer hardware, semiconductors, consumer electronics, fabless manufacturing, fabless silicon design 338.516 billion 4 5 UnitedHealth Group http://www.unitedhealthgroup.com UnitedHealth Group ingenix, service, specialized care services, uniprise, health care, economics, services managed health care 173.889 billion 5 6 McKesson http://www.mckesson.com McKesson Corporation pharmaceuticals, medical technology, health care services healthcare 60.381 billion 6 7 CVS Health http://www.cvshealth.com CVS Health health care, retail 196.456 billion 7 8 Amazon.com http://www.amazon.com Amazon (company) amazon fire os, fire os, amazon fire tablet, amazon kindle, amazon fire tv, amazon fire, amazon echo artificial intelligence, e-commerce, cloud computing, digital distribution, consumer electronics, grocery stores 162.648 billion 8 9 AT&T http://www.att.com AT&T film production, sports management, video games, landline, podcasts, publishing, pay television, satellite television, television production, internet service provider, network security, iptv, cable television, fixed-line telephones, ott services, mobile telephones, internet services, digital television, mobile phone, news agency, home security, over-the-top media services, broadband, filmmaking technology, entertainment, mass media, technology company, telecommunications industry, telecommunications 531.0 billion 9 10 General Motors http://www.gm.com General Motors automobiles, car, automobile parts, commercial vehicles automotive, automotive industry 227.339 billion 10 11 Ford Motor http://www.corporate.ford.com Ford Motor Company automobiles, automotive parts, pickup trucks, car, luxury car, commercial vehicles, list of auto parts, suvs, luxury vehicles, commercial vehicle automotive, automotive industry 256.54 billion 11 12 AmerisourceBergen http://www.amerisourcebergen.com AmerisourceBergen pharmacy services, pharmaceuticals pharmaceutical 37.66 billion 12 13 Chevron http://www.chevron.com Chevron Corporation other, see chevron products, petrochemicals, marketing brands, petroleum, natural gas gas, gas industry, oil 253.9 billion 13 14 Cardinal Health http://www.cardinalhealth.com Cardinal Health pharmaceutical products, medical, services pharmaceuticals 39.95 billion 14 15 Costco http://www.costco.com Costco retail 45.4 billion 15 16 Verizon http://www.verizon.com Verizon Communications mobile phone, iptv, telematics, internet, cable television, broadband, internet of things, landline, digital media, digital television telecommunications industry, telecommunications, mass media 264.82 billion 16 17 Kroger http://www.thekrogerco.com Kroger superstore, other specialty, supermarket, supercenter retail 38.11 billion 17 18 General Electric http://www.ge.com General Electric lighting, finance, wind turbines, electrical distribution, software, aircraft engines, energy, health care, electric power distribution, electric motors conglomerate, company 309.129 billion 18 19 Walgreens Boots Alliance http://www.walgreensbootsalliance.com Walgreens Boots Alliance drug store, pharmacy pharmaceutical, retail 67.59 billion 19 20 JPMorgan Chase http://www.jpmorganchase.com JPMorgan Chase broker services, finance, institutional investing, trustee services, currency exchange, debt settlement, financial markets, venture capital, foreign exchange market, loan servicing, mortgage brokers, index funds, investment banking, hedge funds, exchange-traded funds, mortgage-backed security, mortgage, mutual funds, underwriting, stock trading, security, investment management, prime brokerage, mortgage brokering, retail banking, backed securities, mortgage loans, bond, digital banking, merchant services, money market trading, american depositary receipts, financial analysis, credit cards, institutional investor, private equity, commodity market, insurance, capital market services, investment, retail, mobile banking, treasury services, alternative financial services, futures exchange, private banking, portfolios, subprime lending, bond trading, financial capital, custodian banking, commodities trading, risk management, pension funds, estate planning, brokerage, collateralized debt obligations, portfolio, asset management, securities lending, wholesale mortgage lenders, credit default swap, wealth management, remittance, wholesale mortgage lending, wire transfers, investment capital, credit derivative trading, information processing, security services, stock trader, asset allocation, wholesale funding, commercial banking financial services, banking 2.687 trillion And export them to a csv file, 1 df . to_csv ( 'top_20_companies.csv' , index = False ) Challenge \u00b6 Create a bar chart of equity of all the top 20 Fortune 500 companies and find out which has the highest value.","title":"Wrangling and Analysis"},{"location":"section-4-wrangling-and-analysis/#what-type-of-products-are-sold-by-the-top-20-companies","text":"Looking at a sample for products , 1 2 3 for i in range ( 0 , 3 ): print ( wiki_data [ i ][ 'products' ]) print ( ' \\n ' ) 1 2 3 4 5 6 7 8 9 10 11 12 {{hlist|Electronics|Movies and music|Home and furniture|Home improvement|Clothing|Footwear|Jewelry|Toys|Health and beauty|Pet supplies|Sporting goods and fitness|Auto|Photo finishing|Craft supplies|Party supplies|Grocery}} {{Unbulleted list | [[Crude oil]] | [[Oil products]] | [[Natural gas]] | [[Petrochemical]]s | [[Power generation]]}} [[Investment|Diversified investments]], [[Insurance#Types|Property & casualty insurance]], [[Public utility|Utilities]], [[Restaurants]], [[Food processing]], [[Aerospace]], [[Toys]], [[Mass media|Media]], [[Automotive industry|Automotive]], [[Sports equipment|Sporting goods]], [[Final good|Consumer products]], [[Internet]], [[Real estate]] We can observe that we need to : Extract only the products from between {{ and }} or [[ and ]] , and Only keep alphanumeric characters, - and preserve the spaces between Let's define a regular expressions to clean and extract the products from the dictionary. 1 regex = re . compile ( r '[\\w\\s-]+' ) Regex breakdown : \\w : Indicates alphanumeric characters i.e. a-z , A-z and 0-9 \\s : Indicates a space [..] : Captures a single instance of the above (a single letter or number) + : Captures one or more of the above Note: \\ is used for escaping and to avoid the regex from interpreting \\w and \\s as alphabets: w and s . Also, we notice several words are part of HTML tags and not relevant to the data we require. Lets define a list called rogue_words to handle the same, 1 2 # list of words we want filtered out rogue_words = [ 'unbulleted list' , 'hlist' , 'flat list' , 'flatlist' , 'plainlist' , 's' , 'br' , '' ] Now we can extract the products as follows, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 products = [] data = [] for x in wiki_data : product = x [ 'products' ] . lower () # get products and lowercase them # replace 'and' with '|' and ']]' with empty string product = product . replace ( ' and ' , '|' ) . replace ( ']]' , '' ) product = regex . findall ( product ) # extract all alphanumeric occurences product = [ p . strip () for p in product ] # get rid of leading or trailing whitespace # filter rogue words and de-duplicate product = list ( set ([ p for p in product if p not in rogue_words ])) # add to data data . append ({ 'wiki_title' : x [ 'company_name' ], 'product' : ', ' . join ( product ) }) # add to list of products products . extend ( product ) print ( products ) 1 ['beauty', 'footwear', 'furniture', 'party supplies', 'auto', 'fitness', 'pet supplies', 'movies', 'electronics', 'jewelry', 'craft supplies', 'clothing', 'grocery', 'music', 'photo finishing', 'home improvement', 'home', 'toys', 'sporting goods', 'health', 'oil products', 'petrochemicals', 'crude oil', 'power generation', 'natural gas', 'diversified investments', 'mass media', 'automotive industry', 'media', 'final good', 'internet', 'food processing', 'public utility', 'sports equipment', 'insurance', 'investment', 'restaurants', 'casualty insurance', 'real estate', 'types', 'aerospace', 'automotive', 'consumer products', 'property', 'utilities', 'toys', 'sporting goods', 'ipad', 'apple tv', 'ilife', 'application', 'ipados', 'siri', 'ios', 'shazam', 'iwork', 'ipod', 'homepod', 'macintosh', 'tvos', 'logic pro', 'garageband', 'watchos', 'apple watch', 'macos', 'iphone', 'final cut pro', 'ingenix', 'service', 'specialized care services', 'uniprise', 'health care', 'economics', 'services', 'pharmaceuticals', 'medical technology', 'health care services', 'amazon fire os', 'fire os', 'amazon fire tablet', 'amazon kindle', 'amazon fire tv', 'amazon fire', 'amazon echo', 'film production', 'sports management', 'video games', 'landline', 'podcasts', 'publishing', 'pay television', 'satellite television', 'television production', 'internet service provider', 'network security', 'iptv', 'cable television', 'fixed-line telephones', 'ott services', 'mobile telephones', 'internet services', 'digital television', 'mobile phone', 'news agency', 'home security', 'over-the-top media services', 'broadband', 'filmmaking', 'automobiles', 'car', 'automobile parts', 'commercial vehicles', 'automobiles', 'automotive parts', 'pickup trucks', 'car', 'luxury car', 'commercial vehicles', 'list of auto parts', 'suvs', 'luxury vehicles', 'commercial vehicle', 'pharmacy services', 'pharmaceuticals', 'other', 'see chevron products', 'petrochemicals', 'marketing brands', 'petroleum', 'natural gas', 'pharmaceutical products', 'medical', 'services', 'mobile phone', 'iptv', 'telematics', 'internet', 'cable television', 'broadband', 'internet of things', 'landline', 'digital media', 'digital television', 'superstore', 'other specialty', 'supermarket', 'supercenter', 'lighting', 'finance', 'wind turbines', 'electrical distribution', 'software', 'aircraft engines', 'energy', 'health care', 'electric power distribution', 'electric motors', 'drug store', 'pharmacy', 'broker services', 'finance', 'institutional investing', 'trustee services', 'currency exchange', 'debt settlement', 'financial markets', 'venture capital', 'foreign exchange market', 'loan servicing', 'mortgage brokers', 'index funds', 'investment banking', 'hedge funds', 'exchange-traded funds', 'mortgage-backed security', 'mortgage', 'mutual funds', 'underwriting', 'stock trading', 'security', 'investment management', 'prime brokerage', 'mortgage brokering', 'retail banking', 'backed securities', 'mortgage loans', 'bond', 'digital banking', 'merchant services', 'money market trading', 'american depositary receipts', 'financial analysis', 'credit cards', 'institutional investor', 'private equity', 'commodity market', 'insurance', 'capital market services', 'investment', 'retail', 'mobile banking', 'treasury services', 'alternative financial services', 'futures exchange', 'private banking', 'portfolios', 'subprime lending', 'bond trading', 'financial capital', 'custodian banking', 'commodities trading', 'risk management', 'pension funds', 'estate planning', 'brokerage', 'collateralized debt obligations', 'portfolio', 'asset management', 'securities lending', 'wholesale mortgage lenders', 'credit default swap', 'wealth management', 'remittance', 'wholesale mortgage lending', 'wire transfers', 'investment capital', 'credit derivative trading', 'information processing', 'security services', 'stock trader', 'asset allocation', 'wholesale funding', 'commercial banking'] Now let's create a wordcloud function which will visually inform us about which products are more prominent than the others. The size of the word would indicate its frequency. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def create_wordcloud ( items , stopwords = []): '''create the wordcloud object args items : the items we need to display stopwords : a list of stopwords to filter out tokens ''' text = ' ' . join ( items ) wordcloud = WordCloud ( width = 1600 , height = 800 , margin = 0 , stopwords = stopwords ) . generate ( text ) # optionally we can set max_words=20 plt . figure ( figsize = ( 20 , 10 )) # set the final figure size plt . imshow ( wordcloud , interpolation = \"bilinear\" ) # pass the image and set interpolation type plt . axis ( \"off\" ) # turn all axes off plt . tight_layout ( pad = 0 ) # no padding plt . show () # display plot Creating wordclouds for products , 1 create_wordcloud ( products )","title":"What type of products are sold by the top 20 companies?"},{"location":"section-4-wrangling-and-analysis/#what-type-of-industries-do-the-top-20-company-belong-from","text":"Similarly, looking at a sample for industry , 1 2 for i in range ( 0 , 5 ): print ( wiki_data [ i ][ 'industry' ]) 1 2 3 4 5 [[Retail]] [[Energy industry|Energy]]: [[Oil and gas industry|Oil and gas]] [[Conglomerate (company)|Conglomerate]] {{Unbulleted list | [[Computer hardware]] | [[Computer software]] | [[Consumer electronics]] | [[Cloud computing]] | [[Digital distribution]] | [[Fabless manufacturing|Fabless silicon design]] | [[Semiconductors]] | [[Financial technology]] | [[Artificial intelligence]]}} [[Managed health care]] We can observe that we need to : - Extract products from between [[ and ]] - Split and seperate by the delimiter | - Only keep alphanumeric characters, - and preserve spaces Using the same regex and rogue_words to clean and extract the industries, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 industries = [] for i , x in enumerate ( wiki_data ): industry = x [ 'industry' ] . lower () # get industries and lowercase # replace 'and' with '|' and ']]' with empty space industry = industry . replace ( ' and ' , '|' ) . replace ( ']]' , '' ) # extract all industries industry = regex . findall ( industry ) # strip trailing and leading spaces industry = [ i . strip () for i in industry ] # filter rogue words and de-duplicate industry = list ( set ([ i for i in industry if i not in rogue_words ])) # add to data data [ i ][ 'industry' ] = ', ' . join ( industry ) # add to list of industries industries . extend ( industry ) print ( industries ) 1 ['retail', 'gas industry', 'gas', 'energy', 'energy industry', 'oil', 'conglomerate', 'company', 'artificial intelligence', 'cloud computing', 'digital distribution', 'computer software', 'financial technology', 'computer hardware', 'semiconductors', 'consumer electronics', 'fabless manufacturing', 'fabless silicon design', 'managed health care', 'healthcare', 'health care', 'retail', 'artificial intelligence', 'e-commerce', 'cloud computing', 'digital distribution', 'consumer electronics', 'grocery stores', 'technology', 'entertainment', 'mass media', 'technology company', 'telecommunications industry', 'telecommunications', 'automotive', 'automotive industry', 'automotive', 'automotive industry', 'pharmaceutical', 'gas', 'gas industry', 'oil', 'pharmaceuticals', 'retail', 'telecommunications industry', 'telecommunications', 'mass media', 'retail', 'conglomerate', 'company', 'pharmaceutical', 'retail', 'financial services', 'banking'] Just as before let's create a wordcloud for industry , 1 create_wordcloud ( industries , [ 'industry' ]) # adding \"industry\" to the stopword list","title":"What type of industries do the top 20 company belong from?"},{"location":"section-4-wrangling-and-analysis/#what-the-assets-of-the-top-20-companies-look-like","text":"Taking a look at a sample of assets below, 1 2 for i in range ( 0 , len ( wiki_data )): print ( wiki_data [ i ][ 'assets' ]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 {{increase}} {{US$|219.295 billion}} (2019) {{Nowrap|Decrease| |US$|346.2 billion|ref| name=\"201310K\"}} {{Decrease}} {{US$|346.2 billion}} {{increase}} US$707.8 billion (2018) {{Decrease}} {{US$|338.516&nbsp;billion}} {{increase}} $173.889 billion (2019) {{nowrap|increase| |US$|60.381 billion| |small|(2018)|ref| name=FY}} {{increase}} {{US$|60.381 billion}} {{small|(2018)}} {{increase}} {{US$|196.456 billion}} {{decrease}} {{US$|162.648 billion}} {{increase}} {{US$|link|=|yes}} 531 billion (2018) {{increase}} US$227.339 billion {{small|(2018)}} {{decrease}} {{US$|256.54 billion}} {{small|(2018)}} {{increase}} {{US$|37.66 billion}} (2018) {{decrease}} {{US$|253.9 billion}} {{small|(2018)}} {{increase}} US$39.95 billion {{small|(2018)}} {{increase}} US$45.40 billion {{increase}} {{US$|264.82 billion}} {{increase}} {{US$|38.11 billion}} (2019) {{nowrap|Decrease| US$ 309.129 billion |small|(2018)}} {{Decrease}} US$ 309.129 billion {{small|(2018)}} {{decrease}} {{US$|67.59 billion}} {{increase}} [[United States dollar|US$]]2.687 [[trillion]] We would need to : Extract both numbers and the unit i.e. billion or trillion Keep only the monetary values (discard year) We can also observe that the asset value always appears first and then the year follows. Defining a simple regular expression for the same, 1 regex1 = re . compile ( '([\\d\\.]+)' ) Regex breakdown : ([\\d\\.]+) : matches and captures one or more (+) numbers (0-9) with decimal (.) . 1 regex2 = re . compile ( '(billion|trillion)' ) Regex breakdown : (billion|trillion) : matches and captures either billion or trillion 1 2 3 4 5 6 7 8 9 10 11 assets = [] for i , x in enumerate ( wiki_data ): y = x [ 'assets' ] # get assets z = regex1 . findall ( y )[ 0 ] # extract assets unit = regex2 . findall ( y )[ 0 ] # extract the unit asset = float ( z ) # convert to the numeric data [ i ][ 'assets' ] = str ( asset ) + ' ' + unit # add to data # add to list of assets assets . append ({ 'company' : x [ 'company_name' ], 'value' : asset , 'unit' : unit }) assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [{'company': 'Walmart', 'value': 219.295, 'unit': 'billion'}, {'company': 'ExxonMobil', 'value': 346.2, 'unit': 'billion'}, {'company': 'Berkshire Hathaway', 'value': 707.8, 'unit': 'billion'}, {'company': 'Apple Inc.', 'value': 338.516, 'unit': 'billion'}, {'company': 'UnitedHealth Group', 'value': 173.889, 'unit': 'billion'}, {'company': 'McKesson Corporation', 'value': 60.381, 'unit': 'billion'}, {'company': 'CVS Health', 'value': 196.456, 'unit': 'billion'}, {'company': 'Amazon (company)', 'value': 162.648, 'unit': 'billion'}, {'company': 'AT&T', 'value': 531.0, 'unit': 'billion'}, {'company': 'General Motors', 'value': 227.339, 'unit': 'billion'}, {'company': 'Ford Motor Company', 'value': 256.54, 'unit': 'billion'}, {'company': 'AmerisourceBergen', 'value': 37.66, 'unit': 'billion'}, {'company': 'Chevron Corporation', 'value': 253.9, 'unit': 'billion'}, {'company': 'Cardinal Health', 'value': 39.95, 'unit': 'billion'}, {'company': 'Costco', 'value': 45.4, 'unit': 'billion'}, {'company': 'Verizon Communications', 'value': 264.82, 'unit': 'billion'}, {'company': 'Kroger', 'value': 38.11, 'unit': 'billion'}, {'company': 'General Electric', 'value': 309.129, 'unit': 'billion'}, {'company': 'Walgreens Boots Alliance', 'value': 67.59, 'unit': 'billion'}, {'company': 'JPMorgan Chase', 'value': 2.687, 'unit': 'trillion'}] Since we have both billion as well as trillion , let's normalize all the values, 1 2 3 4 for i , asset in enumerate ( assets ): if asset [ 'unit' ] == 'trillion' : # if unit is in trillion assets [ i ][ 'value' ] = asset [ 'value' ] * 1000 # convert trillion to billion assets [ i ][ 'unit' ] = 'billion' And create a new dataframe from the same, 1 2 df_assets = pd . DataFrame ( assets ) df_assets company value unit 0 Walmart 219.295 billion 1 ExxonMobil 346.2 billion 2 Berkshire Hathaway 707.8 billion 3 Apple Inc. 338.516 billion 4 UnitedHealth Group 173.889 billion 5 McKesson Corporation 60.381 billion 6 CVS Health 196.456 billion 7 Amazon (company) 162.648 billion 8 AT&T 531 billion 9 General Motors 227.339 billion 10 Ford Motor Company 256.54 billion 11 AmerisourceBergen 37.66 billion 12 Chevron Corporation 253.9 billion 13 Cardinal Health 39.95 billion 14 Costco 45.4 billion 15 Verizon Communications 264.82 billion 16 Kroger 38.11 billion 17 General Electric 309.129 billion 18 Walgreens Boots Alliance 67.59 billion 19 JPMorgan Chase 2687 billion Now finally let's create a bar plot showcasing the assets from all the companies, 1 2 3 4 5 6 7 8 9 10 11 12 ax = df_assets . plot ( kind = 'bar' , title = \"Assets from the Top 20 Companies on Fortune 500\" , color = 'skyblue' , figsize = ( 15 , 10 ), legend = True , fontsize = 12 ) ax . set_xticklabels ( df_assets . company ) ax . set_xlabel ( \"Company Name\" , fontsize = 12 ) ax . set_ylabel ( \"Assets (in Billions)\" , fontsize = 12 ) plt . show () Now let's create a new dataframe containing data related to products , industry and assets as follows, 1 2 df_wiki = pd . DataFrame ( data ) df_wiki . head () wiki_title product industry assets 0 Walmart pet supplies, party supplies, clothing, footwear, photo finishing, fitness, auto, grocery, electronics, home, movies, sporting goods, toys, beauty, jewelry, craft supplies, home improvement, music, furniture, health retail 219.295 billion 1 ExxonMobil petrochemicals, crude oil, oil products, natural gas, power generation gas, energy, oil, energy industry, gas industry 346.2 billion 2 Berkshire Hathaway food processing, casualty insurance, property, mass media, media, aerospace, insurance, final good, utilities, real estate, automotive industry, consumer products, internet, public utility, restaurants, sports equipment, sporting goods, toys, types, diversified investments, investment, automotive company, conglomerate 707.8 billion 3 Apple Inc. siri, iphone, watchos, tvos, shazam, ilife, iwork, final cut pro, ipod, application, apple watch, ipad, logic pro, macintosh, homepod, apple tv, macos, garageband, ios, ipados fabless manufacturing, cloud computing, semiconductors, artificial intelligence, fabless silicon design, consumer electronics, computer software, financial technology, digital distribution, computer hardware 338.516 billion 4 UnitedHealth Group health care, uniprise, service, specialized care services, ingenix, economics, services managed health care 173.889 billion And finally let's combine the datasets from Section 2 and 3 as follows, 1 2 3 4 # If you dont have the file, you can use the below code to fetch it: import urllib.request url = 'https://raw.githubusercontent.com/MonashDataFluency/python-web-scraping/master/data/fortune_500_companies.csv' urllib . request . urlretrieve ( url , 'fortune_500_companies.csv' ) 1 ('fortune_500_companies.csv', <http.client.HTTPMessage at 0x7fa6743ebc40>) 1 2 3 df = pd . read_csv ( 'fortune_500_companies.csv' , nrows = 20 ) # reading only the first 20 rows df = pd . concat ([ df , df_wiki ], axis = 1 ) # concatenating both the datasets df rank company_name company_website wiki_title product industry assets 0 1 Walmart http://www.stock.walmart.com Walmart beauty, footwear, furniture, party supplies, auto, fitness, pet supplies, movies, electronics, jewelry, craft supplies, clothing, grocery, music, photo finishing, home improvement, home, toys, sporting goods, health retail 219.295 billion 1 2 Exxon Mobil http://www.exxonmobil.com ExxonMobil oil products, petrochemicals, crude oil, power generation, natural gas gas industry, gas, energy, energy industry, oil 346.2 billion 2 3 Berkshire Hathaway http://www.berkshirehathaway.com Berkshire Hathaway diversified investments, mass media, automotive industry, media, final good, internet, food processing, public utility, sports equipment, insurance, investment, restaurants, casualty insurance, real estate, types, aerospace, automotive, consumer products, property, utilities, toys, sporting goods conglomerate, company 707.8 billion 3 4 Apple http://www.apple.com Apple Inc. ipad, apple tv, ilife, application, ipados, siri, ios, shazam, iwork, ipod, homepod, macintosh, tvos, logic pro, garageband, watchos, apple watch, macos, iphone, final cut pro artificial intelligence, cloud computing, digital distribution, computer software, financial technology, computer hardware, semiconductors, consumer electronics, fabless manufacturing, fabless silicon design 338.516 billion 4 5 UnitedHealth Group http://www.unitedhealthgroup.com UnitedHealth Group ingenix, service, specialized care services, uniprise, health care, economics, services managed health care 173.889 billion 5 6 McKesson http://www.mckesson.com McKesson Corporation pharmaceuticals, medical technology, health care services healthcare 60.381 billion 6 7 CVS Health http://www.cvshealth.com CVS Health health care, retail 196.456 billion 7 8 Amazon.com http://www.amazon.com Amazon (company) amazon fire os, fire os, amazon fire tablet, amazon kindle, amazon fire tv, amazon fire, amazon echo artificial intelligence, e-commerce, cloud computing, digital distribution, consumer electronics, grocery stores 162.648 billion 8 9 AT&T http://www.att.com AT&T film production, sports management, video games, landline, podcasts, publishing, pay television, satellite television, television production, internet service provider, network security, iptv, cable television, fixed-line telephones, ott services, mobile telephones, internet services, digital television, mobile phone, news agency, home security, over-the-top media services, broadband, filmmaking technology, entertainment, mass media, technology company, telecommunications industry, telecommunications 531.0 billion 9 10 General Motors http://www.gm.com General Motors automobiles, car, automobile parts, commercial vehicles automotive, automotive industry 227.339 billion 10 11 Ford Motor http://www.corporate.ford.com Ford Motor Company automobiles, automotive parts, pickup trucks, car, luxury car, commercial vehicles, list of auto parts, suvs, luxury vehicles, commercial vehicle automotive, automotive industry 256.54 billion 11 12 AmerisourceBergen http://www.amerisourcebergen.com AmerisourceBergen pharmacy services, pharmaceuticals pharmaceutical 37.66 billion 12 13 Chevron http://www.chevron.com Chevron Corporation other, see chevron products, petrochemicals, marketing brands, petroleum, natural gas gas, gas industry, oil 253.9 billion 13 14 Cardinal Health http://www.cardinalhealth.com Cardinal Health pharmaceutical products, medical, services pharmaceuticals 39.95 billion 14 15 Costco http://www.costco.com Costco retail 45.4 billion 15 16 Verizon http://www.verizon.com Verizon Communications mobile phone, iptv, telematics, internet, cable television, broadband, internet of things, landline, digital media, digital television telecommunications industry, telecommunications, mass media 264.82 billion 16 17 Kroger http://www.thekrogerco.com Kroger superstore, other specialty, supermarket, supercenter retail 38.11 billion 17 18 General Electric http://www.ge.com General Electric lighting, finance, wind turbines, electrical distribution, software, aircraft engines, energy, health care, electric power distribution, electric motors conglomerate, company 309.129 billion 18 19 Walgreens Boots Alliance http://www.walgreensbootsalliance.com Walgreens Boots Alliance drug store, pharmacy pharmaceutical, retail 67.59 billion 19 20 JPMorgan Chase http://www.jpmorganchase.com JPMorgan Chase broker services, finance, institutional investing, trustee services, currency exchange, debt settlement, financial markets, venture capital, foreign exchange market, loan servicing, mortgage brokers, index funds, investment banking, hedge funds, exchange-traded funds, mortgage-backed security, mortgage, mutual funds, underwriting, stock trading, security, investment management, prime brokerage, mortgage brokering, retail banking, backed securities, mortgage loans, bond, digital banking, merchant services, money market trading, american depositary receipts, financial analysis, credit cards, institutional investor, private equity, commodity market, insurance, capital market services, investment, retail, mobile banking, treasury services, alternative financial services, futures exchange, private banking, portfolios, subprime lending, bond trading, financial capital, custodian banking, commodities trading, risk management, pension funds, estate planning, brokerage, collateralized debt obligations, portfolio, asset management, securities lending, wholesale mortgage lenders, credit default swap, wealth management, remittance, wholesale mortgage lending, wire transfers, investment capital, credit derivative trading, information processing, security services, stock trader, asset allocation, wholesale funding, commercial banking financial services, banking 2.687 trillion And export them to a csv file, 1 df . to_csv ( 'top_20_companies.csv' , index = False )","title":"What the assets of the top 20 companies look like?"},{"location":"section-4-wrangling-and-analysis/#challenge","text":"Create a bar chart of equity of all the top 20 Fortune 500 companies and find out which has the highest value.","title":"Challenge"},{"location":"section-5-legal-and-ethical-considerations/","text":"This section does not constitute legal advice \u00b6 Please note that the information provided on this page is for information purposes only and does not constitute professional legal advice on the practice of web scraping. If you are concerned about the legal implications of using web scraping on a project you are working on, it is probably a good idea to seek advice from a professional, preferably someone who has knowledge of the intellectual property (copyright) legislation in effect in your country. Now that we have seen several different ways to scrape data from websites and are ready to start working on potentially larger projects, we may ask ourselves whether there are any legal implications of writing a piece of computer code that downloads information from the Internet. In this section, we will be discussing some of the issues to be aware of when scraping websites, and we will establish a code of conduct (below) to guide our web scraping projects. Don't break the web: Denial of Service attacks \u00b6 The first and most important thing to be careful about when writing a web scraper is that it typically involves querying a website repeatedly and accessing a potentially large number of pages. For each of these pages, a request will be sent to the web server that is hosting the site, and the server will have to process the request and send a response back to the computer that is running our code. Each of these requests will consume resources on the server, during which it will not be doing something else, like for example responding to someone else trying to access the same site. If we send too many such requests over a short span of time, we can prevent other \u201cnormal\u201d users from accessing the site during that time, or even cause the server to run out of resources and crash. In fact, this is such an efficient way to disrupt a web site that hackers are often doing it on purpose. This is called a Denial of Service (DoS) attack . Since DoS attacks are unfortunately a common occurence on the Internet, modern web servers include measures to ward off such illegitimate use of their resources. They are watchful for large amounts of requests appearing to come from a single computer or IP address, and their first line of defense often involves refusing any further requests coming from this IP address. A web scraper, even one with legitimate purposes and no intent to bring a website down, can exhibit similar behaviour and, if we are not careful, result in our computer being banned from accessing a website. Copyright: respecting other's intellectual property \u00b6 It is important to recognize that in certain circumstances web scraping can be illegal, and this differs from country to country . If the terms and conditions of the web site we are scraping specifically prohibit downloading and copying its content, then we could be in trouble for scraping it. In practice, however, web scraping is a tolerated practice, provided reasonable care is taken not to disrupt the \u201cregular\u201d use of a web site, as we have seen above. However you must be aware that without permisson from the copyright owner you may be in breach of copyright law. In a sense, web scraping is no different than using a web browser to visit a web page, in that it amounts to using computer software (a browser vs a scraper) to acccess data that is publicly available on the web. However, researchers should be aware of the risk since the law views web browsing differently to automated web scraping. In general, if data is publicly available (the content that is being scraped is not behind a password-protected authentication system), then it may be OK to scrape it, provided we don\u2019t break the web site doing so. What is potentially problematic is if the scraped data will be shared further. For example, downloading content off one website and posting it on another website (as our own), unless explicitly permitted, may constitute a violation of copyright law. Copyright law in some countries recognises \"fair use\" (USA, Israel, Singapore and South Korea) or \"fair dealing\" (Australia) which may, under very specific circumstances, allow reusing some copyrighted material. However the scope of these exceptions is narrow, and moreso for \"fair dealing\" under Australian law; you should not assume they apply to your case. For an interesting (Australian) copyright case involving web scraping, see IceTV vs Channel Nine . Better be safe than sorry \u00b6 Be aware that copyright and data privacy legislation typically differs from country to country. Be sure to check the laws that apply in your context. For example, in Australia, it can be illegal to scrape and store personal information such as names, phone numbers and email addresses, even if they are publicly available. If you are looking to scrape data for your own personal use, then the above guidelines should probably be all that you need to worry about. However, if you plan to start harvesting a large amount of data for research or commercial purposes, you should probably seek legal advice first. If you work in a university, chances are it has a copyright office that will help you sort out the legal aspects of your project. The university library is often the best place to start looking for help on copyright related queries. Challenge \u00b6 What are the contact details for the copyright office (or similar) at your organisation ? Be nice: ask and share \u00b6 Depending on the scope of your project, it might be worthwhile to consider asking the owners or curators of the data you are planning to scrape if they have it already available in a structured format that could suit your project. If your aim is do use their data for research, or to use it in a way that could potentially interest them, not only it could save you the trouble of writing a web scraper, but it could also help clarify straight away what you can and cannot do with the data. On the other hand, when you are publishing your own data, as part of a research project, documentation or a public website, you might want to think about whether someone might be interested in getting your data for their own project. If you can, try to provide others with a way to download your raw data in a structured format, and thus save them the trouble to try and scrape your own pages! Web scraping code of conduct \u00b6 This all being said, if you adhere to the following simple rules, you will probably be fine. Ask nicely. If your project requires data from a particular organisation, for example, you can try asking them directly if they could provide you what you are looking for. With some luck, they will have the primary data that they used on their website in a structured format, saving you the trouble. Don't download copies of documents that are clearly not public. For example, academic journal publishers often have very strict rules about what you can and what you cannot do with their databases. Mass downloading article PDFs is probably prohibited and can put you (or at the very least your friendly university librarian) in trouble. If your project requires local copies of documents (e.g. for text mining projects), special agreements can be reached with the publisher. The library is a good place to start investigating something like that. Check your local legislation. For example, certain countries have laws protecting personal information such as email addresses and phone numbers. Scraping such information, even from publicly available web sites, can be illegal (e.g. in Australia). Don't share downloaded content illegally. Scraping for personal purposes is usually OK, even if it is copyrighted information, as it could fall under the fair use provision of the intellectual property legislation. However, sharing data for which you don\u2019t hold the right to share is illegal. Share what you can. If the data you scraped is in the public domain or you got permission to share it, then put it out there for other people to reuse it (e.g. on datahub.io). If you wrote a web scraper to access it, share its code (e.g. on GitHub) so that others can benefit from it. Don't break the Internet. Not all web sites are designed to withstand thousands of requests per second. If you are writing a recursive scraper (i.e. that follows hyperlinks), test it on a smaller dataset first to make sure it does what it is supposed to do. Adjust the settings of your scraper to allow for a delay between requests. By default, Scrapy uses conservative settings that should minimize this risk. Publish your own data in a reusable way. Don\u2019t force others to write their own scrapers to get at your data. Use open and software-agnostic formats (e.g. JSON, XML), provide metadata (data about your data: where it came from, what it represents, how to use it, etc.) and make sure it can be indexed by search engines so that people can find it. View robots.txt file . Robots.txt is a file used by websites to let 'bots' know if or how the site should be crawled and indexed. When you are trying to extract data from the web, it is critical to understand what robots.txt is and how to respect it to avoid legal ramifications. This file can be accessed for any domain by accessing <domain_url>/robots.txt . For eg: monash.edu/robots.txt , facebook.com/robots.txt , linkedin.com/robots.txt . Happy scraping! References \u00b6 The Web scraping Wikipedia page has a concise definition of many concepts discussed here. This case study is a great example of what can be done using web scraping and a stepping stone to a more advanced python library scrapy . This recent case about Linkedin data is a good read. A crisp and simple explanation to robots.txt can be found here . Commencing 25 May 2018, Monash University will also become subject to the European Union\u2019s General Data Protection Regulation ( GDPR ). Software Carpentry is a non-profit organisation that runs learn-to-code workshops worldwide. All lessons are publicly available and can be followed indepentently. This lesson is heavily inspired by Software Carpentry. Data Carpentry is a sister organisation of Software Carpentry focused on the fundamental data management skills required to conduct research. Library Carpentry is another Software Carpentry spinoff focused on software skills for librarians.","title":"Legal and Ethical Considerations"},{"location":"section-5-legal-and-ethical-considerations/#this-section-does-not-constitute-legal-advice","text":"Please note that the information provided on this page is for information purposes only and does not constitute professional legal advice on the practice of web scraping. If you are concerned about the legal implications of using web scraping on a project you are working on, it is probably a good idea to seek advice from a professional, preferably someone who has knowledge of the intellectual property (copyright) legislation in effect in your country. Now that we have seen several different ways to scrape data from websites and are ready to start working on potentially larger projects, we may ask ourselves whether there are any legal implications of writing a piece of computer code that downloads information from the Internet. In this section, we will be discussing some of the issues to be aware of when scraping websites, and we will establish a code of conduct (below) to guide our web scraping projects.","title":"This section does not constitute legal advice"},{"location":"section-5-legal-and-ethical-considerations/#dont-break-the-web-denial-of-service-attacks","text":"The first and most important thing to be careful about when writing a web scraper is that it typically involves querying a website repeatedly and accessing a potentially large number of pages. For each of these pages, a request will be sent to the web server that is hosting the site, and the server will have to process the request and send a response back to the computer that is running our code. Each of these requests will consume resources on the server, during which it will not be doing something else, like for example responding to someone else trying to access the same site. If we send too many such requests over a short span of time, we can prevent other \u201cnormal\u201d users from accessing the site during that time, or even cause the server to run out of resources and crash. In fact, this is such an efficient way to disrupt a web site that hackers are often doing it on purpose. This is called a Denial of Service (DoS) attack . Since DoS attacks are unfortunately a common occurence on the Internet, modern web servers include measures to ward off such illegitimate use of their resources. They are watchful for large amounts of requests appearing to come from a single computer or IP address, and their first line of defense often involves refusing any further requests coming from this IP address. A web scraper, even one with legitimate purposes and no intent to bring a website down, can exhibit similar behaviour and, if we are not careful, result in our computer being banned from accessing a website.","title":"Don't break the web: Denial of Service attacks"},{"location":"section-5-legal-and-ethical-considerations/#copyright-respecting-others-intellectual-property","text":"It is important to recognize that in certain circumstances web scraping can be illegal, and this differs from country to country . If the terms and conditions of the web site we are scraping specifically prohibit downloading and copying its content, then we could be in trouble for scraping it. In practice, however, web scraping is a tolerated practice, provided reasonable care is taken not to disrupt the \u201cregular\u201d use of a web site, as we have seen above. However you must be aware that without permisson from the copyright owner you may be in breach of copyright law. In a sense, web scraping is no different than using a web browser to visit a web page, in that it amounts to using computer software (a browser vs a scraper) to acccess data that is publicly available on the web. However, researchers should be aware of the risk since the law views web browsing differently to automated web scraping. In general, if data is publicly available (the content that is being scraped is not behind a password-protected authentication system), then it may be OK to scrape it, provided we don\u2019t break the web site doing so. What is potentially problematic is if the scraped data will be shared further. For example, downloading content off one website and posting it on another website (as our own), unless explicitly permitted, may constitute a violation of copyright law. Copyright law in some countries recognises \"fair use\" (USA, Israel, Singapore and South Korea) or \"fair dealing\" (Australia) which may, under very specific circumstances, allow reusing some copyrighted material. However the scope of these exceptions is narrow, and moreso for \"fair dealing\" under Australian law; you should not assume they apply to your case. For an interesting (Australian) copyright case involving web scraping, see IceTV vs Channel Nine .","title":"Copyright: respecting other's intellectual property"},{"location":"section-5-legal-and-ethical-considerations/#better-be-safe-than-sorry","text":"Be aware that copyright and data privacy legislation typically differs from country to country. Be sure to check the laws that apply in your context. For example, in Australia, it can be illegal to scrape and store personal information such as names, phone numbers and email addresses, even if they are publicly available. If you are looking to scrape data for your own personal use, then the above guidelines should probably be all that you need to worry about. However, if you plan to start harvesting a large amount of data for research or commercial purposes, you should probably seek legal advice first. If you work in a university, chances are it has a copyright office that will help you sort out the legal aspects of your project. The university library is often the best place to start looking for help on copyright related queries.","title":"Better be safe than sorry"},{"location":"section-5-legal-and-ethical-considerations/#challenge","text":"What are the contact details for the copyright office (or similar) at your organisation ?","title":"Challenge"},{"location":"section-5-legal-and-ethical-considerations/#be-nice-ask-and-share","text":"Depending on the scope of your project, it might be worthwhile to consider asking the owners or curators of the data you are planning to scrape if they have it already available in a structured format that could suit your project. If your aim is do use their data for research, or to use it in a way that could potentially interest them, not only it could save you the trouble of writing a web scraper, but it could also help clarify straight away what you can and cannot do with the data. On the other hand, when you are publishing your own data, as part of a research project, documentation or a public website, you might want to think about whether someone might be interested in getting your data for their own project. If you can, try to provide others with a way to download your raw data in a structured format, and thus save them the trouble to try and scrape your own pages!","title":"Be nice: ask and share"},{"location":"section-5-legal-and-ethical-considerations/#web-scraping-code-of-conduct","text":"This all being said, if you adhere to the following simple rules, you will probably be fine. Ask nicely. If your project requires data from a particular organisation, for example, you can try asking them directly if they could provide you what you are looking for. With some luck, they will have the primary data that they used on their website in a structured format, saving you the trouble. Don't download copies of documents that are clearly not public. For example, academic journal publishers often have very strict rules about what you can and what you cannot do with their databases. Mass downloading article PDFs is probably prohibited and can put you (or at the very least your friendly university librarian) in trouble. If your project requires local copies of documents (e.g. for text mining projects), special agreements can be reached with the publisher. The library is a good place to start investigating something like that. Check your local legislation. For example, certain countries have laws protecting personal information such as email addresses and phone numbers. Scraping such information, even from publicly available web sites, can be illegal (e.g. in Australia). Don't share downloaded content illegally. Scraping for personal purposes is usually OK, even if it is copyrighted information, as it could fall under the fair use provision of the intellectual property legislation. However, sharing data for which you don\u2019t hold the right to share is illegal. Share what you can. If the data you scraped is in the public domain or you got permission to share it, then put it out there for other people to reuse it (e.g. on datahub.io). If you wrote a web scraper to access it, share its code (e.g. on GitHub) so that others can benefit from it. Don't break the Internet. Not all web sites are designed to withstand thousands of requests per second. If you are writing a recursive scraper (i.e. that follows hyperlinks), test it on a smaller dataset first to make sure it does what it is supposed to do. Adjust the settings of your scraper to allow for a delay between requests. By default, Scrapy uses conservative settings that should minimize this risk. Publish your own data in a reusable way. Don\u2019t force others to write their own scrapers to get at your data. Use open and software-agnostic formats (e.g. JSON, XML), provide metadata (data about your data: where it came from, what it represents, how to use it, etc.) and make sure it can be indexed by search engines so that people can find it. View robots.txt file . Robots.txt is a file used by websites to let 'bots' know if or how the site should be crawled and indexed. When you are trying to extract data from the web, it is critical to understand what robots.txt is and how to respect it to avoid legal ramifications. This file can be accessed for any domain by accessing <domain_url>/robots.txt . For eg: monash.edu/robots.txt , facebook.com/robots.txt , linkedin.com/robots.txt . Happy scraping!","title":"Web scraping code of conduct"},{"location":"section-5-legal-and-ethical-considerations/#references","text":"The Web scraping Wikipedia page has a concise definition of many concepts discussed here. This case study is a great example of what can be done using web scraping and a stepping stone to a more advanced python library scrapy . This recent case about Linkedin data is a good read. A crisp and simple explanation to robots.txt can be found here . Commencing 25 May 2018, Monash University will also become subject to the European Union\u2019s General Data Protection Regulation ( GDPR ). Software Carpentry is a non-profit organisation that runs learn-to-code workshops worldwide. All lessons are publicly available and can be followed indepentently. This lesson is heavily inspired by Software Carpentry. Data Carpentry is a sister organisation of Software Carpentry focused on the fundamental data management skills required to conduct research. Library Carpentry is another Software Carpentry spinoff focused on software skills for librarians.","title":"References"},{"location":"section-6-advanced-topics/","text":"Variable arguments: \u00b6 1 2 3 4 5 def plus ( * args ): return sum ( args ) # Calculate the sum plus ( 1 , 4 , 5 ) 1 10 Variable keyword arguments: \u00b6 1 2 3 4 5 6 7 8 def concatenate ( ** kwargs ): result = \"\" # Iterating over the Python kwargs dictionary for arg in kwargs . values (): result = result + arg + \" \" return result print ( concatenate ( a = \"Real\" , b = \"Python\" , c = \"Is\" , d = \"Great\" , e = \"!\" )) 1 Real Python Is Great ! the correct order for your parameters is: 1 2 3 Standard arguments *args arguments **kwargs arguments 1","title":"Section 6 advanced topics"},{"location":"section-6-advanced-topics/#variable-arguments","text":"1 2 3 4 5 def plus ( * args ): return sum ( args ) # Calculate the sum plus ( 1 , 4 , 5 ) 1 10","title":"Variable arguments:"},{"location":"section-6-advanced-topics/#variable-keyword-arguments","text":"1 2 3 4 5 6 7 8 def concatenate ( ** kwargs ): result = \"\" # Iterating over the Python kwargs dictionary for arg in kwargs . values (): result = result + arg + \" \" return result print ( concatenate ( a = \"Real\" , b = \"Python\" , c = \"Is\" , d = \"Great\" , e = \"!\" )) 1 Real Python Is Great ! the correct order for your parameters is: 1 2 3 Standard arguments *args arguments **kwargs arguments 1","title":"Variable keyword arguments:"},{"location":"section-7-references/","text":"References \u00b6 Library Carpentry is another Software Carpentry spinoff focused on software skills for librarians. W3School Tutorials Point Python Getting Started Image Source","title":"References"},{"location":"section-7-references/#references","text":"Library Carpentry is another Software Carpentry spinoff focused on software skills for librarians. W3School Tutorials Point Python Getting Started Image Source","title":"References"}]}